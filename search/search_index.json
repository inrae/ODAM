{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"ODAM: Open Data for Access and Mining \u00b6 ODAM (Open Data for Access and Mining) is an Experimental Data Table Management System (EDTMS) Experimental data table management software to make research data accessible and available for reuse with minimal effort on the part of the data provider. Designed to manage experimental data tables in an easy way for users, ODAM provides a model for structuring both data and metadata that facilitates data handling and analysis. It also encourages data dissemination according to FAIR principles by making the data interoperable and reusable by both humans and machines, allowing the dataset to be explored and then extracted in whole or in part as needed. Publication Daniel Jacob, Romain David, Sophie Aubin, Yves Gibon (2020) Making experimental data tables in the life sciences more FAIR: a pragmatic approach, GigaScience, Volume 9, Issue 12, December 2020, doi:10.1093/gigascience/giaa144","title":"Home"},{"location":"#odam-open-data-for-access-and-mining","text":"ODAM (Open Data for Access and Mining) is an Experimental Data Table Management System (EDTMS) Experimental data table management software to make research data accessible and available for reuse with minimal effort on the part of the data provider. Designed to manage experimental data tables in an easy way for users, ODAM provides a model for structuring both data and metadata that facilitates data handling and analysis. It also encourages data dissemination according to FAIR principles by making the data interoperable and reusable by both humans and machines, allowing the dataset to be explored and then extracted in whole or in part as needed. Publication Daniel Jacob, Romain David, Sophie Aubin, Yves Gibon (2020) Making experimental data tables in the life sciences more FAIR: a pragmatic approach, GigaScience, Volume 9, Issue 12, December 2020, doi:10.1093/gigascience/giaa144","title":"ODAM: Open Data for Access and Mining"},{"location":"about/","text":"ODAM: Open Data for Access and Mining \u00b6 (C) INRAE UMR 1332 BFP, PMB - 2020 - Version 1.1 We have grouped together here all the links to documents and software produced about ODAM, as well as those concerning the tools, specifications and repositories (data and software) used directly (production) or indirectly (use, referencing) by ODAM. Online resources related to ODAM \u00b6 Description Type Link Data Preparation Protocol for ODAM Compliance Documentation Data collection and preparation API Documentation based on Swagger Web API Tool INRA-PMB/ODAM/1.2.0/ R ODAM package and How to use it Package https://cran.r-project.org/package=Rodam R ODAM package Vignette Package Rodam/vignettes/Rodam.html Virtual Machine embedding the ODAM software on Oracle VM VirtualBox along with its installation guide Virtual Machine + Documentation https://doi.org/10.15454/C9LAEF Docker containers on DockerHub for installing on a Linux machine Container https://hub.docker.com/r/odam/getdata/ https://hub.docker.com/r/odam/dataexplorer/ A very lightweight local web server for Windows to deploy the ODAM API Web API Tool ODAMwebserver Examples of Jupyter notebooks (R & Python) based on the ODAM Web API Notebook https://github.com/djacob65/binder_odam https://doi.org/10.24433/CO.8981049.v1 https://doi.org/10.24433/CO.0011270.v1 Modeling the growth of tomato fruits based on enzyme activity profiles (Example of data analysis interfaced by ODAM) Notebook https://hal.inrae.fr/hal-02611223 ODAM Source code on GitHub Source Code https://github.com/inrae/ODAM ODAM Source code on Software Heritage Source Code https://www.softwareheritage.org/ JSON Schema for ODAM data package Source Code https://github.com/djacob65/odam-datapackage/ Note : In order either to improve the use of ODAM or to complement its toolbox, a number of additional developments are planned. External resources used \u00b6 Tools Name Description Link docker Container management tool https://www.docker.com/ Oracle VM VirtualBox Virtual Machine (VM) management tool https://www.virtualbox.org/ Rstudio Integrated Development Environment (IDE) for R https://rstudio.com/ R shiny interactive web apps for R https://shiny.rstudio.com/ jupyter notebook Web application for e-notebooks https://jupyter.org/ q-text-as-data A command line tool for running SQL directly on CSV or TSV files http://harelba.github.io/q/ sqlite3 A command line tool for executing SQL statements against an SQLite database https://www.sqlite.org/cli.html goodtables.io data publishing with confidence - data validation on every change http://goodtables.io/ codemeta-generator Create your codemeta file https://codemeta.github.io/codemeta-generator/ ---- Repositories Name Type Link GitHub Source code https://github.com/ Software Heritage Source Code https://www.softwareheritage.org/ Docker Hub Containers https://hub.docker.com/ Data INRAE Data https://data.inrae.fr/ HAL INRAE Documents https://hal.inrae.fr/ protocols.io Documents https://www.protocols.io/ Code Ocean Notebooks https://codeocean.com/ R cran Source code https://cran.r-project.org/ Specifications Name Type Link Swagger API schema https://swagger.io/specification/ Data Packages Data schema https://specs.frictionlessdata.io/data-package/ The CodeMeta Project Code metadata schema https://codemeta.github.io/ Registries Name Type Link bio.tools Tool and data services registry. https://bio.tools/ RRID Research Resource Identification registry https://scicrunch.org/browse/resourcedashboard Identifiers Central registry which provides compact identifiers https://registry.identifiers.org/ FAIRsharing Standards, databases and policies registry https://fairsharing.org/ RDA Metadata Directory Standards, extensions, tools, use cases https://rd-alliance.github.io/metadata-directory/ FAIR evaluation/assessment grids Name Link 5 \u2605 Data Rating Tool https://confluence.csiro.au/display/OZNOME/Oznome+5-star+Processes RDA FAIR Data Maturity Model WG https://www.rd-alliance.org/groups/fair-data-maturity-model-wg RDA Sharing Rewards and Credit IG https://www.rd-alliance.org/groups/sharing-rewards-and-credit-sharc-ig Tools used for this online documentation \u00b6 Name Link Make the Docs https://www.mkdocs.org/ Material for MkDocs https://squidfunk.github.io/mkdocs-material/ Lightbox https://lokeshdhakar.com/projects/lightbox2/ Mermaid http://mermaid-js.github.io/mermaid/","title":"ressources"},{"location":"about/#odam-open-data-for-access-and-mining","text":"(C) INRAE UMR 1332 BFP, PMB - 2020 - Version 1.1 We have grouped together here all the links to documents and software produced about ODAM, as well as those concerning the tools, specifications and repositories (data and software) used directly (production) or indirectly (use, referencing) by ODAM.","title":"ODAM: Open Data for Access and Mining"},{"location":"about/#online-resources-related-to-odam","text":"Description Type Link Data Preparation Protocol for ODAM Compliance Documentation Data collection and preparation API Documentation based on Swagger Web API Tool INRA-PMB/ODAM/1.2.0/ R ODAM package and How to use it Package https://cran.r-project.org/package=Rodam R ODAM package Vignette Package Rodam/vignettes/Rodam.html Virtual Machine embedding the ODAM software on Oracle VM VirtualBox along with its installation guide Virtual Machine + Documentation https://doi.org/10.15454/C9LAEF Docker containers on DockerHub for installing on a Linux machine Container https://hub.docker.com/r/odam/getdata/ https://hub.docker.com/r/odam/dataexplorer/ A very lightweight local web server for Windows to deploy the ODAM API Web API Tool ODAMwebserver Examples of Jupyter notebooks (R & Python) based on the ODAM Web API Notebook https://github.com/djacob65/binder_odam https://doi.org/10.24433/CO.8981049.v1 https://doi.org/10.24433/CO.0011270.v1 Modeling the growth of tomato fruits based on enzyme activity profiles (Example of data analysis interfaced by ODAM) Notebook https://hal.inrae.fr/hal-02611223 ODAM Source code on GitHub Source Code https://github.com/inrae/ODAM ODAM Source code on Software Heritage Source Code https://www.softwareheritage.org/ JSON Schema for ODAM data package Source Code https://github.com/djacob65/odam-datapackage/ Note : In order either to improve the use of ODAM or to complement its toolbox, a number of additional developments are planned.","title":"Online resources related to ODAM"},{"location":"about/#external-resources-used","text":"Tools Name Description Link docker Container management tool https://www.docker.com/ Oracle VM VirtualBox Virtual Machine (VM) management tool https://www.virtualbox.org/ Rstudio Integrated Development Environment (IDE) for R https://rstudio.com/ R shiny interactive web apps for R https://shiny.rstudio.com/ jupyter notebook Web application for e-notebooks https://jupyter.org/ q-text-as-data A command line tool for running SQL directly on CSV or TSV files http://harelba.github.io/q/ sqlite3 A command line tool for executing SQL statements against an SQLite database https://www.sqlite.org/cli.html goodtables.io data publishing with confidence - data validation on every change http://goodtables.io/ codemeta-generator Create your codemeta file https://codemeta.github.io/codemeta-generator/ ---- Repositories Name Type Link GitHub Source code https://github.com/ Software Heritage Source Code https://www.softwareheritage.org/ Docker Hub Containers https://hub.docker.com/ Data INRAE Data https://data.inrae.fr/ HAL INRAE Documents https://hal.inrae.fr/ protocols.io Documents https://www.protocols.io/ Code Ocean Notebooks https://codeocean.com/ R cran Source code https://cran.r-project.org/ Specifications Name Type Link Swagger API schema https://swagger.io/specification/ Data Packages Data schema https://specs.frictionlessdata.io/data-package/ The CodeMeta Project Code metadata schema https://codemeta.github.io/ Registries Name Type Link bio.tools Tool and data services registry. https://bio.tools/ RRID Research Resource Identification registry https://scicrunch.org/browse/resourcedashboard Identifiers Central registry which provides compact identifiers https://registry.identifiers.org/ FAIRsharing Standards, databases and policies registry https://fairsharing.org/ RDA Metadata Directory Standards, extensions, tools, use cases https://rd-alliance.github.io/metadata-directory/ FAIR evaluation/assessment grids Name Link 5 \u2605 Data Rating Tool https://confluence.csiro.au/display/OZNOME/Oznome+5-star+Processes RDA FAIR Data Maturity Model WG https://www.rd-alliance.org/groups/fair-data-maturity-model-wg RDA Sharing Rewards and Credit IG https://www.rd-alliance.org/groups/sharing-rewards-and-credit-sharc-ig","title":"External resources used"},{"location":"about/#tools-used-for-this-online-documentation","text":"Name Link Make the Docs https://www.mkdocs.org/ Material for MkDocs https://squidfunk.github.io/mkdocs-material/ Lightbox https://lokeshdhakar.com/projects/lightbox2/ Mermaid http://mermaid-js.github.io/mermaid/","title":"Tools used for this online documentation"},{"location":"api/","text":"ODAM: Web API \u00b6 Services \u00b6 The ODAM API services are based on REST services using a Resource Naming convention i.e an understandable naming of resources leading to a Web API (resource identification/query) that is easily exploitable and easy to implement in a script language ( R or Python ). Moreover, The ODAM API services follow the OpenAPI specifications . API Documentation on SwaggerHub : INRA-PMB/ODAM/1.2.0 Four main services (i.e query-paths) are provided, as described below: Service Description check Check the structure and consistency of the metadata files with the data files build Build the sqlite3 database allowing an acceleration of the queries query Query data based on metadata infos Get the content of the infos.md file (markdown format) query Using the two metadata files namely a_attributes.tsv and s_subsets.tsv , it is possible to build a tree structure from which the data files can be queried to extract a subset. The tree structure is built on the Entity-Attribute-Value scheme. Field Description Examples <dataset name> Short name (tag) of your dataset frim1 <subset> Short name of a data subset samples <entry> Name of an attribute entry (defined by the user in the a_attribute file (column \u2018entry\u2019) sampleid <category> Name of the attribute category; (assigned by the user in the a_attribute file (column \u2018category\u2019). Possible values are: \u2018identifier\u2019, \u2018factor\u2019, \u2018qualitative\u2019, \u2018quantitative\u2019 quantitative (<subset>) Set of data subsets by merging all the subsets with lower rank than the specified subset and following the pathway defined by the \"obtainedFrom\" links. (samples) <=> plants + samples <value> Exact value of the desired entry or category 1 (for subset) or Factor (for category) <data format> Format of the retrieved data; possible values are: CSV , TSV , JSON or XML tsv infos This service allows you to retrieve additional information. See Additional information in Data collection an preparation . check This service allows to perform a set of tests to check if the dataset is well formatted and structured according to the expected specifications. See Final checking in Data collection an preparation . build This service builds the corresponding sqlite3 database to speed up response times on large files. See below in the section Very detailed example of API querying . Two additional services (i.e query-paths) are provided (see Additional information section in Data collection and preparation ), as described below: Service Description image Retrieve an image: /getdata/image/{dataset}/{imagefile.png} pdf Retrieve a PDF file: /getdata/pdf/{dataset}/{pdffile.pdf} Some services (i.e query-paths) are available as shortcuts Service Description json Query data with data formatted in JSON as output (shortcut for ../query/{dataset}/...?format=json ) tsv Query data with data formatted in TSV as output (shortcut for ../query/{dataset}/...?format=tsv ) xml Query data with data formatted in XML as output (shortcut for ../query/{dataset}/...?format=xml ) Parameters that can be specified in the query-string as described below. These parameters can be combined : e.g ?format=tsv&limit=100&debug=1 Parameter Description Example format Format of the retrieved data; possible values are: 'xml', \u2018json\u2019, 'csv' or \u2018tsv' format=tsv auth Set the API Key if required (non-preferential method) auth=secret.key limit Limitation of the number of records provided limit=100 debug Provide information about the request (1 for debug, 0 for query output) debug=1 Authorization mechanism \u00b6 Currently, the ODAM API implements a key-based authorization mechanism , which is different from the authentication mechanism. (see this blog ). The API key approach seems to us to be an adequate solution insofar as only the \"read\" functionality is concerned. Thus, without the need to edit, modify or delete, security is less of a concern. Furthermore, we have associated to these API keys the possibility to limit their usage according to the internet address (IP) of the machines from which the users make their requests. There is no centralized management of these keys. The management is done for each dataset independently. The principle is simple: if you want to put a key protection on a dataset, just create a file named authkeys.tsv ( TSV format) and add it in the same directory as the dataset. Without this file, the dataset is completely open. The file authkeys.tsv ( TSV format) has three columns as described below: column description 1 st IP Address or sub-network : in case of sub-network, only the common part of all IP adresses in the sub-network have to be specified. To signify all IPs, the character '*' will be used 2 nd Can be either the API key or one of the two following keywords: 'deny' or 'allow'. 'deny' to forbid access (with or without API key), and 'allow' to allow access without API key. 3 rd Comment or description Knowing that as soon as a rule is satisfied, the other rules are not read, it is important to order the rules according to a decreasing priority, the first one being the most important and so on. Example of Authorization file (authkeys.tsv) IP Authorization Description 77.111 deny VPN Opera Browser 10.0.0 allow local VMs 192.168.100.50 allow a PC Desktop 192.168.100.51 allow another one PC Desktop * secret.key Others Another example of authorization file for a private dataset could be this: IP Authorization Description 192.168.100.50 allow a PC Desktop * deny Others The API key, when required, can be specified: in the API URL by adding 'auth={API key}' as a query parameter (i.e query string) : curl \"https:/myserver/{query-paths}/{dataset}/.../?format={format}&auth={API key}\" in the HTTP header (recommended method): curl \"https:/myserver/{query-paths}/{dataset}/.../?format={format}\" -H \"X-Api-Key: {API key}\" Note 1 : You have to take into account that there may be several network cards on your machine, especially virtual network cards in the case of using a virtualization software (e.g. VirtualBox). So it is the IP address of the virtual card involved that should be put in the authkeys.tsv file in order to set specific authorizations. For example, the virtual network card of the distributed virtual machine for VirtualBox (see Installation ) has the IP address set as 192.168.99.100. See ipconfig for Windows and ifconfig/ip for linux. Note 2 : If an application hosted on a remote server makes requests, then of course the IP address of that server (which is not the IP of the client) will be taken as the default for comparison with the authorization settings. However, it is possible to implement a mechanism to modify the API query header by specifying the client's originating IP (see X-Forwarded-For ). This is what has been done for example in the data explorer. The approach used in the data explorer is as follows: using a javascript, we retrieve the client's IP address which is sent to the server side (see examples ) On the server side, we modify the API request header by adding the X-Forwarded-For parameter positioned with the client IP address (using the R package httr, see add_headers ) Very detailed example of API querying \u00b6 Since all the experimental data tables were generated as part of an experiment associated with a Design of Experiment ( DoE ), each file thus contains data acquired sequentially as the experiment progressed. There must therefore be a link between each file, i.e. information that connects them together. In most cases (if not all), this information corresponds to identifiers that make it possible to precisely reference within the experiment each of the elements belonging to the same observation entity forming a coherent observation unit. For example, each plant, each sample has its own identifier, and each of these entities corresponds to a separate data file. The files generated during data collection have to be organized according to the entity-relationship model similar to relational database management systems ( RDBMS ), as shown below with the FRIM1 dataset. Extract of structural metadata (file s_subsets.tsv) for FRIM1 dataset Each file (an entity , i.e an observational unit) corresponds to a type of collected data (samples, compounds, ...) for which is associated a set of attributes , i.e. a set of variables that may include observed or measured variables ( quantitative or qualitative ), controlled independent variables ( factors ) and obviously an identifier . Since there is a relationship between each of the files, it is therefore possible to combine them by following the paths of their mutual links. Suppose we want to retrieve activome data (enzyme expressions) combined with NMR observed metabolite data , including experimental metadata (factors and phenotypic data). The ODAM API offers the possibility of combining data by following relational paths, using the ' ( { subset } ) ' operator in the API query. Simply specify the desired subsets separated by a comma, as shown below: '(activome,qNMR_metabo)' operator in the API query The ' ( { subset } ) ' operator allows you to get data subsets by merging all the subsets with lower rank than the specified subsets and following the pathway defined by the \u201cobtainedFrom\" links. Let's further assume that we only want the data for the 'Control' treatment. The ODAM API also offers the possibility of setting a selector called an 'entry' provided that such an entry has been specified for the attribute to which one wants to apply a selection, as shown below: Extract of structural metadata (file a_attributes.tsv) for FRIM1 dataset In our case, the 'Treatment' factor has the entry (alias) 'treatment'. It is therefore possible to use it as a selector in the API query. Thus, our complete API query would be this one: Complete API query To test this query by yourself, you can go to ODAM's Swagger interface and play with the followed specific query : ODAM API in Swagger Finally, how does it work internally? How are the different subsets of data combined in practice? Since each data subset being a data file, and linked to one or more other data subsets by means of common identifiers, it is possible to apply a SQL query from the data files as shown below: SQL query corresponding to the API query The application of SQL queries is performed either with the q - Text as Data tool which allows to apply SQL queries directly on CSV and TSV files. It is the default tool. or with the sqlite3 tool which, by building a database and indexing it, allows to accelerate considerably the response times on large files. To proceed with the creation and indexing of the database, a call to the API is necessary (/getdata/build/{dataset}). To see what types of operations are made, try i.e this API query In addition, if you want to display information about the API query, just add '/debug' at the end of the query, as shown below: https://pmb-bordeaux.fr/getdata/query/frim1/(activome,qNMR_metabo)/treatment/Control/debug","title":"Web API"},{"location":"api/#odam-web-api","text":"","title":"ODAM: Web API"},{"location":"api/#services","text":"The ODAM API services are based on REST services using a Resource Naming convention i.e an understandable naming of resources leading to a Web API (resource identification/query) that is easily exploitable and easy to implement in a script language ( R or Python ). Moreover, The ODAM API services follow the OpenAPI specifications . API Documentation on SwaggerHub : INRA-PMB/ODAM/1.2.0 Four main services (i.e query-paths) are provided, as described below: Service Description check Check the structure and consistency of the metadata files with the data files build Build the sqlite3 database allowing an acceleration of the queries query Query data based on metadata infos Get the content of the infos.md file (markdown format) query Using the two metadata files namely a_attributes.tsv and s_subsets.tsv , it is possible to build a tree structure from which the data files can be queried to extract a subset. The tree structure is built on the Entity-Attribute-Value scheme. Field Description Examples <dataset name> Short name (tag) of your dataset frim1 <subset> Short name of a data subset samples <entry> Name of an attribute entry (defined by the user in the a_attribute file (column \u2018entry\u2019) sampleid <category> Name of the attribute category; (assigned by the user in the a_attribute file (column \u2018category\u2019). Possible values are: \u2018identifier\u2019, \u2018factor\u2019, \u2018qualitative\u2019, \u2018quantitative\u2019 quantitative (<subset>) Set of data subsets by merging all the subsets with lower rank than the specified subset and following the pathway defined by the \"obtainedFrom\" links. (samples) <=> plants + samples <value> Exact value of the desired entry or category 1 (for subset) or Factor (for category) <data format> Format of the retrieved data; possible values are: CSV , TSV , JSON or XML tsv infos This service allows you to retrieve additional information. See Additional information in Data collection an preparation . check This service allows to perform a set of tests to check if the dataset is well formatted and structured according to the expected specifications. See Final checking in Data collection an preparation . build This service builds the corresponding sqlite3 database to speed up response times on large files. See below in the section Very detailed example of API querying . Two additional services (i.e query-paths) are provided (see Additional information section in Data collection and preparation ), as described below: Service Description image Retrieve an image: /getdata/image/{dataset}/{imagefile.png} pdf Retrieve a PDF file: /getdata/pdf/{dataset}/{pdffile.pdf} Some services (i.e query-paths) are available as shortcuts Service Description json Query data with data formatted in JSON as output (shortcut for ../query/{dataset}/...?format=json ) tsv Query data with data formatted in TSV as output (shortcut for ../query/{dataset}/...?format=tsv ) xml Query data with data formatted in XML as output (shortcut for ../query/{dataset}/...?format=xml ) Parameters that can be specified in the query-string as described below. These parameters can be combined : e.g ?format=tsv&limit=100&debug=1 Parameter Description Example format Format of the retrieved data; possible values are: 'xml', \u2018json\u2019, 'csv' or \u2018tsv' format=tsv auth Set the API Key if required (non-preferential method) auth=secret.key limit Limitation of the number of records provided limit=100 debug Provide information about the request (1 for debug, 0 for query output) debug=1","title":"Services"},{"location":"api/#authorization-mechanism","text":"Currently, the ODAM API implements a key-based authorization mechanism , which is different from the authentication mechanism. (see this blog ). The API key approach seems to us to be an adequate solution insofar as only the \"read\" functionality is concerned. Thus, without the need to edit, modify or delete, security is less of a concern. Furthermore, we have associated to these API keys the possibility to limit their usage according to the internet address (IP) of the machines from which the users make their requests. There is no centralized management of these keys. The management is done for each dataset independently. The principle is simple: if you want to put a key protection on a dataset, just create a file named authkeys.tsv ( TSV format) and add it in the same directory as the dataset. Without this file, the dataset is completely open. The file authkeys.tsv ( TSV format) has three columns as described below: column description 1 st IP Address or sub-network : in case of sub-network, only the common part of all IP adresses in the sub-network have to be specified. To signify all IPs, the character '*' will be used 2 nd Can be either the API key or one of the two following keywords: 'deny' or 'allow'. 'deny' to forbid access (with or without API key), and 'allow' to allow access without API key. 3 rd Comment or description Knowing that as soon as a rule is satisfied, the other rules are not read, it is important to order the rules according to a decreasing priority, the first one being the most important and so on. Example of Authorization file (authkeys.tsv) IP Authorization Description 77.111 deny VPN Opera Browser 10.0.0 allow local VMs 192.168.100.50 allow a PC Desktop 192.168.100.51 allow another one PC Desktop * secret.key Others Another example of authorization file for a private dataset could be this: IP Authorization Description 192.168.100.50 allow a PC Desktop * deny Others The API key, when required, can be specified: in the API URL by adding 'auth={API key}' as a query parameter (i.e query string) : curl \"https:/myserver/{query-paths}/{dataset}/.../?format={format}&auth={API key}\" in the HTTP header (recommended method): curl \"https:/myserver/{query-paths}/{dataset}/.../?format={format}\" -H \"X-Api-Key: {API key}\" Note 1 : You have to take into account that there may be several network cards on your machine, especially virtual network cards in the case of using a virtualization software (e.g. VirtualBox). So it is the IP address of the virtual card involved that should be put in the authkeys.tsv file in order to set specific authorizations. For example, the virtual network card of the distributed virtual machine for VirtualBox (see Installation ) has the IP address set as 192.168.99.100. See ipconfig for Windows and ifconfig/ip for linux. Note 2 : If an application hosted on a remote server makes requests, then of course the IP address of that server (which is not the IP of the client) will be taken as the default for comparison with the authorization settings. However, it is possible to implement a mechanism to modify the API query header by specifying the client's originating IP (see X-Forwarded-For ). This is what has been done for example in the data explorer. The approach used in the data explorer is as follows: using a javascript, we retrieve the client's IP address which is sent to the server side (see examples ) On the server side, we modify the API request header by adding the X-Forwarded-For parameter positioned with the client IP address (using the R package httr, see add_headers )","title":"Authorization mechanism"},{"location":"api/#very-detailed-example-of-api-querying","text":"Since all the experimental data tables were generated as part of an experiment associated with a Design of Experiment ( DoE ), each file thus contains data acquired sequentially as the experiment progressed. There must therefore be a link between each file, i.e. information that connects them together. In most cases (if not all), this information corresponds to identifiers that make it possible to precisely reference within the experiment each of the elements belonging to the same observation entity forming a coherent observation unit. For example, each plant, each sample has its own identifier, and each of these entities corresponds to a separate data file. The files generated during data collection have to be organized according to the entity-relationship model similar to relational database management systems ( RDBMS ), as shown below with the FRIM1 dataset. Extract of structural metadata (file s_subsets.tsv) for FRIM1 dataset Each file (an entity , i.e an observational unit) corresponds to a type of collected data (samples, compounds, ...) for which is associated a set of attributes , i.e. a set of variables that may include observed or measured variables ( quantitative or qualitative ), controlled independent variables ( factors ) and obviously an identifier . Since there is a relationship between each of the files, it is therefore possible to combine them by following the paths of their mutual links. Suppose we want to retrieve activome data (enzyme expressions) combined with NMR observed metabolite data , including experimental metadata (factors and phenotypic data). The ODAM API offers the possibility of combining data by following relational paths, using the ' ( { subset } ) ' operator in the API query. Simply specify the desired subsets separated by a comma, as shown below: '(activome,qNMR_metabo)' operator in the API query The ' ( { subset } ) ' operator allows you to get data subsets by merging all the subsets with lower rank than the specified subsets and following the pathway defined by the \u201cobtainedFrom\" links. Let's further assume that we only want the data for the 'Control' treatment. The ODAM API also offers the possibility of setting a selector called an 'entry' provided that such an entry has been specified for the attribute to which one wants to apply a selection, as shown below: Extract of structural metadata (file a_attributes.tsv) for FRIM1 dataset In our case, the 'Treatment' factor has the entry (alias) 'treatment'. It is therefore possible to use it as a selector in the API query. Thus, our complete API query would be this one: Complete API query To test this query by yourself, you can go to ODAM's Swagger interface and play with the followed specific query : ODAM API in Swagger Finally, how does it work internally? How are the different subsets of data combined in practice? Since each data subset being a data file, and linked to one or more other data subsets by means of common identifiers, it is possible to apply a SQL query from the data files as shown below: SQL query corresponding to the API query The application of SQL queries is performed either with the q - Text as Data tool which allows to apply SQL queries directly on CSV and TSV files. It is the default tool. or with the sqlite3 tool which, by building a database and indexing it, allows to accelerate considerably the response times on large files. To proceed with the creation and indexing of the database, a call to the API is necessary (/getdata/build/{dataset}). To see what types of operations are made, try i.e this API query In addition, if you want to display information about the API query, just add '/debug' at the end of the query, as shown below: https://pmb-bordeaux.fr/getdata/query/frim1/(activome,qNMR_metabo)/treatment/Control/debug","title":"Very detailed example of API querying"},{"location":"data-explorer/","text":"ODAM: Data Explorer \u00b6 Data view : The data explorer offers a complete view of all data and associated metadata. On a single page, all the information about the dataset is gathered. In addition a graph of the relationships between the datasets is provided. Example online : https://pmb-bordeaux.fr/dataexplorer/?ds=frim1 Dataset information Data analysis : The Data Explorer makes also data easy to explore, visualize, and subsequently to better understand the data as a whole. Explore your data in several ways according to your concerns by interacting with the graphs. For instance, univariate, bivariate, multi-univariate (volcano plot) and multivariate (PCA, ICA, t-SNE, COR, GGM) approaches have been implemented so that they are very easy to be interactively used. This is very useful in order to have a first glimpse of the data that can show trends and this allows the data to be well characterized, which is necessary to then choose how to deeply analyze them later on. For each analysis, several possibilities can be explored by selecting many parameters and interacting with the graphs as illustrated below. Example of univariate analysis Example of multivariate analysis Data merging : One or more subsets of data can be selected to be combined in bivariate and multivariate analysis. Save graphics : Graphics can be easily exported as PNG/SVG files. Parameters in the query_string : It is possible to put parameters in the query_string within the URL in order to obtain sub-parts of graphical features easily integrated in web pages (see web portal .) Parameters that can be specified in the query-string as described below. Parameter Description Example subset selection the data subsets. subset=activome,qNMR_metabo tab specifies which analysis tab will be selected; the possible values are: univariate, bivariate, multiunivariate and multivariate tab=univariate type specifies which type of analysis will be selected; Only works for multivariate analysis; the possible values are: PCA,ICA,COR and GGM type=PCA fac, fac1, fac2 specifies which fator will be selected; fac or fac1 for factor X, fac2 for factor Y depending on the analysis fac1=stage var, var1, var2 specifies which variable will be selected; var for univariate only, var1 and var2 for bivariate only var1=glucose&var2=fructose lev1, lev2 specifies which levels will be selected for volcano plot, i.e lev2 vs lev1 lev1=Control&lev2=Shaded frame indicates if the main frame is displayed (default) or not frame=off fup,fdwn indicates whether the top and bottom frames of the analysis will be displayed (by default) or not. Only works if the main frame is disabled. fup=off Examples : Example 1 : Univariate analysis on the variable 'glucose' from the data subset 'qNMR_metabo' based on the factor 'stage'; No main frame and no bottom frame . Query string: ?ds=frim1&subset=qNMR_metabo&tab=univariate&fac=stage&var=glucose&frame=off&fdwn=off Example 2 : PCA analysis on the data subset 'activome' based on the factor 'stage'; No main frame and no top and bottom frames . Query string: ?ds=frim1&subset=activome&tab=multivariate&type=PCA&fac=stage&frame=off&fup=off&fdwn=off Data exporting into a spreadsheet : Finally, an interesting possibility is that you can make a selection of a subset of data and then export it directly into your spreadsheet. Export a previously selected data subset","title":"Data Explorer"},{"location":"data-explorer/#odam-data-explorer","text":"Data view : The data explorer offers a complete view of all data and associated metadata. On a single page, all the information about the dataset is gathered. In addition a graph of the relationships between the datasets is provided. Example online : https://pmb-bordeaux.fr/dataexplorer/?ds=frim1 Dataset information Data analysis : The Data Explorer makes also data easy to explore, visualize, and subsequently to better understand the data as a whole. Explore your data in several ways according to your concerns by interacting with the graphs. For instance, univariate, bivariate, multi-univariate (volcano plot) and multivariate (PCA, ICA, t-SNE, COR, GGM) approaches have been implemented so that they are very easy to be interactively used. This is very useful in order to have a first glimpse of the data that can show trends and this allows the data to be well characterized, which is necessary to then choose how to deeply analyze them later on. For each analysis, several possibilities can be explored by selecting many parameters and interacting with the graphs as illustrated below. Example of univariate analysis Example of multivariate analysis Data merging : One or more subsets of data can be selected to be combined in bivariate and multivariate analysis. Save graphics : Graphics can be easily exported as PNG/SVG files. Parameters in the query_string : It is possible to put parameters in the query_string within the URL in order to obtain sub-parts of graphical features easily integrated in web pages (see web portal .) Parameters that can be specified in the query-string as described below. Parameter Description Example subset selection the data subsets. subset=activome,qNMR_metabo tab specifies which analysis tab will be selected; the possible values are: univariate, bivariate, multiunivariate and multivariate tab=univariate type specifies which type of analysis will be selected; Only works for multivariate analysis; the possible values are: PCA,ICA,COR and GGM type=PCA fac, fac1, fac2 specifies which fator will be selected; fac or fac1 for factor X, fac2 for factor Y depending on the analysis fac1=stage var, var1, var2 specifies which variable will be selected; var for univariate only, var1 and var2 for bivariate only var1=glucose&var2=fructose lev1, lev2 specifies which levels will be selected for volcano plot, i.e lev2 vs lev1 lev1=Control&lev2=Shaded frame indicates if the main frame is displayed (default) or not frame=off fup,fdwn indicates whether the top and bottom frames of the analysis will be displayed (by default) or not. Only works if the main frame is disabled. fup=off Examples : Example 1 : Univariate analysis on the variable 'glucose' from the data subset 'qNMR_metabo' based on the factor 'stage'; No main frame and no bottom frame . Query string: ?ds=frim1&subset=qNMR_metabo&tab=univariate&fac=stage&var=glucose&frame=off&fdwn=off Example 2 : PCA analysis on the data subset 'activome' based on the factor 'stage'; No main frame and no top and bottom frames . Query string: ?ds=frim1&subset=activome&tab=multivariate&type=PCA&fac=stage&frame=off&fup=off&fdwn=off Data exporting into a spreadsheet : Finally, an interesting possibility is that you can make a selection of a subset of data and then export it directly into your spreadsheet. Export a previously selected data subset","title":"ODAM: Data Explorer"},{"location":"dataset-example/","text":"ODAM: A dataset example \u00b6 The FRIM dataset \u00b6 Fruit Integrative Modelling, an ERASysBio+ project : Yves Gibon (Coordinator) The project aimed to build a virtual tomato fruit that enables the prediction of metabolite levels given genetic and environmental inputs, by an iterative process between laboratories which combine expertise in fruit biology, ecophysiology, theoretical and experimental biochemistry, and biotechnology. Purposes of the project To build a kinetic model encompassing the routes carbon takes, once imported into the fruit cells from the source organs of the mother plant. To integrate the kinetic model with a phenomenological model predicting sugar and organic acid contents as functions of time, light intensity, temperature and water availability. To obtain large-scale experimental measures of the consequences of altered environmental conditions. To assess the influence of the environment on fruit metabolism, tomato ( Solanum lycopersicum 'Moneymaker' ) plants were grown under contrasting conditions (optimal for commercial, shaded production) and locations. Samples were harvested at nine stages of development, and 36 enzyme activities of central metabolism were measured as well as protein, starch, and major metabolites, such as hexoses, sucrose, organic acids, and amino acids. Experimental data tables from the Frim project ('frim1' study) About 580 tomato plants were grown in a greenhouse in the southwest of France ( Sainte-Livrade sur Lot ) during the summer of 2010 according to usual production practices. Links related to the dataset \u00b6 Description Link Data INRAE (*) https://doi.org/10.15454/95JUTK Data explorer (*) https://pmb-bordeaux.fr/dataexplorer/?ds=frim1 Modeling the growth of tomato fruits based on enzyme activity profiles : An example of data analysis interfaced by ODAM https://hal-cnrs.archives-ouvertes.fr/hal-02611223/file/FRIM1_Growth_model.html Jupyter notebooks (R & Python) https://nbviewer.jupyter.org/github/djacob65/binder_odam/tree/master/ (*) Both repositories are supported by INRAE (France) for a minimum period of 10 years (until 2030) Publication of the dataset according to FAIR principles \u00b6 Data publishing Because ODAM is primarily an Experimental Data Table Management System (EDTMS) for data sharing, it must be associated with a suitable data repository in order to support data publishing. So the ODAM approach has to be regarded as complementary with publication of the data online within an institutional data repository as described in re3data.org (e.g. Data INRAE ) associated or not with a scientific paper. To be compliant with the FAIR principles , not all data, documents, workflows and other tools need to be located in a single system, but from a central repository, it is the set of links that constitutes the true information management system. It must be able to be traversed by a human being as well as by machines. Dataset information need to be linked but not necessary in the same repository Data standardisation Data INRAE repository as a hub (based on Dataverse ) allows to interconnect the different elements of the FRIM dataset. A file named ' datapackage.json ', structured according to an explicit schema, was generated based on the structural metadata previously defined with the help of spreadsheets. By relying on explicit schemas ( JSON-LD , JSON Schema ) for both metadata and data, this makes it possible for both humans and machines to reuse data without friction.. Data INRAE repository as a hub (based on Dataverse) An explicit schema allows to define structural metadata along with unambiguous definitions of all internal elements (e.g. column definitions, units of measurement), through links to accessible (standard) definitions. Thus, this results in better annotated and more easily usable data that meets effortlessly the FAIR criteria for reusability. Indeed, structured data using a discoverable, community-endorsed schema or data model can only have a positive impact on the FAIR criteria 'Interoperable' and 'Reusable'. Data referencing In addition, a good practice for referencing datasets on the Internet is to manage URIs instead of URLs. A URI is a string of characters that unambiguously identifies a particular resource. It consists of a prefix linked to the resource and an identifier within the resource. For this purpose, the ODAM repository was registered at the central registry Identifiers.org 1 . Its URI consists of the prefix 'odam' followed by the short name of the dataset, i.e. odam:<dataset> . The resource pointed to by this URI after de-reification is precisely the structural metadata returned according to the JSON datapackage schema. URI of the Frim1 dataset: odam:frim1 which can be de-reified using the identifiers.org registry : - https://identifiers.org/odam:frim1 See also for more details: ODAM data-package based on JSON-Schema ODAM data-package from a Dataverse repository Find a Dataverse dataset via Google Search FAIR grids applied on FRIM dataset References \u00b6 Biais B, B\u00e9nard C, Beauvoit B, Colombi\u00e9 S, Prodhomme D, M\u00e9nard G, Bernillon S, Gehl B, Gautier H, Ballias P, Mazat J-P, Sweetlove L, G\u00e9nard M, Gibon Y. 2014. Remarkable reproducibility of enzyme activity profiles in tomato fruits grown under contrasting environments provides a roadmap for studies of fruit metabolism. Plant Physiology 164, 1204-1221. doi: 10.1104/pp.113.231241 Nick Juty, Nicolas Le Nov\u00e8re, Camille Laibe, Identifiers.org and MIRIAM Registry: community resources to provide persistent identification, Nucleic Acids Research, Volume 40, Issue D1, 1 January 2012, Pages D580\u2013D586, https://doi.org/10.1093/nar/gkr1097 \u21a9","title":"A dataset example"},{"location":"dataset-example/#odam-a-dataset-example","text":"","title":"ODAM: A dataset example"},{"location":"dataset-example/#the-frim-dataset","text":"Fruit Integrative Modelling, an ERASysBio+ project : Yves Gibon (Coordinator) The project aimed to build a virtual tomato fruit that enables the prediction of metabolite levels given genetic and environmental inputs, by an iterative process between laboratories which combine expertise in fruit biology, ecophysiology, theoretical and experimental biochemistry, and biotechnology. Purposes of the project To build a kinetic model encompassing the routes carbon takes, once imported into the fruit cells from the source organs of the mother plant. To integrate the kinetic model with a phenomenological model predicting sugar and organic acid contents as functions of time, light intensity, temperature and water availability. To obtain large-scale experimental measures of the consequences of altered environmental conditions. To assess the influence of the environment on fruit metabolism, tomato ( Solanum lycopersicum 'Moneymaker' ) plants were grown under contrasting conditions (optimal for commercial, shaded production) and locations. Samples were harvested at nine stages of development, and 36 enzyme activities of central metabolism were measured as well as protein, starch, and major metabolites, such as hexoses, sucrose, organic acids, and amino acids. Experimental data tables from the Frim project ('frim1' study) About 580 tomato plants were grown in a greenhouse in the southwest of France ( Sainte-Livrade sur Lot ) during the summer of 2010 according to usual production practices.","title":"The FRIM dataset"},{"location":"dataset-example/#links-related-to-the-dataset","text":"Description Link Data INRAE (*) https://doi.org/10.15454/95JUTK Data explorer (*) https://pmb-bordeaux.fr/dataexplorer/?ds=frim1 Modeling the growth of tomato fruits based on enzyme activity profiles : An example of data analysis interfaced by ODAM https://hal-cnrs.archives-ouvertes.fr/hal-02611223/file/FRIM1_Growth_model.html Jupyter notebooks (R & Python) https://nbviewer.jupyter.org/github/djacob65/binder_odam/tree/master/ (*) Both repositories are supported by INRAE (France) for a minimum period of 10 years (until 2030)","title":"Links related to the dataset"},{"location":"dataset-example/#publication-of-the-dataset-according-to-fair-principles","text":"Data publishing Because ODAM is primarily an Experimental Data Table Management System (EDTMS) for data sharing, it must be associated with a suitable data repository in order to support data publishing. So the ODAM approach has to be regarded as complementary with publication of the data online within an institutional data repository as described in re3data.org (e.g. Data INRAE ) associated or not with a scientific paper. To be compliant with the FAIR principles , not all data, documents, workflows and other tools need to be located in a single system, but from a central repository, it is the set of links that constitutes the true information management system. It must be able to be traversed by a human being as well as by machines. Dataset information need to be linked but not necessary in the same repository Data standardisation Data INRAE repository as a hub (based on Dataverse ) allows to interconnect the different elements of the FRIM dataset. A file named ' datapackage.json ', structured according to an explicit schema, was generated based on the structural metadata previously defined with the help of spreadsheets. By relying on explicit schemas ( JSON-LD , JSON Schema ) for both metadata and data, this makes it possible for both humans and machines to reuse data without friction.. Data INRAE repository as a hub (based on Dataverse) An explicit schema allows to define structural metadata along with unambiguous definitions of all internal elements (e.g. column definitions, units of measurement), through links to accessible (standard) definitions. Thus, this results in better annotated and more easily usable data that meets effortlessly the FAIR criteria for reusability. Indeed, structured data using a discoverable, community-endorsed schema or data model can only have a positive impact on the FAIR criteria 'Interoperable' and 'Reusable'. Data referencing In addition, a good practice for referencing datasets on the Internet is to manage URIs instead of URLs. A URI is a string of characters that unambiguously identifies a particular resource. It consists of a prefix linked to the resource and an identifier within the resource. For this purpose, the ODAM repository was registered at the central registry Identifiers.org 1 . Its URI consists of the prefix 'odam' followed by the short name of the dataset, i.e. odam:<dataset> . The resource pointed to by this URI after de-reification is precisely the structural metadata returned according to the JSON datapackage schema. URI of the Frim1 dataset: odam:frim1 which can be de-reified using the identifiers.org registry : - https://identifiers.org/odam:frim1 See also for more details: ODAM data-package based on JSON-Schema ODAM data-package from a Dataverse repository Find a Dataverse dataset via Google Search FAIR grids applied on FRIM dataset","title":"Publication of the dataset according to FAIR principles"},{"location":"dataset-example/#references","text":"Biais B, B\u00e9nard C, Beauvoit B, Colombi\u00e9 S, Prodhomme D, M\u00e9nard G, Bernillon S, Gehl B, Gautier H, Ballias P, Mazat J-P, Sweetlove L, G\u00e9nard M, Gibon Y. 2014. Remarkable reproducibility of enzyme activity profiles in tomato fruits grown under contrasting environments provides a roadmap for studies of fruit metabolism. Plant Physiology 164, 1204-1221. doi: 10.1104/pp.113.231241 Nick Juty, Nicolas Le Nov\u00e8re, Camille Laibe, Identifiers.org and MIRIAM Registry: community resources to provide persistent identification, Nucleic Acids Research, Volume 40, Issue D1, 1 January 2012, Pages D580\u2013D586, https://doi.org/10.1093/nar/gkr1097 \u21a9","title":"References"},{"location":"fair-frim/","text":"ODAM: FAIR grids \u00b6 FAIR grids applied on FRIM dataset \u00b6 Three FAIR grids avery different from each other. 1- 5 \u2605 Data Rating Tool From OZONOME , it aims to carry out an evaluation based on the FAIR principles as defined by Willkinson et al (1) . The main output is a global rating, indicating the global FAIRness of the dataset. It provides implementations of the FORCE 11 FAIR data principles . 2 - FDMM (FAIR Data Maturity Model) (2) FDMM is a recognized and endorsed working group within RDA (Research Data Alliance). They produce a document that describes a maturity model for FAIR assessment with assessment indicators, priorities and evaluation methods, useful for the normalisation of assessment approaches to enable comparison of their results. 3 - SHARC (Sharing Rewards and Credit) (3) SHARC is a recognized and endorsed working group within RDA (Research Data Alliance). They produce a document that allows assessing FAIRness of projects and related human processes by either external evaluators or the researchers themselves, implying to implement simple FAIRness assessment in various communities and identify procedures and training that must be deployed and adapted to their practices and level of understanding. Results \u00b6 5 \u2605 Data Rating Tool Summary table https://oznome.csiro.au/5star/?view=5ec2a9654d0983adde57a21e FDMM (FAIR Data Maturity Model) https://drive.google.com/file/d/1a520Cbu8bryEeZIPI3h1l6zkaO7MZ39-/view?usp=sharing\" SHARC (Sharing Rewards and Credit) https://drive.google.com/file/d/1uif-jy9QBno_WPnpGL14LFpDzL366tMH/view?usp=sharing Synthesis of FAIR evaluation grids applied to the Frim dataset The Fair Data Maturity Model (FDMM) document (A) describes a maturity model for the FAIR assessment with indicators, priorities and assessment methods, which are useful for standardizing assessment approaches in order to allow comparison of their results. Whereas the FAIR SHARC (SHAring Rewards and Credit) (B) document allows the fairness of projects and associated human processes to be assessed, either by external evaluators or by the researchers themselves. Therefore, these grids cannot be compared with each other, but rather complement each other. Summary table of essential FAIR criteria based on force11.org, applied to the Frim dataset References \u00b6 Wilkinson, M., Dumontier, M., Aalbersberg, I. et al. (2016) The FAIR Guiding Principles for scientific data management and stewardship. Sci Data 3, 160018. doi:10.1038/sdata.2016.18 RDA FAIR Data Maturity Model Working Group (2020). FAIR Data Maturity Model: specification and guidelines. Research Data Alliance. DOI: 10.15497/RDA00045 David, R, Mabile, L, Specht, A, Stryeck, S, Thomsen, M, Yahia, M, Jonquet, C, Doll\u00e9, L, Jacob, D, Bailo, D, Bravo, E, Gachet, S, Gunderman, H, Hollebecq, J-E, Ioannidis, V, Bras, YL, Lerigoleur, E, Cambon-Thomsen, A and The Research Data Alliance \u2013 SHAring Reward and Credit (SHARC) Interest Group. 2020. FAIRness Literacy: The Achilles\u2019 Heel of Applying FAIR Principles. Data Science Journal, 19: 32, pp. 1\u201311. DOI: https://doi.org/10.5334/dsj-2020-032","title":"FAIR_on_frim"},{"location":"fair-frim/#odam-fair-grids","text":"","title":"ODAM: FAIR grids"},{"location":"fair-frim/#fair-grids-applied-on-frim-dataset","text":"Three FAIR grids avery different from each other. 1- 5 \u2605 Data Rating Tool From OZONOME , it aims to carry out an evaluation based on the FAIR principles as defined by Willkinson et al (1) . The main output is a global rating, indicating the global FAIRness of the dataset. It provides implementations of the FORCE 11 FAIR data principles . 2 - FDMM (FAIR Data Maturity Model) (2) FDMM is a recognized and endorsed working group within RDA (Research Data Alliance). They produce a document that describes a maturity model for FAIR assessment with assessment indicators, priorities and evaluation methods, useful for the normalisation of assessment approaches to enable comparison of their results. 3 - SHARC (Sharing Rewards and Credit) (3) SHARC is a recognized and endorsed working group within RDA (Research Data Alliance). They produce a document that allows assessing FAIRness of projects and related human processes by either external evaluators or the researchers themselves, implying to implement simple FAIRness assessment in various communities and identify procedures and training that must be deployed and adapted to their practices and level of understanding.","title":"FAIR grids applied on FRIM dataset"},{"location":"fair-frim/#results","text":"5 \u2605 Data Rating Tool Summary table https://oznome.csiro.au/5star/?view=5ec2a9654d0983adde57a21e FDMM (FAIR Data Maturity Model) https://drive.google.com/file/d/1a520Cbu8bryEeZIPI3h1l6zkaO7MZ39-/view?usp=sharing\" SHARC (Sharing Rewards and Credit) https://drive.google.com/file/d/1uif-jy9QBno_WPnpGL14LFpDzL366tMH/view?usp=sharing Synthesis of FAIR evaluation grids applied to the Frim dataset The Fair Data Maturity Model (FDMM) document (A) describes a maturity model for the FAIR assessment with indicators, priorities and assessment methods, which are useful for standardizing assessment approaches in order to allow comparison of their results. Whereas the FAIR SHARC (SHAring Rewards and Credit) (B) document allows the fairness of projects and associated human processes to be assessed, either by external evaluators or by the researchers themselves. Therefore, these grids cannot be compared with each other, but rather complement each other. Summary table of essential FAIR criteria based on force11.org, applied to the Frim dataset","title":"Results"},{"location":"fair-frim/#references","text":"Wilkinson, M., Dumontier, M., Aalbersberg, I. et al. (2016) The FAIR Guiding Principles for scientific data management and stewardship. Sci Data 3, 160018. doi:10.1038/sdata.2016.18 RDA FAIR Data Maturity Model Working Group (2020). FAIR Data Maturity Model: specification and guidelines. Research Data Alliance. DOI: 10.15497/RDA00045 David, R, Mabile, L, Specht, A, Stryeck, S, Thomsen, M, Yahia, M, Jonquet, C, Doll\u00e9, L, Jacob, D, Bailo, D, Bravo, E, Gachet, S, Gunderman, H, Hollebecq, J-E, Ioannidis, V, Bras, YL, Lerigoleur, E, Cambon-Thomsen, A and The Research Data Alliance \u2013 SHAring Reward and Credit (SHARC) Interest Group. 2020. FAIRness Literacy: The Achilles\u2019 Heel of Applying FAIR Principles. Data Science Journal, 19: 32, pp. 1\u201311. DOI: https://doi.org/10.5334/dsj-2020-032","title":"References"},{"location":"install/","text":"ODAM: Installation \u00b6 The ODAM software can be deployed at multiple scales (local, intranet, internet), depending on the need and the target community. graph LR A([ODAM installation]) --> B{MS<br>Windows 10} B -->|Yes| C{Full version} C -->|No| E(ODAMwebserver<br>Web API) C -->|Yes| D(VirtualBox VM) B -->|No| G{MacOS 10.x} G -->|yes| D G -->|yes| F(Docker containers) G -->|No| F Installation flow chart according to your operating system Local installation : Deployment of ODAM with full functionality \u00b6 This type of deployment is reserved more specifically for computers running under MS Windows 10 and Apple MacOS 10.x. All the steps involved in the installation of the virtual machine have been published and are available online on the INRAE Data website. ODAM Virtual Disk Image (VDI) for Oracle VM VirtualBox - Zipped with 7zip. https://doi.org/10.15454/C9LAEF , Portail Data INRAE, V1 The three main steps are shown below: Overview of the ODAM software deployment on your PC (Windows 10, MacOS 10.x) HOWTO \u00b6 Download the virtual machine file, to be unzipped with 7zip . Then follow the installation guide mentioned above. This requires prior installation of Oracle VM VirtualBox software Data collection and preparation Using ODAM Local installation : ODAM API on Windows \u00b6 Deploy the ODAM API with a very lightweight local web server for Windows. For Windows users, ODAMwebserver offers a simple, fast and efficient way to deploy the ODAM API layer locally in (almost) one click. ODAM API using Apache Web Server ODAMwebserver.zip Overview of the ODAM API deployment (Windows 10) HOWTO \u00b6 Download and Unzip ODAMwebserver.zip to the location of your choice on your hard drive Go to the directory ODAMwebserver\\settings , then with the notepad application for example, edit the getdata.conf file. In the line starting with DATAROOT , specify the full path of the root of the data directory (be careful, it is necessary to put slashes \"/\" and not backslashes \"\\\") DATAROOT = C:/PATH/TO/YOUR/DATA/ Save the configuration file, then exit Run WebServer.bat . A small window opens, indicating whether the Apache server is running or not. At the first run, Windows should ask you for permission to pass through the firewall. Validate the authorization so that the web server can function properly. Also, if the system tells you that the libssh2.dll file is missing, don't worry, it will still work. In your browser, go To http://localhost/ to view your site root and to http://localhost/getdata/xml/ <dataset> to view the <dataset> where <dataset> correspondings to your dataset put under the root of the data directory. For testing, you could download the FRIM1 dataset (choose frim1.zip ), then unzip the download file under the DATAROOT folder. In your browser, go To http://localhost/getdata/xml/frim1 Note : For MacOS (10.13 or newer), you can easily install Docker , then proceed as described in the next section \"Intranet/Internet installation\". Intranet/Internet installation \u00b6 This type of deployment is reserved more specifically for computers or IT infrastructures running Linux (including MacOS 10.13 or newer) which allow the deployment of Docker containers. See for more details on \u201cWhat is Docker\u201d on opensource.com. Note : Depending on the scale you want to deploy, your intranet can be local or via a VPN ( Virtual Private Network ) and for the internet, a machine on an institutional data center (e.g. university) is better but a machine on a cloud computing is a perfectly feasible solution. All the steps to deploy the Docker containers corresponding to the ODAM software have been put online : Github https://github.com/inrae/ODAM DockerHub https://hub.docker.com/r/odam/getdata/ Remark : Installing one's own ODAM instance on an institutional server for example allows in this way experimental data tables to be widely accessible and fully reusable including through scripting languages such as R or Python , and this with minimal effort on the part of the data provider. Thus, the web of data could be seen as a data network on the web, based on appropriate technologies ( Web API ), and using standard data formats ( TSV , JSON ). Web applications, each with a clearly defined objective, then operate this network. A data can therefore be used for several applications and vice versa. The data management system becomes completely independent of its operation. The data is thus \u201cdecompartmentalized\", a sine qua non condition for the Web of Data.","title":"Installation"},{"location":"install/#odam-installation","text":"The ODAM software can be deployed at multiple scales (local, intranet, internet), depending on the need and the target community. graph LR A([ODAM installation]) --> B{MS<br>Windows 10} B -->|Yes| C{Full version} C -->|No| E(ODAMwebserver<br>Web API) C -->|Yes| D(VirtualBox VM) B -->|No| G{MacOS 10.x} G -->|yes| D G -->|yes| F(Docker containers) G -->|No| F Installation flow chart according to your operating system","title":"ODAM: Installation"},{"location":"install/#local-installation-deployment-of-odam-with-full-functionality","text":"This type of deployment is reserved more specifically for computers running under MS Windows 10 and Apple MacOS 10.x. All the steps involved in the installation of the virtual machine have been published and are available online on the INRAE Data website. ODAM Virtual Disk Image (VDI) for Oracle VM VirtualBox - Zipped with 7zip. https://doi.org/10.15454/C9LAEF , Portail Data INRAE, V1 The three main steps are shown below: Overview of the ODAM software deployment on your PC (Windows 10, MacOS 10.x)","title":"Local installation : Deployment of ODAM with full functionality"},{"location":"install/#howto","text":"Download the virtual machine file, to be unzipped with 7zip . Then follow the installation guide mentioned above. This requires prior installation of Oracle VM VirtualBox software Data collection and preparation Using ODAM","title":"HOWTO"},{"location":"install/#local-installation-odam-api-on-windows","text":"Deploy the ODAM API with a very lightweight local web server for Windows. For Windows users, ODAMwebserver offers a simple, fast and efficient way to deploy the ODAM API layer locally in (almost) one click. ODAM API using Apache Web Server ODAMwebserver.zip Overview of the ODAM API deployment (Windows 10)","title":"Local installation : ODAM API on Windows"},{"location":"install/#howto_1","text":"Download and Unzip ODAMwebserver.zip to the location of your choice on your hard drive Go to the directory ODAMwebserver\\settings , then with the notepad application for example, edit the getdata.conf file. In the line starting with DATAROOT , specify the full path of the root of the data directory (be careful, it is necessary to put slashes \"/\" and not backslashes \"\\\") DATAROOT = C:/PATH/TO/YOUR/DATA/ Save the configuration file, then exit Run WebServer.bat . A small window opens, indicating whether the Apache server is running or not. At the first run, Windows should ask you for permission to pass through the firewall. Validate the authorization so that the web server can function properly. Also, if the system tells you that the libssh2.dll file is missing, don't worry, it will still work. In your browser, go To http://localhost/ to view your site root and to http://localhost/getdata/xml/ <dataset> to view the <dataset> where <dataset> correspondings to your dataset put under the root of the data directory. For testing, you could download the FRIM1 dataset (choose frim1.zip ), then unzip the download file under the DATAROOT folder. In your browser, go To http://localhost/getdata/xml/frim1 Note : For MacOS (10.13 or newer), you can easily install Docker , then proceed as described in the next section \"Intranet/Internet installation\".","title":"HOWTO"},{"location":"install/#intranetinternet-installation","text":"This type of deployment is reserved more specifically for computers or IT infrastructures running Linux (including MacOS 10.13 or newer) which allow the deployment of Docker containers. See for more details on \u201cWhat is Docker\u201d on opensource.com. Note : Depending on the scale you want to deploy, your intranet can be local or via a VPN ( Virtual Private Network ) and for the internet, a machine on an institutional data center (e.g. university) is better but a machine on a cloud computing is a perfectly feasible solution. All the steps to deploy the Docker containers corresponding to the ODAM software have been put online : Github https://github.com/inrae/ODAM DockerHub https://hub.docker.com/r/odam/getdata/ Remark : Installing one's own ODAM instance on an institutional server for example allows in this way experimental data tables to be widely accessible and fully reusable including through scripting languages such as R or Python , and this with minimal effort on the part of the data provider. Thus, the web of data could be seen as a data network on the web, based on appropriate technologies ( Web API ), and using standard data formats ( TSV , JSON ). Web applications, each with a clearly defined objective, then operate this network. A data can therefore be used for several applications and vice versa. The data management system becomes completely independent of its operation. The data is thus \u201cdecompartmentalized\", a sine qua non condition for the Web of Data.","title":"Intranet/Internet installation"},{"location":"presentation/","text":"ODAM: Presentation \u00b6 ODAM (Open Data for Access and Mining) is an Experiment Data Table Management System (EDTMS) Background \u00b6 In life sciences, (and particularly in plant sciences), each time an experimental design ( DoE ) is implemented, we can, very schematically, represent the data flow according to 4 main steps, from raw data to publication of results. Very Schematically one can represent the data flow according to 4 main steps, from raw data to publication of results. What kind of data From a biological point of view, the data integrating the maximum amount of relevant information are those resulting from the pre-processing of so-called raw data (resulting from analytical techniques) and including annotations with curation then validated by one or more experts; i.e. those involving a transformation of analytical variables (peaks or resonances on spectra, locus on a DNA / RNA / Protein sequence, ...) into biological variables (metabolites, proteins, genes, ...). At this stage, because they are not synthesized, they still have all their variabilities (technological and biological replicas on all factorial levels) and therefore have more potential for reuse. Moreover, these data are not automatically reproducible (e.g. via workflows) because they require human expertise (i.e. know-how). This is why we have focused our data management on these experimental data tables . Data handling When generating data in an experiment involving several types of data from several analytical techniques, and this for the same samples, the task of being able to easily link these different data on the basis of sample identifiers is crucial. Indeed, as data consistency must be ensured throughout the study , it should be ensured that it is not necessary to conduct a laborious survey to find the right identifiers, but to rely confidently on a data management system . Linking each analysis to its corresponding sample throughout the study without laborious investigation or data handling is crucial to ensure data consistency. Data sharing Each time we plan to share data coming from a common experimental design, the classical challenges for fast using data by every partner are data storage and data access. We propose an approach for sharing project data all along its development phase , from the setup of the experimental schema up to the data acquisition from the various analyzes of samples, so that all data is readily available as soon as they are generated . Proposed solution \u00b6 ODAM software is designed to manage experimental data tables in a quick and easy way for users. There is no need to develop a complex data model. Just complete the data with some structural metadata . These structural metadata will be used first to make full use of the data as soon as they are produced and formatted and then to annotate the dataset for later dissemination, either to project partners or more widely. The core idea in one shot The central idea which has been the founding idea of ODAM, is that data producers \"just\" have to drag and drop their data tables onto a storage space, which depending on the chosen infrastructure can be local (i.e. their PC, or a NAS) or remote (virtual disk space). So simply dropping data files (along with two additional metadata files) on the storage space allows users to access them through web services. This means there is no need for additional configuration on the server. ODAM proposes to meet certain needs typically encountered during the implementation of an experimental design in life science including several different analyses of the same sample. Data collecting and preparation The formatting of all the data and matching the data from the different analyses with their experimental context can be a long step. Tasks such as collecting and preparing data in order to combine several data sources require a lot of long, repetitive and tedious data handling. Similarly, when modeling, subsets must be selected and then many scenarios with different parameters must be tested. ( e.g data scientists spend 79% of their time on data preparation 1 ) Data sharing Enabling centralized management of identifiers (e.g. plants, crops, samples, etc.) so that they are unique and shared by all project members. Indeed, as each biological sample is most often aliquoted and then sent for analysis by different techniques, the data returned in tabular form must be able to be linked to the other data according to the identifiers of the samples. Giving access to data for rapid use by each project member and this throughout the development phase, from the implementation of the experimental design to the acquisition of data from the various sample analyses, so that all data are readily available as soon as they are generated. Data publishing To be able to publish one's data without a colossal effort in data curation , and without the need for data archaeology . To be able to publish one's data according to the FAIR principles , at least the essentials. To facilitate the reuse of data by providing structural metadata , thus avoiding that data consumers spend a disproportionate amount of time trying to understand the digital resources they need and devising specific ways to combine them. ODAM sofware suite allows experimental data tables to be widely accessible and fully reusable and this with minimal effort on the part of the data provider. Easily structure your data by adding structural metadata so that you can first exploit it locally yourself, before sharing it more widely just as easily. make research data locally or broadly accessible all along the project allow data to be selected then, downloadable by web API allow data and analysis to be visualized online Based on the following criteria: Centralized management of identifiers (individuals, samples, aliquots, etc ...) so that they are unique and shared by all Avoid the implementation of a complex data management system (requiring a data model) given that many changes can occur during the project. (possibility of new analysis, new measures or give up some others, ...) Facilitates the subsequent publication of data: either the data can serve to fill in an existing database or the data can be broadcast through a web-service approach with the associated metadata. ODAM can be seen as a wheel ... Advantages of this approach According to us the most important thing is to capture structural metadata as early as possible. The objective of our approach is to make this upstream capture an advantage to facilitate data analysis and therefore an incentive to perform this metadata capture. However on the basis of metadata files this does not prevent the development of parsers/converters to other formats or a connection to a more complex data model (e.g. ISA-TAB , MIAPPE 1.1 ) in order to include the data (transformed or not) in existing standards-compliant data infrastructures (e.g. SEEK platform ) Guideline for good data management ODAM's general philosophy is to consider that the data life cycle must be integrated into the scientific research process. Furthermore, by considering the FAIR principles as a guide to produce data that is reproducible and frictionless reusable by humans and machines, data FAIRification process is accomplished without insurmountable effort ( FAIR by design ) For this work, we made the choice to keep the good old way of scientist to use worksheets, thus using the same tool for both data files and metadata definition files. The ODAM software can be deployed at multiple scales (local, intranet, internet), depending on the need and the target community. Guideline keywords : experimental data tables , Research Data Management , OpenAPI , FAIR Data principles , FAIR assessment Documentation \u00b6 Presentation : A presentation on the ODAM software, its aims and what can we do with it for what purposes. FAIR_and_DataLife_DJ_Oct2019.pdf Presentation : How to best manage your data to make the most of it for your research Make your data great now Presentation : A presentation about Data Management in the context of Open Science. Research Data Management CrowdFlower, Data Science Report (2016) https://visit.figure-eight.com/rs/416-ZBE-142/images/CrowdFlower_DataScienceReport_2016.pdf \u21a9","title":"Presentation"},{"location":"presentation/#odam-presentation","text":"ODAM (Open Data for Access and Mining) is an Experiment Data Table Management System (EDTMS)","title":"ODAM: Presentation"},{"location":"presentation/#background","text":"In life sciences, (and particularly in plant sciences), each time an experimental design ( DoE ) is implemented, we can, very schematically, represent the data flow according to 4 main steps, from raw data to publication of results. Very Schematically one can represent the data flow according to 4 main steps, from raw data to publication of results. What kind of data From a biological point of view, the data integrating the maximum amount of relevant information are those resulting from the pre-processing of so-called raw data (resulting from analytical techniques) and including annotations with curation then validated by one or more experts; i.e. those involving a transformation of analytical variables (peaks or resonances on spectra, locus on a DNA / RNA / Protein sequence, ...) into biological variables (metabolites, proteins, genes, ...). At this stage, because they are not synthesized, they still have all their variabilities (technological and biological replicas on all factorial levels) and therefore have more potential for reuse. Moreover, these data are not automatically reproducible (e.g. via workflows) because they require human expertise (i.e. know-how). This is why we have focused our data management on these experimental data tables . Data handling When generating data in an experiment involving several types of data from several analytical techniques, and this for the same samples, the task of being able to easily link these different data on the basis of sample identifiers is crucial. Indeed, as data consistency must be ensured throughout the study , it should be ensured that it is not necessary to conduct a laborious survey to find the right identifiers, but to rely confidently on a data management system . Linking each analysis to its corresponding sample throughout the study without laborious investigation or data handling is crucial to ensure data consistency. Data sharing Each time we plan to share data coming from a common experimental design, the classical challenges for fast using data by every partner are data storage and data access. We propose an approach for sharing project data all along its development phase , from the setup of the experimental schema up to the data acquisition from the various analyzes of samples, so that all data is readily available as soon as they are generated .","title":"Background"},{"location":"presentation/#proposed-solution","text":"ODAM software is designed to manage experimental data tables in a quick and easy way for users. There is no need to develop a complex data model. Just complete the data with some structural metadata . These structural metadata will be used first to make full use of the data as soon as they are produced and formatted and then to annotate the dataset for later dissemination, either to project partners or more widely. The core idea in one shot The central idea which has been the founding idea of ODAM, is that data producers \"just\" have to drag and drop their data tables onto a storage space, which depending on the chosen infrastructure can be local (i.e. their PC, or a NAS) or remote (virtual disk space). So simply dropping data files (along with two additional metadata files) on the storage space allows users to access them through web services. This means there is no need for additional configuration on the server. ODAM proposes to meet certain needs typically encountered during the implementation of an experimental design in life science including several different analyses of the same sample. Data collecting and preparation The formatting of all the data and matching the data from the different analyses with their experimental context can be a long step. Tasks such as collecting and preparing data in order to combine several data sources require a lot of long, repetitive and tedious data handling. Similarly, when modeling, subsets must be selected and then many scenarios with different parameters must be tested. ( e.g data scientists spend 79% of their time on data preparation 1 ) Data sharing Enabling centralized management of identifiers (e.g. plants, crops, samples, etc.) so that they are unique and shared by all project members. Indeed, as each biological sample is most often aliquoted and then sent for analysis by different techniques, the data returned in tabular form must be able to be linked to the other data according to the identifiers of the samples. Giving access to data for rapid use by each project member and this throughout the development phase, from the implementation of the experimental design to the acquisition of data from the various sample analyses, so that all data are readily available as soon as they are generated. Data publishing To be able to publish one's data without a colossal effort in data curation , and without the need for data archaeology . To be able to publish one's data according to the FAIR principles , at least the essentials. To facilitate the reuse of data by providing structural metadata , thus avoiding that data consumers spend a disproportionate amount of time trying to understand the digital resources they need and devising specific ways to combine them. ODAM sofware suite allows experimental data tables to be widely accessible and fully reusable and this with minimal effort on the part of the data provider. Easily structure your data by adding structural metadata so that you can first exploit it locally yourself, before sharing it more widely just as easily. make research data locally or broadly accessible all along the project allow data to be selected then, downloadable by web API allow data and analysis to be visualized online Based on the following criteria: Centralized management of identifiers (individuals, samples, aliquots, etc ...) so that they are unique and shared by all Avoid the implementation of a complex data management system (requiring a data model) given that many changes can occur during the project. (possibility of new analysis, new measures or give up some others, ...) Facilitates the subsequent publication of data: either the data can serve to fill in an existing database or the data can be broadcast through a web-service approach with the associated metadata. ODAM can be seen as a wheel ... Advantages of this approach According to us the most important thing is to capture structural metadata as early as possible. The objective of our approach is to make this upstream capture an advantage to facilitate data analysis and therefore an incentive to perform this metadata capture. However on the basis of metadata files this does not prevent the development of parsers/converters to other formats or a connection to a more complex data model (e.g. ISA-TAB , MIAPPE 1.1 ) in order to include the data (transformed or not) in existing standards-compliant data infrastructures (e.g. SEEK platform ) Guideline for good data management ODAM's general philosophy is to consider that the data life cycle must be integrated into the scientific research process. Furthermore, by considering the FAIR principles as a guide to produce data that is reproducible and frictionless reusable by humans and machines, data FAIRification process is accomplished without insurmountable effort ( FAIR by design ) For this work, we made the choice to keep the good old way of scientist to use worksheets, thus using the same tool for both data files and metadata definition files. The ODAM software can be deployed at multiple scales (local, intranet, internet), depending on the need and the target community. Guideline keywords : experimental data tables , Research Data Management , OpenAPI , FAIR Data principles , FAIR assessment","title":"Proposed solution"},{"location":"presentation/#documentation","text":"Presentation : A presentation on the ODAM software, its aims and what can we do with it for what purposes. FAIR_and_DataLife_DJ_Oct2019.pdf Presentation : How to best manage your data to make the most of it for your research Make your data great now Presentation : A presentation about Data Management in the context of Open Science. Research Data Management CrowdFlower, Data Science Report (2016) https://visit.figure-eight.com/rs/416-ZBE-142/images/CrowdFlower_DataScienceReport_2016.pdf \u21a9","title":"Documentation"},{"location":"publish/","text":"Publish your dataset according to FAIR principles \u00b6 No question sharing data has benefits for a scientific career 1 . But more specifically the main reasons for publishing your data in a long-term repository are : fulfill a journal requirement to publish data facilitate citing your dataset by assigning a permanent uniquely identifiable code ( DOI ) improve discovery of and access to your dataset for others enable versioning of your data enable re-use and greater visibility of your work Most of these reasons coincide with the FAIR principles . But publishing data according to the FAIR principles involves much more, especially well-described, accessible data that complies with community standards 2 . The FAIR principles can be seen as a consolidation of good data management practices to extend management with the notion of machine-driven data reuse. In others words, the FAIR principles define the attributes that data must have to enable and enhance its reuse, by humans and machines. It is recommended that structural metadata (e.g. links between data tables) should also be provided along with unambiguous definitions of all internal elements (e.g. column definitions, units of measurement), through links to accessible (standard) definitions 3 . This is precisely what is targeted in the implementation of data management by the ODAM software suite. Therefore, not only descriptive but also structural metadata should be provided. Descriptive metadata and Structural metadata \u00b6 Descriptive metadata is descriptive information about a resource. It is used for discovery and identification. It includes elements such as title, abstract, author, and keywords. They make it easier for both humans and machines to find them. Structural metadata is metadata about containers of data and indicates how compound objects are put together. It describes the types, versions, relationships and other characteristics of digital materials. They facilitate the reuse of data by providing structural metadata, thus avoiding that data consumers spend a disproportionate amount of time trying to understand the digital resources they need and devising specific ways to combine them. Making data available online can be understood in different ways according to their nature and complexity, and the desired granularity. The simple approach is to deposit the data in flat files (generally a set of TSV/CSV files) in dedicated repositories, with only constraint to describe the data set as a whole with minimal metadata. But without structural metadata it is difficult if not impossible to understand the relationships between different data. Moreover, a dictionary describing each file (entity) as well as all the columns of the tables (attributes) offers a better guarantee in the correct (re)use of the data. Precisely, these last two points have been implemented in the ODAM software. Unlike data sharing, which is simply making data available to a community, data publishing also requires that the data could be referenceable (i.e. a stable identification system) and contextualizable (i.e. the who and what). These points and many others are precisely what the FAIR principles seek to establish as a frame of reference. Data publishing \u00b6 Because ODAM is primarily an Experimental Data Table Management System (EDTMS) for data sharing, it must be associated with a suitable data repository in order to support data publishing. So the ODAM approach has to be regarded as complementary with publication of the data online within an institutional data repository as described in re3data.org (e.g. Data INRAE ) associated or not with a scientific paper. To be compliant with the FAIR principles , not all data, documents, workflows and other tools need to be located in a single system, but from a central repository, it is the set of links that constitutes the true information management system. It must be able to be traversed by a human being as well as by machines. Dataset information need to be linked but not necessary in the same repository Use explicit schema for both data and metadata \u00b6 To publish your data, preferably use a data repository that complies with the JSON-LD standard. Moreover, an ODAM datapackage.json file should be deposited in the repository, which will either contain absolute URLs if an ODAM repository for the data is used, or simply a relative reference to the files if the data are in the same repository. Thus, by relying on explicit schemas ( JSON-LD , JSON Schema ) for both metadata and data, it becomes possible to reuse the data without friction, both by humans and machines. Because, when disseminating data, defining an explicit schema for structural metadata along with unambiguous definitions of all internal elements (e.g. column definitions, units of measurement), through links to accessible (standard) definitions allows machines to better interpret the data for reuse. This will result in better annotated and more easily usable data that meets effortlessly the FAIR criteria for reusability. Indeed, concerning the FAIRification of data, this has a positive impact on the FAIR criteria 'Interoperable' and 'Reusable', encouraging structured data using a discoverable, community-endorsed schema or data model. ODAM data FAIRification workflow \u00b6 graph LR A(Experimental<br>Data Tables) -->B(Add structural<br>metadata) B -->C(Generate a <br>Datapackage<br>descriptor) C -->|Publish|D(JSON-LD compliant <br> data repository) D -->E[FAIR compliant <br> data] click A \"../data-preparation/#2-data-structure-and-organization\" click B \"../data-preparation/#3-structural-metadata\" click C \"../json-schema\" click D \"https://figshare.com/articles/Listing_of_data_repositories_that_embed_schema_org_metadata_in_dataset_landing_pages/11506314/1\" _blank click E \"../fair-frim\" Exporting metadata in datapackage format offers a great flexibility of use data via scripting languages such as R and Python on the basis of existing packages. Furthermore, by linking data files from their URLs within the metadata, the metadata file formatted in this way can be distributed on its own while still allowing full exploitation of the data. Besides, this type of format allows a great variety in the choice of data repository as a distinct separation is established between structural metadata described in the datapackage format on the one hand , and descriptive metadata depending on the type of repository on the other hand. Preferably the chosen data repository should offer the ability to query and retrieve data using an API that conforms to the OpenAPI specification and that meet the essential criteria of the FAIR principles. For example, the following data repositories registered in re3data.org can be cited without being exhaustive: Dataverse , Dryad , FAIRDOMHub based on SEEK platform , FigShare , Zenodo . Among them, Dataverse and the SEEK platform can even be installed and configured as an institutional repository. Based on an ODAM repository \u00b6 Because ODAM is primarily an Experimental Data Table Management System (EDTMS) for data sharing, it must be associated with a suitable data repository in order to support data publishing. Whereas institutional data repositories focus on the experiment description with the corresponding descriptive metadata, the ODAM approach, by adjoining some minimal but relevant structural metadata, gives access to the data themselves with the possibility to explore and mine them. While ODAM ensuring data sharing function allows direct access to subsets of data by Web API request, an institutional data repository ensuring data publishing function has to support an important part of the FAIR criteria. Thus, the combination of the two systems makes it possible to cover all the essential FAIR criteria . One of the benefits of having an ODAM repository for data users is that it avoids the need to download data prior to any data analysis. This ensures that one starts from the data of the repository with help of the web API and not with the duplicate data that could have been changed between analyses without keeping track of it. In the same way, you can distribute your Jupiter notebooks , for example, without having to duplicate the data. In order to make the data fair, and even if there is already a web API for data access, it makes sense to attach an ODAM datapackage.json file (see JSON schema ) to the institutional data repository. This allows machines to use the data, since this datapackage file uses an explicit schema for the data and its organization. In this case, it is imperative to specify absolute URLs in the datapackage.json for the data files. You can get the datapackage.json file from the ODAM API as follows: https://<your ODAM data reposity>/getdata/query/<your dataset>/datapackage?links=1 Example : Data INRAE repository as a hub Based on Dataverse , Data INRAE repository allows to interconnect the different elements of the dataset. A file named ' datapackage.json ', structured according to an explicit schema, was generated based on the structural metadata previously defined with the help of spreadsheets. By relying on explicit schemas ( JSON-LD , JSON Schema ) for both metadata and data, this makes it possible for both humans and machines to reuse data without friction.. Data INRAE repository as a hub (based on Dataverse) Without ODAM repository \u00b6 If you do not wish to set up your own ODAM repository on internet, you can at least simply upload your data files with associated structural metadata to an institutional repository. Add a ODAM datapackage.json file within your collection of data files with relative path for files. (see JSON schema ). You can get the datapackage.json file from the ODAM API as follows: http://<your local ODAM data reposity>/getdata/query/<your dataset>/datapackage In addition, for easy reuse, simply indicate that the data has been formatted and structured to be compatible with the ODAM software, so that users of the data can take advantage of all associated services and tools by installing it on their desktop or laptop. Gabriel Popkin (2019) Data sharing and how it can benefit your scientific career, Nature 569:445-447 https://www.nature.com/articles/d41586-019-01506-x \u21a9 Turning FAIR into reality: Final Report and Action Plan from the European Commission Expert Group on FAIR Data (2018) https://ec.europa.eu/info/publications/turning-fair-reality_en \u21a9 Annika Jacobsen et al (2020), FAIR Principles: Interpretations and Implementation Considerations, Data Intelligence 2020 2: 1-2, 10-29 https://doi.org/10.1162/dint_r_00024 \u21a9 Making the data FAIR, EU FAIRplus project https://fairplus-project.eu/making-fair/ \u21a9 Cost-Benefit analysis for FAIR research data - Cost of not having FAIR research data, European Commission, Directorate-General for Research and Innovation (2018) https://bit.ly/3dixEe1 \u21a9 Bangert, Daniel, Hermans, Emilie, van Horik, Ren\u00e9, de Jong, Maaike, Koers, Hylke, & Mokrane, Mustapha. (2019, December 19). Recommendations for Services in a FAIR data ecosystem. Zenodo. http://doi.org/10.5281/zenodo.3585742 \u21a9 Deniz Beyan, Oya, Chue Hong, Neil, Cozzini, Stefano, Hoffman-Sommer, Marta, Hooft, Rob, Lembinen, Liisi, \u2026 Teperek, Marta. (2020, June 23). Seven Recommendations for Implementation of FAIR Practice. Zenodo. http://doi.org/10.5281/zenodo.3931993 \u21a9","title":"Publish your data"},{"location":"publish/#publish-your-dataset-according-to-fair-principles","text":"No question sharing data has benefits for a scientific career 1 . But more specifically the main reasons for publishing your data in a long-term repository are : fulfill a journal requirement to publish data facilitate citing your dataset by assigning a permanent uniquely identifiable code ( DOI ) improve discovery of and access to your dataset for others enable versioning of your data enable re-use and greater visibility of your work Most of these reasons coincide with the FAIR principles . But publishing data according to the FAIR principles involves much more, especially well-described, accessible data that complies with community standards 2 . The FAIR principles can be seen as a consolidation of good data management practices to extend management with the notion of machine-driven data reuse. In others words, the FAIR principles define the attributes that data must have to enable and enhance its reuse, by humans and machines. It is recommended that structural metadata (e.g. links between data tables) should also be provided along with unambiguous definitions of all internal elements (e.g. column definitions, units of measurement), through links to accessible (standard) definitions 3 . This is precisely what is targeted in the implementation of data management by the ODAM software suite. Therefore, not only descriptive but also structural metadata should be provided.","title":"Publish your dataset according to FAIR principles"},{"location":"publish/#descriptive-metadata-and-structural-metadata","text":"Descriptive metadata is descriptive information about a resource. It is used for discovery and identification. It includes elements such as title, abstract, author, and keywords. They make it easier for both humans and machines to find them. Structural metadata is metadata about containers of data and indicates how compound objects are put together. It describes the types, versions, relationships and other characteristics of digital materials. They facilitate the reuse of data by providing structural metadata, thus avoiding that data consumers spend a disproportionate amount of time trying to understand the digital resources they need and devising specific ways to combine them. Making data available online can be understood in different ways according to their nature and complexity, and the desired granularity. The simple approach is to deposit the data in flat files (generally a set of TSV/CSV files) in dedicated repositories, with only constraint to describe the data set as a whole with minimal metadata. But without structural metadata it is difficult if not impossible to understand the relationships between different data. Moreover, a dictionary describing each file (entity) as well as all the columns of the tables (attributes) offers a better guarantee in the correct (re)use of the data. Precisely, these last two points have been implemented in the ODAM software. Unlike data sharing, which is simply making data available to a community, data publishing also requires that the data could be referenceable (i.e. a stable identification system) and contextualizable (i.e. the who and what). These points and many others are precisely what the FAIR principles seek to establish as a frame of reference.","title":"Descriptive metadata and Structural metadata"},{"location":"publish/#data-publishing","text":"Because ODAM is primarily an Experimental Data Table Management System (EDTMS) for data sharing, it must be associated with a suitable data repository in order to support data publishing. So the ODAM approach has to be regarded as complementary with publication of the data online within an institutional data repository as described in re3data.org (e.g. Data INRAE ) associated or not with a scientific paper. To be compliant with the FAIR principles , not all data, documents, workflows and other tools need to be located in a single system, but from a central repository, it is the set of links that constitutes the true information management system. It must be able to be traversed by a human being as well as by machines. Dataset information need to be linked but not necessary in the same repository","title":"Data publishing"},{"location":"publish/#use-explicit-schema-for-both-data-and-metadata","text":"To publish your data, preferably use a data repository that complies with the JSON-LD standard. Moreover, an ODAM datapackage.json file should be deposited in the repository, which will either contain absolute URLs if an ODAM repository for the data is used, or simply a relative reference to the files if the data are in the same repository. Thus, by relying on explicit schemas ( JSON-LD , JSON Schema ) for both metadata and data, it becomes possible to reuse the data without friction, both by humans and machines. Because, when disseminating data, defining an explicit schema for structural metadata along with unambiguous definitions of all internal elements (e.g. column definitions, units of measurement), through links to accessible (standard) definitions allows machines to better interpret the data for reuse. This will result in better annotated and more easily usable data that meets effortlessly the FAIR criteria for reusability. Indeed, concerning the FAIRification of data, this has a positive impact on the FAIR criteria 'Interoperable' and 'Reusable', encouraging structured data using a discoverable, community-endorsed schema or data model.","title":"Use explicit schema for both data and metadata"},{"location":"publish/#odam-data-fairification-workflow","text":"graph LR A(Experimental<br>Data Tables) -->B(Add structural<br>metadata) B -->C(Generate a <br>Datapackage<br>descriptor) C -->|Publish|D(JSON-LD compliant <br> data repository) D -->E[FAIR compliant <br> data] click A \"../data-preparation/#2-data-structure-and-organization\" click B \"../data-preparation/#3-structural-metadata\" click C \"../json-schema\" click D \"https://figshare.com/articles/Listing_of_data_repositories_that_embed_schema_org_metadata_in_dataset_landing_pages/11506314/1\" _blank click E \"../fair-frim\" Exporting metadata in datapackage format offers a great flexibility of use data via scripting languages such as R and Python on the basis of existing packages. Furthermore, by linking data files from their URLs within the metadata, the metadata file formatted in this way can be distributed on its own while still allowing full exploitation of the data. Besides, this type of format allows a great variety in the choice of data repository as a distinct separation is established between structural metadata described in the datapackage format on the one hand , and descriptive metadata depending on the type of repository on the other hand. Preferably the chosen data repository should offer the ability to query and retrieve data using an API that conforms to the OpenAPI specification and that meet the essential criteria of the FAIR principles. For example, the following data repositories registered in re3data.org can be cited without being exhaustive: Dataverse , Dryad , FAIRDOMHub based on SEEK platform , FigShare , Zenodo . Among them, Dataverse and the SEEK platform can even be installed and configured as an institutional repository.","title":"ODAM data FAIRification workflow"},{"location":"publish/#based-on-an-odam-repository","text":"Because ODAM is primarily an Experimental Data Table Management System (EDTMS) for data sharing, it must be associated with a suitable data repository in order to support data publishing. Whereas institutional data repositories focus on the experiment description with the corresponding descriptive metadata, the ODAM approach, by adjoining some minimal but relevant structural metadata, gives access to the data themselves with the possibility to explore and mine them. While ODAM ensuring data sharing function allows direct access to subsets of data by Web API request, an institutional data repository ensuring data publishing function has to support an important part of the FAIR criteria. Thus, the combination of the two systems makes it possible to cover all the essential FAIR criteria . One of the benefits of having an ODAM repository for data users is that it avoids the need to download data prior to any data analysis. This ensures that one starts from the data of the repository with help of the web API and not with the duplicate data that could have been changed between analyses without keeping track of it. In the same way, you can distribute your Jupiter notebooks , for example, without having to duplicate the data. In order to make the data fair, and even if there is already a web API for data access, it makes sense to attach an ODAM datapackage.json file (see JSON schema ) to the institutional data repository. This allows machines to use the data, since this datapackage file uses an explicit schema for the data and its organization. In this case, it is imperative to specify absolute URLs in the datapackage.json for the data files. You can get the datapackage.json file from the ODAM API as follows: https://<your ODAM data reposity>/getdata/query/<your dataset>/datapackage?links=1 Example : Data INRAE repository as a hub Based on Dataverse , Data INRAE repository allows to interconnect the different elements of the dataset. A file named ' datapackage.json ', structured according to an explicit schema, was generated based on the structural metadata previously defined with the help of spreadsheets. By relying on explicit schemas ( JSON-LD , JSON Schema ) for both metadata and data, this makes it possible for both humans and machines to reuse data without friction.. Data INRAE repository as a hub (based on Dataverse)","title":"Based on an ODAM repository"},{"location":"publish/#without-odam-repository","text":"If you do not wish to set up your own ODAM repository on internet, you can at least simply upload your data files with associated structural metadata to an institutional repository. Add a ODAM datapackage.json file within your collection of data files with relative path for files. (see JSON schema ). You can get the datapackage.json file from the ODAM API as follows: http://<your local ODAM data reposity>/getdata/query/<your dataset>/datapackage In addition, for easy reuse, simply indicate that the data has been formatted and structured to be compatible with the ODAM software, so that users of the data can take advantage of all associated services and tools by installing it on their desktop or laptop. Gabriel Popkin (2019) Data sharing and how it can benefit your scientific career, Nature 569:445-447 https://www.nature.com/articles/d41586-019-01506-x \u21a9 Turning FAIR into reality: Final Report and Action Plan from the European Commission Expert Group on FAIR Data (2018) https://ec.europa.eu/info/publications/turning-fair-reality_en \u21a9 Annika Jacobsen et al (2020), FAIR Principles: Interpretations and Implementation Considerations, Data Intelligence 2020 2: 1-2, 10-29 https://doi.org/10.1162/dint_r_00024 \u21a9 Making the data FAIR, EU FAIRplus project https://fairplus-project.eu/making-fair/ \u21a9 Cost-Benefit analysis for FAIR research data - Cost of not having FAIR research data, European Commission, Directorate-General for Research and Innovation (2018) https://bit.ly/3dixEe1 \u21a9 Bangert, Daniel, Hermans, Emilie, van Horik, Ren\u00e9, de Jong, Maaike, Koers, Hylke, & Mokrane, Mustapha. (2019, December 19). Recommendations for Services in a FAIR data ecosystem. Zenodo. http://doi.org/10.5281/zenodo.3585742 \u21a9 Deniz Beyan, Oya, Chue Hong, Neil, Cozzini, Stefano, Hoffman-Sommer, Marta, Hooft, Rob, Lembinen, Liisi, \u2026 Teperek, Marta. (2020, June 23). Seven Recommendations for Implementation of FAIR Practice. Zenodo. http://doi.org/10.5281/zenodo.3931993 \u21a9","title":"Without ODAM repository"},{"location":"ressources/","text":"ODAM: Open Data for Access and Mining \u00b6 (C) INRAE UMR 1332 BFP, PMB - 2020 - Version 1.1 We have grouped together here all the links to documents and software produced about ODAM, as well as those concerning the tools, specifications and repositories (data and software) used directly (production) or indirectly (use, referencing) by ODAM. Online resources related to ODAM \u00b6 Description Type Link Data Preparation Protocol for ODAM Compliance Documentation Data collection and preparation API Documentation based on Swagger Web API Tool INRA-PMB/ODAM/1.2.0/ R ODAM package and How to use it Package https://cran.r-project.org/package=Rodam R ODAM package Vignette Package Rodam/vignettes/Rodam.html Virtual Machine embedding the ODAM software on Oracle VM VirtualBox along with its installation guide Virtual Machine + Documentation https://doi.org/10.15454/C9LAEF Docker containers on DockerHub for installing on a Linux machine Container https://hub.docker.com/r/odam/getdata/ https://hub.docker.com/r/odam/dataexplorer/ A very lightweight local web server for Windows to deploy the ODAM API Web API Tool ODAMwebserver Examples of Jupyter notebooks (R & Python) based on the ODAM Web API Notebook https://github.com/djacob65/binder_odam https://doi.org/10.24433/CO.8981049.v1 https://doi.org/10.24433/CO.0011270.v1 Modeling the growth of tomato fruits based on enzyme activity profiles (Example of data analysis interfaced by ODAM) Notebook https://hal.inrae.fr/hal-02611223 ODAM Source code on GitHub Source Code https://github.com/inrae/ODAM ODAM Source code on Software Heritage Source Code https://www.softwareheritage.org/ JSON Schema for ODAM data package Source Code https://github.com/djacob65/odam-datapackage/ Note : In order either to improve the use of ODAM or to complement its toolbox, a number of additional developments are planned. External resources used \u00b6 Tools Name Description Link docker Container management tool https://www.docker.com/ Oracle VM VirtualBox Virtual Machine (VM) management tool https://www.virtualbox.org/ Rstudio Integrated Development Environment (IDE) for R https://rstudio.com/ R shiny interactive web apps for R https://shiny.rstudio.com/ jupyter notebook Web application for e-notebooks https://jupyter.org/ q-text-as-data A command line tool for running SQL directly on CSV or TSV files http://harelba.github.io/q/ sqlite3 A command line tool for executing SQL statements against an SQLite database https://www.sqlite.org/cli.html goodtables.io data publishing with confidence - data validation on every change http://goodtables.io/ codemeta-generator Create your codemeta file https://codemeta.github.io/codemeta-generator/ ---- Repositories Name Type Link GitHub Source code https://github.com/ Software Heritage Source Code https://www.softwareheritage.org/ Docker Hub Containers https://hub.docker.com/ Data INRAE Data https://data.inrae.fr/ HAL INRAE Documents https://hal.inrae.fr/ protocols.io Documents https://www.protocols.io/ Code Ocean Notebooks https://codeocean.com/ R cran Source code https://cran.r-project.org/ Specifications Name Type Link Swagger API schema https://swagger.io/specification/ Data Packages Data schema https://specs.frictionlessdata.io/data-package/ The CodeMeta Project Code metadata schema https://codemeta.github.io/ Registries Name Type Link bio.tools Tool and data services registry. https://bio.tools/ RRID Research Resource Identification registry https://scicrunch.org/browse/resourcedashboard Identifiers Central registry which provides compact identifiers https://registry.identifiers.org/ FAIRsharing Standards, databases and policies registry https://fairsharing.org/ RDA Metadata Directory Standards, extensions, tools, use cases https://rd-alliance.github.io/metadata-directory/ FAIR evaluation/assessment grids Name Link 5 \u2605 Data Rating Tool https://confluence.csiro.au/display/OZNOME/Oznome+5-star+Processes RDA FAIR Data Maturity Model WG https://www.rd-alliance.org/groups/fair-data-maturity-model-wg RDA Sharing Rewards and Credit IG https://www.rd-alliance.org/groups/sharing-rewards-and-credit-sharc-ig Tools used for this online documentation \u00b6 Name Link Make the Docs https://www.mkdocs.org/ Material for MkDocs https://squidfunk.github.io/mkdocs-material/ Lightbox https://lokeshdhakar.com/projects/lightbox2/ Mermaid http://mermaid-js.github.io/mermaid/","title":"Online ressources"},{"location":"ressources/#odam-open-data-for-access-and-mining","text":"(C) INRAE UMR 1332 BFP, PMB - 2020 - Version 1.1 We have grouped together here all the links to documents and software produced about ODAM, as well as those concerning the tools, specifications and repositories (data and software) used directly (production) or indirectly (use, referencing) by ODAM.","title":"ODAM: Open Data for Access and Mining"},{"location":"ressources/#online-resources-related-to-odam","text":"Description Type Link Data Preparation Protocol for ODAM Compliance Documentation Data collection and preparation API Documentation based on Swagger Web API Tool INRA-PMB/ODAM/1.2.0/ R ODAM package and How to use it Package https://cran.r-project.org/package=Rodam R ODAM package Vignette Package Rodam/vignettes/Rodam.html Virtual Machine embedding the ODAM software on Oracle VM VirtualBox along with its installation guide Virtual Machine + Documentation https://doi.org/10.15454/C9LAEF Docker containers on DockerHub for installing on a Linux machine Container https://hub.docker.com/r/odam/getdata/ https://hub.docker.com/r/odam/dataexplorer/ A very lightweight local web server for Windows to deploy the ODAM API Web API Tool ODAMwebserver Examples of Jupyter notebooks (R & Python) based on the ODAM Web API Notebook https://github.com/djacob65/binder_odam https://doi.org/10.24433/CO.8981049.v1 https://doi.org/10.24433/CO.0011270.v1 Modeling the growth of tomato fruits based on enzyme activity profiles (Example of data analysis interfaced by ODAM) Notebook https://hal.inrae.fr/hal-02611223 ODAM Source code on GitHub Source Code https://github.com/inrae/ODAM ODAM Source code on Software Heritage Source Code https://www.softwareheritage.org/ JSON Schema for ODAM data package Source Code https://github.com/djacob65/odam-datapackage/ Note : In order either to improve the use of ODAM or to complement its toolbox, a number of additional developments are planned.","title":"Online resources related to ODAM"},{"location":"ressources/#external-resources-used","text":"Tools Name Description Link docker Container management tool https://www.docker.com/ Oracle VM VirtualBox Virtual Machine (VM) management tool https://www.virtualbox.org/ Rstudio Integrated Development Environment (IDE) for R https://rstudio.com/ R shiny interactive web apps for R https://shiny.rstudio.com/ jupyter notebook Web application for e-notebooks https://jupyter.org/ q-text-as-data A command line tool for running SQL directly on CSV or TSV files http://harelba.github.io/q/ sqlite3 A command line tool for executing SQL statements against an SQLite database https://www.sqlite.org/cli.html goodtables.io data publishing with confidence - data validation on every change http://goodtables.io/ codemeta-generator Create your codemeta file https://codemeta.github.io/codemeta-generator/ ---- Repositories Name Type Link GitHub Source code https://github.com/ Software Heritage Source Code https://www.softwareheritage.org/ Docker Hub Containers https://hub.docker.com/ Data INRAE Data https://data.inrae.fr/ HAL INRAE Documents https://hal.inrae.fr/ protocols.io Documents https://www.protocols.io/ Code Ocean Notebooks https://codeocean.com/ R cran Source code https://cran.r-project.org/ Specifications Name Type Link Swagger API schema https://swagger.io/specification/ Data Packages Data schema https://specs.frictionlessdata.io/data-package/ The CodeMeta Project Code metadata schema https://codemeta.github.io/ Registries Name Type Link bio.tools Tool and data services registry. https://bio.tools/ RRID Research Resource Identification registry https://scicrunch.org/browse/resourcedashboard Identifiers Central registry which provides compact identifiers https://registry.identifiers.org/ FAIRsharing Standards, databases and policies registry https://fairsharing.org/ RDA Metadata Directory Standards, extensions, tools, use cases https://rd-alliance.github.io/metadata-directory/ FAIR evaluation/assessment grids Name Link 5 \u2605 Data Rating Tool https://confluence.csiro.au/display/OZNOME/Oznome+5-star+Processes RDA FAIR Data Maturity Model WG https://www.rd-alliance.org/groups/fair-data-maturity-model-wg RDA Sharing Rewards and Credit IG https://www.rd-alliance.org/groups/sharing-rewards-and-credit-sharc-ig","title":"External resources used"},{"location":"ressources/#tools-used-for-this-online-documentation","text":"Name Link Make the Docs https://www.mkdocs.org/ Material for MkDocs https://squidfunk.github.io/mkdocs-material/ Lightbox https://lokeshdhakar.com/projects/lightbox2/ Mermaid http://mermaid-js.github.io/mermaid/","title":"Tools used for this online documentation"},{"location":"services/","text":"Using ODAM \u00b6 The overall usage scheme of ODAM is shown in the following figure: The overall usage scheme of ODAM As input, we have the various data files from the experiment. You can view the data graphically (see Data Explorer ) and you can also merge certain files based on common identifiers and then select a sub-part. The output is the file resulting from the query. The central idea which has been the founding idea of ODAM, is that data producers \"just\" have to drag and drop their data tables onto a storage space, which depending on the chosen infrastructure can be local (i.e. their Desktop Computer, or a NAS ) or remote ( virtual disk space ). See Installation section. The central idea is that data producers \"just\" have to drag and drop their data tables onto a storage space depending on the chosen infrastructure (local, intranet, internet). An API (Application Programming Interface) layer is implemented which allows data handling (data selection and merging) So simply dropping data files (along with two additional metadata files) on the storage space allows users to access them through web services. This means there is no need for additional configuration on the server. The dataset must first be previously formatted according to precise rules which requires that some good practices be followed and adhered to. See Data collection and preparation section. Note : In case of having a remote storage space, the Syncany solution (Dropbox-like) might be a good choice. Several other tools or approaches can be used as: WinSCP , rsync , ... The ODAM software can be deployed at multiple scales (local, intranet, internet), depending on the need and the target community. Provide Services \u00b6 The ODAM software embeds an API ( Application Programming Interface ) layer which offers data selection and merging services. It is this API layer that allows interoperability between the different tables and the applications that will be able to use them (See Web API ). With the help of this layer, it opens up a whole ecosystem of potential applications, depending on your needs but also on your skills in the proposed tools. From the set of data files (which are uncombined tables, each corresponding to a particular observational unit that we name an entity), the user can: 1) Visualize the data associated with their metadata according to several criteria and in a completely interactive way with the help of the data explorer. 2) Export in tabular form subsets selected according to his criteria with combined, merged data. 3) Build and test its models more easily using a scripting language such as R, which allows it to repeat different scenarios according to a variety of parameters. An R package allows to perform extractions according to the same criteria as those proposed in the data explorer. All this is possible thanks to the API layer, which opens up a whole ecosystem of potential applications. graph LR A([Using ODAM]) C(Dataexplorer) D(R/Python scripts) E(API by<br>Web Browser) F(Spreadsheet<br>Excel, Google, ...) G(Jupyter Notebooks) H(Other tools) A --> E A --> D A --> C E --> F C --> F E --> D D --> G F <--> G F <--> D F <--> H click C \"../data-explorer\" click E \"../api\" click G \"https://jupyter.org/try\" _blank Export data from your web browser \u00b6 You can retrieve data directly from your web browser based on the API. To understand the syntax, see Web API . Test from the API Documentation on SwaggerHub https://app.swaggerhub.com/apis-docs/INRA-PMB/ODAM/1.2.0/ Example : First, view the subset list of a dataset along with the metadata https://pmb-bordeaux.fr/getdata/query/frim1?format=xml Then, retrieve a data table by merging \u201cActivome Features\u201d data (enzymes) and \u201cNMR Compounds quantification\u201d data https://pmb-bordeaux.fr/getdata/query/frim1/(activome,qNMR_metabo)?format=tsv If you have associated the files having the extension '.txt' (tab as separator) with e.g. MS Excel, then just click on it to open it. For more details on how API querie work, see a very detailed example of api querying . Using the API with R \u00b6 It is possible to adopt more efficient approaches to analyze data, e.g for automating various processing operations, or for allowing users to select subsets of data and then carry out numerous repetitions of complex processing operations according to a wide variety of parameters (scenarios). For this purpose we have developed an R ( Rodam ) package that allows data extraction according to the API schema . The Rodam package provides a set of functions to retrieve data and their metadata from data sets formatted according to the ODAM data structuring approach. Rodam Package Manual https://cran.r-project.org/web/packages/Rodam/Rodam.pdf Vignette 'Demonstration of the functionalities of the Rodam package' https://cran.r-project.org/web/packages/Rodam/vignettes/Rodam.html Illustration of the use of the API under R using the Rodam package. First of all (top) one can easily view the global structure of the data set. Then (bottom), one can apply a query for a single sample (here id 365) by returning a merge of several subsets (here plant+samples). Illustration of the use of the API under R using the Rodam package. Visualization of set intersections between data subset based on the sample identifier and using the UpSetR package Reproducible research with Jupyter notebooks \u00b6 In order to illustrate a reproducible research process, we provide examples of Jupyter Notebooks (R & Python) based on the ODAM Web API, as well as links to view them (Jupyter nbviewer, CodeOcean) and re-run them ( MyBinder , CodeOcean ) GitHub https://github.com/djacob65/binder_odam CodeOcean Rodam API Demo : https://doi.org/10.24433/CO.8981049.v1 PyODAM API Demo : https://doi.org/10.24433/CO.0011270.v1 To Install Jupyter Notebook linked to your R version (and also Python), here is a simple tutorial . Note : A good way to share your notebooks with colleagues or project members is to install the Littlest JupyterHub in a datacenter (institutional or in the cloud, like Elixir , for Europe, or IFB , for France). It is a simple JupyterHub distribution for a small (0-100) number of users on a single server. See e.g Using JupyterHub within a VM and on the Cloud . Advantages of this approach \u00b6 From a user's point of view, data collection should be done as early as possible in a project. There are several reasons: First, experimental design is defined very early, so it is possible to share in the form of a spreadsheet the list of identifiers of individuals with their unique identifiers (plant identifiers) associated with the particular features defining the factorial group (metadata) such as: genotype, the type of treatment, its position in the greenhouse or in the field, \u2026 Thus, the array of the \"plants\" may be created even before planting the seeds. So too, the array of the \"harvesting\" can be done as soon as harvested, before any analysis. Then, each analysis comes complementing the dataset as soon as they produce their data subset. Whether the data are acquired in the field or in the laboratory, their acquisition is carried out gradually, and is modified according to the first statistical treatments and preliminary results. This iterative aspect of production requires traceability and management tools. Typically, an experiment design often implies if followed as planned plenty of samples due to the multiplication of several numbers: number of factors X the number of factor levels X number of biological replication X different types of analysis X number of technical replication for each analysis type. Giving the possibility to explore the preliminary data (e.g. the distributions of some features of the samples), will allow scientists to eliminate outliers, thus avoiding subsequent analysis of these samples. Data have to be accessible to everyone as soon as they are produced, allowing other data producers in the data string to link the identifiers of their analytical samples within the information system and subsequently allowing all partners to gather data with the metadata (factors, dates, ... ) of the Design of Experiment. By proceeding in this way, we ensure data coherence all the experiment time, and thus it becomes useless for each member to proceed a laborious investigation to find out who possesses the right identifiers. From a technical point of view, the great advantage is to avoid the implementation of a complex data management system (requiring a data model) given that many changes can occur during the project. (possibility of new analysis, new measures or renouncing some others, ...). It also facilitates the subsequent publication of data: either the data can serve to fill in an existing database or the data can be broadcast through a web-service approach with the associated metadata. See Publish your data . Tasks such as combining several data files are time-consuming and require repetitive and tedious handling (e.g data scientists spend 79% of their time on data preparation 1 ). Similarly, when modeling, subsets have to be selected and many scenarios tested with different parameters. If performed manually (i.e., multiple copy-paste in a spreadsheet) these tasks are not only tedious but prone to handling errors. ODAM software is designed to perform precisely these tasks by providing several services for greater flexibility in data handling. CrowdFlower, Data Science Report (2016) https://visit.figure-eight.com/rs/416-ZBE-142/images/CrowdFlower_DataScienceReport_2016.pdf \u21a9","title":"Using ODAM"},{"location":"services/#using-odam","text":"The overall usage scheme of ODAM is shown in the following figure: The overall usage scheme of ODAM As input, we have the various data files from the experiment. You can view the data graphically (see Data Explorer ) and you can also merge certain files based on common identifiers and then select a sub-part. The output is the file resulting from the query. The central idea which has been the founding idea of ODAM, is that data producers \"just\" have to drag and drop their data tables onto a storage space, which depending on the chosen infrastructure can be local (i.e. their Desktop Computer, or a NAS ) or remote ( virtual disk space ). See Installation section. The central idea is that data producers \"just\" have to drag and drop their data tables onto a storage space depending on the chosen infrastructure (local, intranet, internet). An API (Application Programming Interface) layer is implemented which allows data handling (data selection and merging) So simply dropping data files (along with two additional metadata files) on the storage space allows users to access them through web services. This means there is no need for additional configuration on the server. The dataset must first be previously formatted according to precise rules which requires that some good practices be followed and adhered to. See Data collection and preparation section. Note : In case of having a remote storage space, the Syncany solution (Dropbox-like) might be a good choice. Several other tools or approaches can be used as: WinSCP , rsync , ... The ODAM software can be deployed at multiple scales (local, intranet, internet), depending on the need and the target community.","title":"Using ODAM"},{"location":"services/#provide-services","text":"The ODAM software embeds an API ( Application Programming Interface ) layer which offers data selection and merging services. It is this API layer that allows interoperability between the different tables and the applications that will be able to use them (See Web API ). With the help of this layer, it opens up a whole ecosystem of potential applications, depending on your needs but also on your skills in the proposed tools. From the set of data files (which are uncombined tables, each corresponding to a particular observational unit that we name an entity), the user can: 1) Visualize the data associated with their metadata according to several criteria and in a completely interactive way with the help of the data explorer. 2) Export in tabular form subsets selected according to his criteria with combined, merged data. 3) Build and test its models more easily using a scripting language such as R, which allows it to repeat different scenarios according to a variety of parameters. An R package allows to perform extractions according to the same criteria as those proposed in the data explorer. All this is possible thanks to the API layer, which opens up a whole ecosystem of potential applications. graph LR A([Using ODAM]) C(Dataexplorer) D(R/Python scripts) E(API by<br>Web Browser) F(Spreadsheet<br>Excel, Google, ...) G(Jupyter Notebooks) H(Other tools) A --> E A --> D A --> C E --> F C --> F E --> D D --> G F <--> G F <--> D F <--> H click C \"../data-explorer\" click E \"../api\" click G \"https://jupyter.org/try\" _blank","title":"Provide Services"},{"location":"services/#export-data-from-your-web-browser","text":"You can retrieve data directly from your web browser based on the API. To understand the syntax, see Web API . Test from the API Documentation on SwaggerHub https://app.swaggerhub.com/apis-docs/INRA-PMB/ODAM/1.2.0/ Example : First, view the subset list of a dataset along with the metadata https://pmb-bordeaux.fr/getdata/query/frim1?format=xml Then, retrieve a data table by merging \u201cActivome Features\u201d data (enzymes) and \u201cNMR Compounds quantification\u201d data https://pmb-bordeaux.fr/getdata/query/frim1/(activome,qNMR_metabo)?format=tsv If you have associated the files having the extension '.txt' (tab as separator) with e.g. MS Excel, then just click on it to open it. For more details on how API querie work, see a very detailed example of api querying .","title":"Export data from your web browser"},{"location":"services/#using-the-api-with-r","text":"It is possible to adopt more efficient approaches to analyze data, e.g for automating various processing operations, or for allowing users to select subsets of data and then carry out numerous repetitions of complex processing operations according to a wide variety of parameters (scenarios). For this purpose we have developed an R ( Rodam ) package that allows data extraction according to the API schema . The Rodam package provides a set of functions to retrieve data and their metadata from data sets formatted according to the ODAM data structuring approach. Rodam Package Manual https://cran.r-project.org/web/packages/Rodam/Rodam.pdf Vignette 'Demonstration of the functionalities of the Rodam package' https://cran.r-project.org/web/packages/Rodam/vignettes/Rodam.html Illustration of the use of the API under R using the Rodam package. First of all (top) one can easily view the global structure of the data set. Then (bottom), one can apply a query for a single sample (here id 365) by returning a merge of several subsets (here plant+samples). Illustration of the use of the API under R using the Rodam package. Visualization of set intersections between data subset based on the sample identifier and using the UpSetR package","title":"Using the API with R"},{"location":"services/#reproducible-research-with-jupyter-notebooks","text":"In order to illustrate a reproducible research process, we provide examples of Jupyter Notebooks (R & Python) based on the ODAM Web API, as well as links to view them (Jupyter nbviewer, CodeOcean) and re-run them ( MyBinder , CodeOcean ) GitHub https://github.com/djacob65/binder_odam CodeOcean Rodam API Demo : https://doi.org/10.24433/CO.8981049.v1 PyODAM API Demo : https://doi.org/10.24433/CO.0011270.v1 To Install Jupyter Notebook linked to your R version (and also Python), here is a simple tutorial . Note : A good way to share your notebooks with colleagues or project members is to install the Littlest JupyterHub in a datacenter (institutional or in the cloud, like Elixir , for Europe, or IFB , for France). It is a simple JupyterHub distribution for a small (0-100) number of users on a single server. See e.g Using JupyterHub within a VM and on the Cloud .","title":"Reproducible research with Jupyter notebooks"},{"location":"services/#advantages-of-this-approach","text":"From a user's point of view, data collection should be done as early as possible in a project. There are several reasons: First, experimental design is defined very early, so it is possible to share in the form of a spreadsheet the list of identifiers of individuals with their unique identifiers (plant identifiers) associated with the particular features defining the factorial group (metadata) such as: genotype, the type of treatment, its position in the greenhouse or in the field, \u2026 Thus, the array of the \"plants\" may be created even before planting the seeds. So too, the array of the \"harvesting\" can be done as soon as harvested, before any analysis. Then, each analysis comes complementing the dataset as soon as they produce their data subset. Whether the data are acquired in the field or in the laboratory, their acquisition is carried out gradually, and is modified according to the first statistical treatments and preliminary results. This iterative aspect of production requires traceability and management tools. Typically, an experiment design often implies if followed as planned plenty of samples due to the multiplication of several numbers: number of factors X the number of factor levels X number of biological replication X different types of analysis X number of technical replication for each analysis type. Giving the possibility to explore the preliminary data (e.g. the distributions of some features of the samples), will allow scientists to eliminate outliers, thus avoiding subsequent analysis of these samples. Data have to be accessible to everyone as soon as they are produced, allowing other data producers in the data string to link the identifiers of their analytical samples within the information system and subsequently allowing all partners to gather data with the metadata (factors, dates, ... ) of the Design of Experiment. By proceeding in this way, we ensure data coherence all the experiment time, and thus it becomes useless for each member to proceed a laborious investigation to find out who possesses the right identifiers. From a technical point of view, the great advantage is to avoid the implementation of a complex data management system (requiring a data model) given that many changes can occur during the project. (possibility of new analysis, new measures or renouncing some others, ...). It also facilitates the subsequent publication of data: either the data can serve to fill in an existing database or the data can be broadcast through a web-service approach with the associated metadata. See Publish your data . Tasks such as combining several data files are time-consuming and require repetitive and tedious handling (e.g data scientists spend 79% of their time on data preparation 1 ). Similarly, when modeling, subsets have to be selected and many scenarios tested with different parameters. If performed manually (i.e., multiple copy-paste in a spreadsheet) these tasks are not only tedious but prone to handling errors. ODAM software is designed to perform precisely these tasks by providing several services for greater flexibility in data handling. CrowdFlower, Data Science Report (2016) https://visit.figure-eight.com/rs/416-ZBE-142/images/CrowdFlower_DataScienceReport_2016.pdf \u21a9","title":"Advantages of this approach"},{"location":"tests/","text":"ODAM: Deployment and User's Guide \u00b6","title":"Tests"},{"location":"tests/#odam-deployment-and-users-guide","text":"","title":"ODAM: Deployment and User's Guide"},{"location":"todo/","text":"ODAM: Development roadmap \u00b6 Data extraction \u00b6 Extension of the Rodam package Data exploitation only based on the datapackage.json file - an equivalent of the datapackage-r package for ODAM datapackage . It will facilitate the reuse of the data provided with this structure by installing a single R package. Development of a python package equivalent to Rodam Build a lightweight interface ( jQuery/Bootstrap ) for Data extraction - equivalent to Web API functions Generation of the datapackage.json file with additional metadata e.g. description of the dataset, choice of license, keywords, sources, ... Data annotation \u00b6 Development carried out in interaction with the FooSIN project Build a lightweight interface ( jQuery/Bootstrap ) for Attribute annotations with terms selected from ontologies. The choice of ontologies will be based on predefined sets according to the domain under consideration. For example, in the domain of plants, a set of ontologies will be predefined according to AgroPortal . The chosen solution will be greatly inspired by the RightField project , the Swate tool or the annotation module of the MeRyB database Automatic proposals for annotation based on the annotation module of AgroPortal through its API. Linked Data \u00b6 Development carried out in interaction with the FooSIN project Build an ontology-based model to describe the structure of tabular and relational data, primarily focused on concepts of data entity 1 , data attributes 2 , and categories of variables (identifier 3 , factor 4 , quantitative 5 , qualitative 6 ), as shown below : See Make your data great again : How to ensure that open data works for research - Towards Linked Data Make a proposal to convert a ODAM datapackage to JSON-LD and then to RDF . Mainly based on the ontology defined at point 1. the Tbox will be based on the AgroPortal ontologies as a Proof of Concept. A data entity is an object in a data model. Data is typically designed by breaking things down into their smallest parts that are useful for representing data relationships. For example, a plant can generate a list of samples. Each sample can be associated with several types of analytical variables. All three objects: plant, sample and type of analytical variables are considered data entities. \u21a9 Data attributes are characteristics of a data object. From data science view, they are the features of a data entity. They exist most often as a column in a data table. \u21a9 Identifiers precisely reference within the experiment each of the elements belonging to the same observation entity forming a coherent observation unit. For example, each plant, each sample has its own identifier, \u21a9 A factor of an experiment is a controlled independent variable; a variable whose levels are set by the experimenter. Treatments (control vs. stress), genotype (WT vs. mutant), the course of time (development stages) or even tissues, are typical factors of experiments. \u21a9 Quantitative data are values that describe a measurable quantity, in the form of numbers that can be calculated. \u21a9 Qualitative data describe qualities or characteristics. They answer questions such as \"what type\" or \"what category\". These values \u21a9","title":"ToDo"},{"location":"todo/#odam-development-roadmap","text":"","title":"ODAM: Development roadmap"},{"location":"todo/#data-extraction","text":"Extension of the Rodam package Data exploitation only based on the datapackage.json file - an equivalent of the datapackage-r package for ODAM datapackage . It will facilitate the reuse of the data provided with this structure by installing a single R package. Development of a python package equivalent to Rodam Build a lightweight interface ( jQuery/Bootstrap ) for Data extraction - equivalent to Web API functions Generation of the datapackage.json file with additional metadata e.g. description of the dataset, choice of license, keywords, sources, ...","title":"Data extraction"},{"location":"todo/#data-annotation","text":"Development carried out in interaction with the FooSIN project Build a lightweight interface ( jQuery/Bootstrap ) for Attribute annotations with terms selected from ontologies. The choice of ontologies will be based on predefined sets according to the domain under consideration. For example, in the domain of plants, a set of ontologies will be predefined according to AgroPortal . The chosen solution will be greatly inspired by the RightField project , the Swate tool or the annotation module of the MeRyB database Automatic proposals for annotation based on the annotation module of AgroPortal through its API.","title":"Data annotation"},{"location":"todo/#linked-data","text":"Development carried out in interaction with the FooSIN project Build an ontology-based model to describe the structure of tabular and relational data, primarily focused on concepts of data entity 1 , data attributes 2 , and categories of variables (identifier 3 , factor 4 , quantitative 5 , qualitative 6 ), as shown below : See Make your data great again : How to ensure that open data works for research - Towards Linked Data Make a proposal to convert a ODAM datapackage to JSON-LD and then to RDF . Mainly based on the ontology defined at point 1. the Tbox will be based on the AgroPortal ontologies as a Proof of Concept. A data entity is an object in a data model. Data is typically designed by breaking things down into their smallest parts that are useful for representing data relationships. For example, a plant can generate a list of samples. Each sample can be associated with several types of analytical variables. All three objects: plant, sample and type of analytical variables are considered data entities. \u21a9 Data attributes are characteristics of a data object. From data science view, they are the features of a data entity. They exist most often as a column in a data table. \u21a9 Identifiers precisely reference within the experiment each of the elements belonging to the same observation entity forming a coherent observation unit. For example, each plant, each sample has its own identifier, \u21a9 A factor of an experiment is a controlled independent variable; a variable whose levels are set by the experimenter. Treatments (control vs. stress), genotype (WT vs. mutant), the course of time (development stages) or even tissues, are typical factors of experiments. \u21a9 Quantitative data are values that describe a measurable quantity, in the form of numbers that can be calculated. \u21a9 Qualitative data describe qualities or characteristics. They answer questions such as \"what type\" or \"what category\". These values \u21a9","title":"Linked Data"},{"location":"data-preparation/","text":"Data Preparation Protocol for ODAM Compliance \u00b6 The purpose of this protocol is to describe all the steps involved in collecting, preparing and annotating the data from an experiment associated with an experimental design ( DoE ) that will then allow the user to benefit from the services offered by ODAM. The overall approach is based on good data management practices concerning data structuring and the description of structural metadata. Indeed, the strong point of the approach is to define metadata in depth, i.e. at the level of the data itself (i.e. metadata at column-level such as factors, variables...) and not just descriptive metadata on the top of the dataset. Thus, having structural metadata allows datasets to achieve a higher level of interoperability and greatly facilitates functional interconnection and analysis in a broader context. Based on an example In order to illustrate the different stages of this protocol, we have chosen an example from an experiment on tomato fruits grown in a greenhouse. The aim of this study was to build a model of fruit growth. For this, a certain amount of data was required, and we will limit ourselves to some of them in order to simplify the size of the data set. See the complete example: Data explorer https://pmb-bordeaux.fr/dataexplorer/?ds=frim1 Dataverse : https://doi.org/10.15454/95JUTK 1- Data Gathering \u00b6 In our data subset example, we have 5 data files, one by type of object ( plants, harvests, samples, compounds and enzymes ). 5 different entities within the study, each corresponding to a file of data tables: plants, harvests, samples, compounds and enzymes 2 factors: Treatment, Development stages 53 quantitative variables: compounds (12) + enzymes (38) + weight, height, diameter (3) 1 - First, we put them under the same directory by giving it a name corresponding to the study or project (e.g. acronym of the project with a suffix corresponding to a study) 2 - Data subsets files must be compliant with the TSV standard ( Tab-Separator-Values ). So an ODAM dataset is a bundle that contains a set of TSV files. The TSV files are simple tables containing the data of the dataset. In choosing this format, we follow the 5 gold stars principle, considered as a good practice and a necessary and indispensable step towards \"Linked Open Data\". Note : Data files must have the extension ' txt ' in order to distinguish them from metadata files, see below. Advice: To be sure to have the right format, do a copy of data from the spreadsheet then paste them into a new file, then save as TSV format (separator: a tab character)' 2 - Data structure and organization \u00b6 Since all the experimental data tables were generated as part of an experiment associated with a Design of Experiment ( DoE ), each file thus contains data acquired sequentially as the experiment progressed. There must therefore be a link between each file, i.e. information that connects them together. In most cases (if not all), this information corresponds to identifiers 4 that make it possible to precisely reference within the experiment each of the elements belonging to the same observation entity 1 forming a coherent observation unit. For example, each plant, each sample has its own identifier, and each of these entities corresponds to a separate data file. Well organized data means that each data table must be correctly structured, i.e.: Each variable forms a column, Each observation forms a line, Each type of \"unit observational\" (defined as an entity) forms a table, i.e. a file, Each data table file must have a column defined as an identifier (similar as a primary key) corresponding to each observation of the entity (e.g. plant, sample, \u2026), Missing values can either be an empty cell or have the value 'NA', The header names must short without special characters. Only use the alphanumerical characters, and the underline character as word separator, The file should only contain data in matrix form and nothing else, i.e. no annotation on top, bottom, or sides. Example of the \u2018samples.txt\u2019 file: The files generated during data collection have to be organized according to the entity-relationship model similar to relational database management systems (RDBMS). Indeed, each entity 1 corresponds to a type of collected data (samples, compounds, ...) for which is associated a set of attributes 2 , i.e. a set of variables that may include observed or measured variables (quantitative or qualitative), controlled independent variables (factors) and an identifier. Then, a link is established for each subset with the subset from which it was obtained, so that the links can be interpreted as \" obtained from \u201d as shown in the figure below: We have to organize your data subsets so that links could be established between them. In practical, it means to add when necessary a column (colored in green in the figure below) containing the identifiers corresponding to the entity to which we want to connect the subset. It is to be noted that this duplication of identifiers must be the only redundant information, through all data subsets. 3 - Structural Metadata \u00b6 ODAM provides a model for structuring both data and metadata that facilitates data handling and analysis. Whatever the kind of experiment, this assumes a design of experiment ( DoE ) involving individuals, samples or whatever things, as the main objects of study and producing several experimental data tables . This also assumes the observation of dependent variables resulting from effects of some controlled independent variables ( factors 3 ). Moreover, the objects of study usually have an identifier 4 for each of them, and the variables can be quantitative 5 or qualitative 6 . We can have either one entity within the study or several kinds, but in this latter case, it must exist a relationship between entities that we assume of \u201c obtained from \" type as describe above. Thus, the data table of samples can be viewed according to the repartition by category we have just introduced as shown below: a data entity (data subset 'samples') consisting of its attributes (columns) divided by category (identifier, factor, quantitative, qualitative) The four categories Identifier : precisely reference within the experiment each of the elements belonging to the same observation entity forming a coherent observation unit. For example, each plant, each sample has its own identifier, Factor : a factor of an experiment is a controlled independent variable; a variable whose levels are set by the experimenter. Treatments (control vs. stress), genotype (WT vs. mutant), the course of time (development stages) or even tissues, are typical factors of experiments. Quantitative : Quantitative data are values that describe a measurable quantity, in the form of numbers that can be calculated. Qualitative : Qualitative data describe qualities or characteristics. They answer questions such as \"what type\" or \"what category\". These values are no longer numbers, but a set of modalities. These values cannot be calculated. In order to allow data to be ODAM-compliant, we need two specific files (subsets & attributes) to describe the structural metadata of the whole dataset, i.e. some minimal but relevant metadata. For that, two metadata files are required , namely s_subsets.tsv and a_attributes.tsv . (1) The subset metadata file (s_subsets.tsv) makes it possible to associate each data subset with a key concept corresponding to the main entity of the subset, each subset being stored as a file (the grey rectangle). It also defines for each subset the link to the subset it originates. (2) The attribute metadata file (a_attributes.tsv) allows to annotate each attribute (concept/variable) with some minimal but relevant metadata, such as: its description with its unit, the data type, but also its category. The category defined by controlled vocabulary (CV) is used to specify the type of each variable. In each of these two files (subsets and attributes), it is possible to annotate each of the terms with unambiguous definitions through links to accessible definitions (standardized CV terms). Thus constructed, this metadata constitutes a dictionary describing each file (subsets) as well as all the columns of the tables (attributes) offering a better guarantee in the correct (re)use of the data for users who have not produced these data. These metadata lets therefore non-expert users explore and visualize your data. By making data interoperable and reusable by both humans and machines, it also encourages data dissemination according to FAIR principles. s_subsets.tsv \u00b6 A metadata file allowing to associate with each subset of data a key concept corresponding to the main entity of the subset and the relations of the type \" obtainedFrom \" between these concepts. The full version of this metadata file for the FRIM1 dataset can be accessed online using API: subset metadata A column : Unique rank number of the data subset B column : father rank the rank of the data subset (father rank) from which the data subset was obtained, implying a 'obtained from' relationship between both data subsets C column : short name of the data subset i.e the entity name associated to the subset in the form of a short name only the alphanumerical characters and the underscore are allowed (i.e. 'a-z', 'A-Z', '0-9' and '_'). D column : The identifier attributes should be the only attribute declared as ' identifier ' in the ' category ' column in the a_attributes.tsv file ( D column ) should be available as a column item in the corresponding data subset file E column : names of the files only the alphanumerical characters, the underscore and the dot are allowed ( i.e. 'a-z', 'A-Z', '0-9' and '_' , '.' ) Moreover, these names should not start with a digit! F column : description of the entity the allowed characters are: 0-9 a-z A-Z , : + * () [] {} - % ! | / . ? G & H columns : annotations based on ontology use an ontology term (G) along with its corresponding URL (H) Sites such as BioPortal or AgroPortal are great sources for finding Control Vocabulary (CV) based on ontology These annotations are optional but it is mandatory to specify at least one so that the data table has exactly 8 columns. So a minimum good practice is to put for the first an NULL annotation e.g. 'NULL' (G), '/voc/null' (H). a_attributes.tsv \u00b6 A metadata file allowing each attribute (concept/variable) to be annotated with some minimal but relevant metadata The full version of this metadata file for the FRIM1 dataset can be accessed online using API: attribute metadata A column : Short names of the data subsets must be declared in the s_subsets.tsv file ( C column ) and vice versa. only the alphanumerical characters and the underscore are allowed (i.e. 'a-z', 'A-Z', '0-9' and '_'). B column : attributes short name of the variables (data table column names) only the alphanumerical characters and the underscore are allowed (i.e. 'a-z', 'A-Z', '0-9' and '_'). a set of variables that may include observed or measured variables (quantitative or qualitative), controlled independent variables (factors) and identifiers. one and only one attribute ( B column ) must be declared as ' identifier ' in the ' category ' column ( D column ) per data subset ( A column ) This column can be easily filled by copy-paste from the data table files, as shown below: C column : Entry gives opportunity to make a selection on the attributes via web-services by associating them an alias name (called an \u201centry\u201d). only the alphanumerical characters and the underscore are allowed (i.e. 'a-z', 'A-Z', '0-9' and '_'). Example: we put \u2018 treatment \u2019 as an entry for the \u2018 Treatment \u2019 factor. By this, we could retrieve all samples data for Treatment equal to \u2018 Control \u2019 by applying the API query, e.g. https://pmb-bordeaux.fr/getdata/query/frim1/(samples)/treatment/Control?format=xml See Very detailed example of API querying D column : Category has a limited choice of words: the set of terms are fixed, namely: ' identifier ', ' factor ', ' quantitative ', ' qualitative '. Leave as blank otherwise. dependent variables resulting of effects of some controlled experimental factors must be defined as \u2018 factor \u2019. Each entity identifier must be defined as \u2018 identifier \u2019 Variables can be defined as \u2018 quantitative \u2019 or \u2018 qualitative \u2019. External identifier which serves as a link must have an empty cell. E column : data types the allowed names are restricted to ' numeric ' or ' string '. All ' quantitative ' variables must be \u2018 numeric ' type and it is preferable if the ' qualitative ' variables are \u2018 string ' type. F column : description of the attribute the allowed characters are: 0-9 a-z A-Z , : + * () [] {} - % ! | / . ? If a unit must be specified for a variable, we can add it in brackets at the end of the text of the description G & H columns : annotations based on ontology use an ontology term (G) along with its corresponding URL (H) Sites such as BioPortal or AgroPortal are great sources for finding Control Vocabulary (CV) based on ontology These annotations are optional but it is mandatory to specify at least one so that the data table has exactly 8 columns. So a minimum good practice is to put for all attributes of type 'identifier' the corresponding annotation within the EDAM ontology , i.e. 'identifier' (G), http://edamontology.org/data_0842 (H) 4 - Additional information \u00b6 Although descriptive metadata have to be associated with a suitable data repository in order to support data publishing (see Publish your data ), it is nevertheless possible to add descriptive information about the dataset. This information must be provided in markdown format , a text format with a simple formatting syntax. The name of the file must be ' infos.md ' and add it in the same directory as the dataset. It is also possible to add images. To do this, you must create a directory named ' images '. To reference these images within the infos.md file, use the @@IMAGE@@ macro as a path. This macro will be automatically replaced by the correct URL when the page is loaded. An example is given below : ![frim1](@@IMAGE@@/tomato_icon.png) <font size=\"+3\"> [Tomato][1] </font> In the same way, it is also possible to add links to PDF files. To do this, you must create a directory named ' pdf '. To add links to PDF files within the infos.md file, use the @@PDF@@ macro as a path. This macro will be automatically replaced by the correct URL when the page is loaded. An example is given below : * [Protocol](@@PDF@@/protocol.pdf) 5 - Final checking \u00b6 So now our ODAM dataset directory should look like this: To complete this phase of data preparation, here is a list of some points to check and summarized below: # Note Description 1 A directory named as the dataset name should be actually created in the data repository; Be careful in the spelling, see note 6; 2 The s_subsets.tsv and a_attributes.tsv files should be present in the data repository. 3 All data subset files declared in the s_subsets.tsv (col. E) should be available in the data repository 4 To be sure to have the right format, do a 'copy' of data from the spreadsheet then 'paste' them into a new file, then 'save as TSV format (separator: a tab character)' 5 1) all subsets in the a_attributes.tsv file (col. A) should be declared in the s_subsets.tsv file (col. C) 2) all subsets in the s_subsets.tsv file (col. C) should be declared in the a_attributes.tsv file (col. A) 3) all attribute names in the a_attributes.tsv file (col. B) should be available as a column in the corresponding data subset file declared in the s_subsets.tsv file (col. E) 6 Be careful in the spelling: 1) for data subset file names (col. E in s_subsets.tsv), identifier name (col. D in s_subsets.tsv), attribute names (col. B in a_attributes.tsv), subset short names (col. C in s_subsets.tsv and col. A in a_attributes.tsv) and entry names (col. C in a_attributes.tsv), only the alphanumerical characters and the underscore are allowed (i.e. 'a-z', 'A-Z', '0-9' and '_'). Moreover, these names should not start with a digit! 2) for categorical names (col. D in a_attributes.tsv), the number of terms and their spelling are fixed, namely: 'identifier', 'factor', 'quantitative\u2019, 'qualitative'. 3) for type (col. E in a_attributes.tsv), the allowed names are restricted to 'numeric' or 'string'. 4) for description, the allowed characters are: 0-9 a-z A-Z , : + * () [] 7 Identifiers declared in the s_subsets.tsv file (col. D) 1) should be declared as 'identifier' in the 'category' column in the a_attributes.tsv file (col. D) 2) should be available as a column item in the corresponding data subset file 3) should be the only one attribute declared as identifier for the corresponding data subset file in the a_attributes.tsv file (col. D) 8 Each subset having a 'father_rank' greater than 0 in the s_subsets.tsv file (col. B) 1) should include in its data file a column corresponding to the identifier of the subset to which it is linked (i.e. corresponding to the father_rank in col. A) 2) should have the linked subset identifier with no category (i.e. empty) in the a_attributes.tsv file (col. D), except if the subset and the linked subset have the same identifier Fortunately, all of these checks can be done for you. See how it looks for our complete online example: https://pmb-bordeaux.fr/getdata/check/frim1 This assumes that you have installed ODAM software (see Installation ) and that you know how to use the API (see Web API ). A data entity is an object in a data model. Data is typically designed by breaking things down into their smallest parts that are useful for representing data relationships. For example, a plant can generate a list of samples. Each sample can be associated with several types of analytical variables. All three objects: plant, sample and type of analytical variables are considered data entities. \u21a9 \u21a9 Data attributes are characteristics of a data object. From data science view, they are the features of a data entity. They exist most often as a column in a data table. \u21a9 A factor of an experiment is a controlled independent variable; a variable whose levels are set by the experimenter. Treatments (control vs. stress), genotype (WT vs. mutant), the course of time (development stages) or even tissues, are typical factors of experiments. \u21a9 Identifiers precisely reference within the experiment each of the elements belonging to the same observation entity forming a coherent observation unit. For example, each plant, each sample has its own identifier, \u21a9 \u21a9 Quantitative data are values that describe a measurable quantity, in the form of numbers that can be calculated. \u21a9 Qualitative data describe qualities or characteristics. They answer questions such as \"what type\" or \"what category\". These values are no longer numbers, but a set of modalities. These values cannot be calculated. \u21a9","title":"Data collection and preparation"},{"location":"data-preparation/#data-preparation-protocol-for-odam-compliance","text":"The purpose of this protocol is to describe all the steps involved in collecting, preparing and annotating the data from an experiment associated with an experimental design ( DoE ) that will then allow the user to benefit from the services offered by ODAM. The overall approach is based on good data management practices concerning data structuring and the description of structural metadata. Indeed, the strong point of the approach is to define metadata in depth, i.e. at the level of the data itself (i.e. metadata at column-level such as factors, variables...) and not just descriptive metadata on the top of the dataset. Thus, having structural metadata allows datasets to achieve a higher level of interoperability and greatly facilitates functional interconnection and analysis in a broader context. Based on an example In order to illustrate the different stages of this protocol, we have chosen an example from an experiment on tomato fruits grown in a greenhouse. The aim of this study was to build a model of fruit growth. For this, a certain amount of data was required, and we will limit ourselves to some of them in order to simplify the size of the data set. See the complete example: Data explorer https://pmb-bordeaux.fr/dataexplorer/?ds=frim1 Dataverse : https://doi.org/10.15454/95JUTK","title":"Data Preparation Protocol for ODAM Compliance"},{"location":"data-preparation/#1-data-gathering","text":"In our data subset example, we have 5 data files, one by type of object ( plants, harvests, samples, compounds and enzymes ). 5 different entities within the study, each corresponding to a file of data tables: plants, harvests, samples, compounds and enzymes 2 factors: Treatment, Development stages 53 quantitative variables: compounds (12) + enzymes (38) + weight, height, diameter (3) 1 - First, we put them under the same directory by giving it a name corresponding to the study or project (e.g. acronym of the project with a suffix corresponding to a study) 2 - Data subsets files must be compliant with the TSV standard ( Tab-Separator-Values ). So an ODAM dataset is a bundle that contains a set of TSV files. The TSV files are simple tables containing the data of the dataset. In choosing this format, we follow the 5 gold stars principle, considered as a good practice and a necessary and indispensable step towards \"Linked Open Data\". Note : Data files must have the extension ' txt ' in order to distinguish them from metadata files, see below. Advice: To be sure to have the right format, do a copy of data from the spreadsheet then paste them into a new file, then save as TSV format (separator: a tab character)'","title":"1- Data Gathering"},{"location":"data-preparation/#2-data-structure-and-organization","text":"Since all the experimental data tables were generated as part of an experiment associated with a Design of Experiment ( DoE ), each file thus contains data acquired sequentially as the experiment progressed. There must therefore be a link between each file, i.e. information that connects them together. In most cases (if not all), this information corresponds to identifiers 4 that make it possible to precisely reference within the experiment each of the elements belonging to the same observation entity 1 forming a coherent observation unit. For example, each plant, each sample has its own identifier, and each of these entities corresponds to a separate data file. Well organized data means that each data table must be correctly structured, i.e.: Each variable forms a column, Each observation forms a line, Each type of \"unit observational\" (defined as an entity) forms a table, i.e. a file, Each data table file must have a column defined as an identifier (similar as a primary key) corresponding to each observation of the entity (e.g. plant, sample, \u2026), Missing values can either be an empty cell or have the value 'NA', The header names must short without special characters. Only use the alphanumerical characters, and the underline character as word separator, The file should only contain data in matrix form and nothing else, i.e. no annotation on top, bottom, or sides. Example of the \u2018samples.txt\u2019 file: The files generated during data collection have to be organized according to the entity-relationship model similar to relational database management systems (RDBMS). Indeed, each entity 1 corresponds to a type of collected data (samples, compounds, ...) for which is associated a set of attributes 2 , i.e. a set of variables that may include observed or measured variables (quantitative or qualitative), controlled independent variables (factors) and an identifier. Then, a link is established for each subset with the subset from which it was obtained, so that the links can be interpreted as \" obtained from \u201d as shown in the figure below: We have to organize your data subsets so that links could be established between them. In practical, it means to add when necessary a column (colored in green in the figure below) containing the identifiers corresponding to the entity to which we want to connect the subset. It is to be noted that this duplication of identifiers must be the only redundant information, through all data subsets.","title":"2 - Data structure and organization"},{"location":"data-preparation/#3-structural-metadata","text":"ODAM provides a model for structuring both data and metadata that facilitates data handling and analysis. Whatever the kind of experiment, this assumes a design of experiment ( DoE ) involving individuals, samples or whatever things, as the main objects of study and producing several experimental data tables . This also assumes the observation of dependent variables resulting from effects of some controlled independent variables ( factors 3 ). Moreover, the objects of study usually have an identifier 4 for each of them, and the variables can be quantitative 5 or qualitative 6 . We can have either one entity within the study or several kinds, but in this latter case, it must exist a relationship between entities that we assume of \u201c obtained from \" type as describe above. Thus, the data table of samples can be viewed according to the repartition by category we have just introduced as shown below: a data entity (data subset 'samples') consisting of its attributes (columns) divided by category (identifier, factor, quantitative, qualitative) The four categories Identifier : precisely reference within the experiment each of the elements belonging to the same observation entity forming a coherent observation unit. For example, each plant, each sample has its own identifier, Factor : a factor of an experiment is a controlled independent variable; a variable whose levels are set by the experimenter. Treatments (control vs. stress), genotype (WT vs. mutant), the course of time (development stages) or even tissues, are typical factors of experiments. Quantitative : Quantitative data are values that describe a measurable quantity, in the form of numbers that can be calculated. Qualitative : Qualitative data describe qualities or characteristics. They answer questions such as \"what type\" or \"what category\". These values are no longer numbers, but a set of modalities. These values cannot be calculated. In order to allow data to be ODAM-compliant, we need two specific files (subsets & attributes) to describe the structural metadata of the whole dataset, i.e. some minimal but relevant metadata. For that, two metadata files are required , namely s_subsets.tsv and a_attributes.tsv . (1) The subset metadata file (s_subsets.tsv) makes it possible to associate each data subset with a key concept corresponding to the main entity of the subset, each subset being stored as a file (the grey rectangle). It also defines for each subset the link to the subset it originates. (2) The attribute metadata file (a_attributes.tsv) allows to annotate each attribute (concept/variable) with some minimal but relevant metadata, such as: its description with its unit, the data type, but also its category. The category defined by controlled vocabulary (CV) is used to specify the type of each variable. In each of these two files (subsets and attributes), it is possible to annotate each of the terms with unambiguous definitions through links to accessible definitions (standardized CV terms). Thus constructed, this metadata constitutes a dictionary describing each file (subsets) as well as all the columns of the tables (attributes) offering a better guarantee in the correct (re)use of the data for users who have not produced these data. These metadata lets therefore non-expert users explore and visualize your data. By making data interoperable and reusable by both humans and machines, it also encourages data dissemination according to FAIR principles.","title":"3 - Structural Metadata"},{"location":"data-preparation/#s_subsetstsv","text":"A metadata file allowing to associate with each subset of data a key concept corresponding to the main entity of the subset and the relations of the type \" obtainedFrom \" between these concepts. The full version of this metadata file for the FRIM1 dataset can be accessed online using API: subset metadata A column : Unique rank number of the data subset B column : father rank the rank of the data subset (father rank) from which the data subset was obtained, implying a 'obtained from' relationship between both data subsets C column : short name of the data subset i.e the entity name associated to the subset in the form of a short name only the alphanumerical characters and the underscore are allowed (i.e. 'a-z', 'A-Z', '0-9' and '_'). D column : The identifier attributes should be the only attribute declared as ' identifier ' in the ' category ' column in the a_attributes.tsv file ( D column ) should be available as a column item in the corresponding data subset file E column : names of the files only the alphanumerical characters, the underscore and the dot are allowed ( i.e. 'a-z', 'A-Z', '0-9' and '_' , '.' ) Moreover, these names should not start with a digit! F column : description of the entity the allowed characters are: 0-9 a-z A-Z , : + * () [] {} - % ! | / . ? G & H columns : annotations based on ontology use an ontology term (G) along with its corresponding URL (H) Sites such as BioPortal or AgroPortal are great sources for finding Control Vocabulary (CV) based on ontology These annotations are optional but it is mandatory to specify at least one so that the data table has exactly 8 columns. So a minimum good practice is to put for the first an NULL annotation e.g. 'NULL' (G), '/voc/null' (H).","title":"s_subsets.tsv"},{"location":"data-preparation/#a_attributestsv","text":"A metadata file allowing each attribute (concept/variable) to be annotated with some minimal but relevant metadata The full version of this metadata file for the FRIM1 dataset can be accessed online using API: attribute metadata A column : Short names of the data subsets must be declared in the s_subsets.tsv file ( C column ) and vice versa. only the alphanumerical characters and the underscore are allowed (i.e. 'a-z', 'A-Z', '0-9' and '_'). B column : attributes short name of the variables (data table column names) only the alphanumerical characters and the underscore are allowed (i.e. 'a-z', 'A-Z', '0-9' and '_'). a set of variables that may include observed or measured variables (quantitative or qualitative), controlled independent variables (factors) and identifiers. one and only one attribute ( B column ) must be declared as ' identifier ' in the ' category ' column ( D column ) per data subset ( A column ) This column can be easily filled by copy-paste from the data table files, as shown below: C column : Entry gives opportunity to make a selection on the attributes via web-services by associating them an alias name (called an \u201centry\u201d). only the alphanumerical characters and the underscore are allowed (i.e. 'a-z', 'A-Z', '0-9' and '_'). Example: we put \u2018 treatment \u2019 as an entry for the \u2018 Treatment \u2019 factor. By this, we could retrieve all samples data for Treatment equal to \u2018 Control \u2019 by applying the API query, e.g. https://pmb-bordeaux.fr/getdata/query/frim1/(samples)/treatment/Control?format=xml See Very detailed example of API querying D column : Category has a limited choice of words: the set of terms are fixed, namely: ' identifier ', ' factor ', ' quantitative ', ' qualitative '. Leave as blank otherwise. dependent variables resulting of effects of some controlled experimental factors must be defined as \u2018 factor \u2019. Each entity identifier must be defined as \u2018 identifier \u2019 Variables can be defined as \u2018 quantitative \u2019 or \u2018 qualitative \u2019. External identifier which serves as a link must have an empty cell. E column : data types the allowed names are restricted to ' numeric ' or ' string '. All ' quantitative ' variables must be \u2018 numeric ' type and it is preferable if the ' qualitative ' variables are \u2018 string ' type. F column : description of the attribute the allowed characters are: 0-9 a-z A-Z , : + * () [] {} - % ! | / . ? If a unit must be specified for a variable, we can add it in brackets at the end of the text of the description G & H columns : annotations based on ontology use an ontology term (G) along with its corresponding URL (H) Sites such as BioPortal or AgroPortal are great sources for finding Control Vocabulary (CV) based on ontology These annotations are optional but it is mandatory to specify at least one so that the data table has exactly 8 columns. So a minimum good practice is to put for all attributes of type 'identifier' the corresponding annotation within the EDAM ontology , i.e. 'identifier' (G), http://edamontology.org/data_0842 (H)","title":"a_attributes.tsv"},{"location":"data-preparation/#4-additional-information","text":"Although descriptive metadata have to be associated with a suitable data repository in order to support data publishing (see Publish your data ), it is nevertheless possible to add descriptive information about the dataset. This information must be provided in markdown format , a text format with a simple formatting syntax. The name of the file must be ' infos.md ' and add it in the same directory as the dataset. It is also possible to add images. To do this, you must create a directory named ' images '. To reference these images within the infos.md file, use the @@IMAGE@@ macro as a path. This macro will be automatically replaced by the correct URL when the page is loaded. An example is given below : ![frim1](@@IMAGE@@/tomato_icon.png) <font size=\"+3\"> [Tomato][1] </font> In the same way, it is also possible to add links to PDF files. To do this, you must create a directory named ' pdf '. To add links to PDF files within the infos.md file, use the @@PDF@@ macro as a path. This macro will be automatically replaced by the correct URL when the page is loaded. An example is given below : * [Protocol](@@PDF@@/protocol.pdf)","title":"4 - Additional information"},{"location":"data-preparation/#5-final-checking","text":"So now our ODAM dataset directory should look like this: To complete this phase of data preparation, here is a list of some points to check and summarized below: # Note Description 1 A directory named as the dataset name should be actually created in the data repository; Be careful in the spelling, see note 6; 2 The s_subsets.tsv and a_attributes.tsv files should be present in the data repository. 3 All data subset files declared in the s_subsets.tsv (col. E) should be available in the data repository 4 To be sure to have the right format, do a 'copy' of data from the spreadsheet then 'paste' them into a new file, then 'save as TSV format (separator: a tab character)' 5 1) all subsets in the a_attributes.tsv file (col. A) should be declared in the s_subsets.tsv file (col. C) 2) all subsets in the s_subsets.tsv file (col. C) should be declared in the a_attributes.tsv file (col. A) 3) all attribute names in the a_attributes.tsv file (col. B) should be available as a column in the corresponding data subset file declared in the s_subsets.tsv file (col. E) 6 Be careful in the spelling: 1) for data subset file names (col. E in s_subsets.tsv), identifier name (col. D in s_subsets.tsv), attribute names (col. B in a_attributes.tsv), subset short names (col. C in s_subsets.tsv and col. A in a_attributes.tsv) and entry names (col. C in a_attributes.tsv), only the alphanumerical characters and the underscore are allowed (i.e. 'a-z', 'A-Z', '0-9' and '_'). Moreover, these names should not start with a digit! 2) for categorical names (col. D in a_attributes.tsv), the number of terms and their spelling are fixed, namely: 'identifier', 'factor', 'quantitative\u2019, 'qualitative'. 3) for type (col. E in a_attributes.tsv), the allowed names are restricted to 'numeric' or 'string'. 4) for description, the allowed characters are: 0-9 a-z A-Z , : + * () [] 7 Identifiers declared in the s_subsets.tsv file (col. D) 1) should be declared as 'identifier' in the 'category' column in the a_attributes.tsv file (col. D) 2) should be available as a column item in the corresponding data subset file 3) should be the only one attribute declared as identifier for the corresponding data subset file in the a_attributes.tsv file (col. D) 8 Each subset having a 'father_rank' greater than 0 in the s_subsets.tsv file (col. B) 1) should include in its data file a column corresponding to the identifier of the subset to which it is linked (i.e. corresponding to the father_rank in col. A) 2) should have the linked subset identifier with no category (i.e. empty) in the a_attributes.tsv file (col. D), except if the subset and the linked subset have the same identifier Fortunately, all of these checks can be done for you. See how it looks for our complete online example: https://pmb-bordeaux.fr/getdata/check/frim1 This assumes that you have installed ODAM software (see Installation ) and that you know how to use the API (see Web API ). A data entity is an object in a data model. Data is typically designed by breaking things down into their smallest parts that are useful for representing data relationships. For example, a plant can generate a list of samples. Each sample can be associated with several types of analytical variables. All three objects: plant, sample and type of analytical variables are considered data entities. \u21a9 \u21a9 Data attributes are characteristics of a data object. From data science view, they are the features of a data entity. They exist most often as a column in a data table. \u21a9 A factor of an experiment is a controlled independent variable; a variable whose levels are set by the experimenter. Treatments (control vs. stress), genotype (WT vs. mutant), the course of time (development stages) or even tissues, are typical factors of experiments. \u21a9 Identifiers precisely reference within the experiment each of the elements belonging to the same observation entity forming a coherent observation unit. For example, each plant, each sample has its own identifier, \u21a9 \u21a9 Quantitative data are values that describe a measurable quantity, in the form of numbers that can be calculated. \u21a9 Qualitative data describe qualities or characteristics. They answer questions such as \"what type\" or \"what category\". These values are no longer numbers, but a set of modalities. These values cannot be calculated. \u21a9","title":"5 - Final checking"},{"location":"data-preparation/collection/","text":"Collection of Datasets \u00b6 A collection in the sense of ODAM is a set of datasets. The link between datasets can be very varied, either a project, a theme, or anything else. For example, if you have produced several datasets within the same project, it can be interesting to manage them by creating a link between them. This link is precisely what in ODAM we call a collection. In ODAM, a collection has exactly the same structure as a dataset, i.e. there are data files (only one in fact) and the two metadata files (s_subsets.tsv & a_attributes.tsv) previously described in the main section (see Data preparation ). By keeping the same structure as a dataset, it means that we can use the same API for both a collection and a datatset. You can see an example of a collection accessible online via the data explorer. Content of the collection \u00b6 only one file is used to describe the content of the collection. Here is its structure : A column : Data type. Put simply 'dataset' for a dataset or 'collection' for collection B column : the dataset identifier i.e the name of the dataset/collection directory C column : a short label more explicite than the dataset identifier must be less than 20 characters D column : API URL of the dataset or the collection by default, left empty this field. This means that the data are on the same server than the collection Otherwise, put the URL of the API. E column : Description of the dataset or the collection the allowed characters are: 0-9 a-z A-Z , : + * () [] {} - % ! | / . ? Remarks : You can not only manage collections of datsets but also collections of collections and thus manage a hierarchy in your datasets. A dataset or collection can be declared as a member of several collections. Metadata of the collection \u00b6 In addition to the file describing the content of the collection, it is necessary, as with any ODAM dataset, to attach the two metadata files previously described in the main section, namely s_subsets.tsv and a_attributes.tsv (see Data preparation ). The difference is that for a collection these two files are already predefined for any collection and very little modification is required. s_subsets.tsv \u00b6 Only the description must be changed according to your collection. This description must be as short as possible (20 characters), because it appears in the data explorer as the title of the collection in the top bar. You can also change the name of the file describing the content of the collection but this is not necessary. a_attributes.tsv \u00b6 In this file, nothing must be changed. It must be added as is. Additional information \u00b6 In the same way as with a dataset (reminder: a collection is seen as a dataset in ODAM), it is possible to add an information file (i.e. infos.md), as well as images and PDF files. ( See Data preparation ) Download \u00b6 You can download the files describe in this section as template: collection.txt s_subsets.tsv a_attributes.tsv","title":"Collection of datasets"},{"location":"data-preparation/collection/#collection-of-datasets","text":"A collection in the sense of ODAM is a set of datasets. The link between datasets can be very varied, either a project, a theme, or anything else. For example, if you have produced several datasets within the same project, it can be interesting to manage them by creating a link between them. This link is precisely what in ODAM we call a collection. In ODAM, a collection has exactly the same structure as a dataset, i.e. there are data files (only one in fact) and the two metadata files (s_subsets.tsv & a_attributes.tsv) previously described in the main section (see Data preparation ). By keeping the same structure as a dataset, it means that we can use the same API for both a collection and a datatset. You can see an example of a collection accessible online via the data explorer.","title":"Collection of Datasets"},{"location":"data-preparation/collection/#content-of-the-collection","text":"only one file is used to describe the content of the collection. Here is its structure : A column : Data type. Put simply 'dataset' for a dataset or 'collection' for collection B column : the dataset identifier i.e the name of the dataset/collection directory C column : a short label more explicite than the dataset identifier must be less than 20 characters D column : API URL of the dataset or the collection by default, left empty this field. This means that the data are on the same server than the collection Otherwise, put the URL of the API. E column : Description of the dataset or the collection the allowed characters are: 0-9 a-z A-Z , : + * () [] {} - % ! | / . ? Remarks : You can not only manage collections of datsets but also collections of collections and thus manage a hierarchy in your datasets. A dataset or collection can be declared as a member of several collections.","title":"Content of the collection"},{"location":"data-preparation/collection/#metadata-of-the-collection","text":"In addition to the file describing the content of the collection, it is necessary, as with any ODAM dataset, to attach the two metadata files previously described in the main section, namely s_subsets.tsv and a_attributes.tsv (see Data preparation ). The difference is that for a collection these two files are already predefined for any collection and very little modification is required.","title":"Metadata of the collection"},{"location":"data-preparation/collection/#s_subsetstsv","text":"Only the description must be changed according to your collection. This description must be as short as possible (20 characters), because it appears in the data explorer as the title of the collection in the top bar. You can also change the name of the file describing the content of the collection but this is not necessary.","title":"s_subsets.tsv"},{"location":"data-preparation/collection/#a_attributestsv","text":"In this file, nothing must be changed. It must be added as is.","title":"a_attributes.tsv"},{"location":"data-preparation/collection/#additional-information","text":"In the same way as with a dataset (reminder: a collection is seen as a dataset in ODAM), it is possible to add an information file (i.e. infos.md), as well as images and PDF files. ( See Data preparation )","title":"Additional information"},{"location":"data-preparation/collection/#download","text":"You can download the files describe in this section as template: collection.txt s_subsets.tsv a_attributes.tsv","title":"Download"},{"location":"json-schema/","text":"ODAM datapackage based on JSON-Schema \u00b6 A data package is a simple container format based on JSON Schema specifications used to describe and package a collection of data. Defining an explicit schema for structural metadata allows machines to better interpret the data for reuse. Thus, when disseminating data, a file named datapackage.json by convention can be added to the collection of your data files. This datapackage.json file contains all structural metadata along with unambiguous definitions of all internal elements (e.g. column definitions, units of measurement), through links to accessible (standard) definitions. ODAM data package schema is very close to the Frictionless Data framework. graph TD A(Data Source) H(Table Schema) B(ODAM Table Schema) C(CSV Data Descriptor) D(ODAM Categories) E(ODAM Data Resource) F(Data Package) G(ODAM Data package) K(ODAM CV Term) H-->B K-->B D-->B A-->E B-->E C-->E E-->G F-->G click A \"https://specs.frictionlessdata.io/data-resource/\" _blank click C \"https://specs.frictionlessdata.io/csv-dialect/\" _blank click E \"./odam-data-resource\" click F \"https://specs.frictionlessdata.io/data-package/\" _blank click G \"./odam-data-package\" click H \"https://specs.frictionlessdata.io/table-schema/\" _blank classDef ODAM fill:#f9f class B,D,E,G,K ODAM How the ODAM Data Package is composed out of Frictionless Data specs odam-data-package.json : JSON Schema for ODAM datapackage odam-data-resource.json : JSON Schema for ODAM dataresource Based on structural metadata previously defined using the Data preparation protocol for ODAM compliance , the datapackage.json file can be generated directly from the ODAM API by specifying ' /datapackage ' at the end of the request. By default, the reference to the data files is relative. To have a URL as reference for the data files, it is necessary to add at the end of the request ' ?links=1 ' ODAM data-package on github https://github.com/djacob65/odam-datapackage data validation by goodtables.io : Frim1 datapackage as an example The Identifiers.org URI for the Frim1 dataset: odam:frim1 (after de-reification) returns the structural metadata returned according to the JSON datapackage schema - https://identifiers.org/odam:frim1 Data Package and FAIR Frictionless Data and FAIR Research Principles Visible, findable, shareable data Datapackage on FAIRsharing Frictionless Data Specifications http://specs.frictionlessdata.io/ https://github.com/frictionlessdata/specs/tree/master/schemas","title":"JSON Schema"},{"location":"json-schema/#odam-datapackage-based-on-json-schema","text":"A data package is a simple container format based on JSON Schema specifications used to describe and package a collection of data. Defining an explicit schema for structural metadata allows machines to better interpret the data for reuse. Thus, when disseminating data, a file named datapackage.json by convention can be added to the collection of your data files. This datapackage.json file contains all structural metadata along with unambiguous definitions of all internal elements (e.g. column definitions, units of measurement), through links to accessible (standard) definitions. ODAM data package schema is very close to the Frictionless Data framework. graph TD A(Data Source) H(Table Schema) B(ODAM Table Schema) C(CSV Data Descriptor) D(ODAM Categories) E(ODAM Data Resource) F(Data Package) G(ODAM Data package) K(ODAM CV Term) H-->B K-->B D-->B A-->E B-->E C-->E E-->G F-->G click A \"https://specs.frictionlessdata.io/data-resource/\" _blank click C \"https://specs.frictionlessdata.io/csv-dialect/\" _blank click E \"./odam-data-resource\" click F \"https://specs.frictionlessdata.io/data-package/\" _blank click G \"./odam-data-package\" click H \"https://specs.frictionlessdata.io/table-schema/\" _blank classDef ODAM fill:#f9f class B,D,E,G,K ODAM How the ODAM Data Package is composed out of Frictionless Data specs odam-data-package.json : JSON Schema for ODAM datapackage odam-data-resource.json : JSON Schema for ODAM dataresource Based on structural metadata previously defined using the Data preparation protocol for ODAM compliance , the datapackage.json file can be generated directly from the ODAM API by specifying ' /datapackage ' at the end of the request. By default, the reference to the data files is relative. To have a URL as reference for the data files, it is necessary to add at the end of the request ' ?links=1 ' ODAM data-package on github https://github.com/djacob65/odam-datapackage data validation by goodtables.io : Frim1 datapackage as an example The Identifiers.org URI for the Frim1 dataset: odam:frim1 (after de-reification) returns the structural metadata returned according to the JSON datapackage schema - https://identifiers.org/odam:frim1 Data Package and FAIR Frictionless Data and FAIR Research Principles Visible, findable, shareable data Datapackage on FAIRsharing Frictionless Data Specifications http://specs.frictionlessdata.io/ https://github.com/frictionlessdata/specs/tree/master/schemas","title":"ODAM datapackage based on JSON-Schema"},{"location":"json-schema/dataverse/","text":"ODAM: Datapackage from a Dataverse repository \u00b6 Example of a session with R showing how it is possible to retrieve an ODAM datapackage using a keyword from Data INRAE 1 , a Dataverse repository. 1 Institut National de Recherche pour l'Agriculture, l'Alimentation et l'Environnement. (2018). Data INRAE. https://doi.org/10.14758/9T8G-WJ20 Introduction to Dataverse with R https://cran.r-project.org/web/packages/dataverse/vignettes/A-introduction.html Code packages <- c ( 'httr' , 'jsonlite' , 'jsonvalidate' , 'dataverse' ) if ( length ( setdiff ( packages , rownames ( installed.packages ()))) > 0 ) { install.packages ( setdiff ( packages , rownames ( installed.packages ())), repos = 'http://cran.rstudio.com' ) } library ( dataverse ) library ( httr ) library ( jsonvalidate ) library ( jsonlite ) # User Parameters dataverse_server <- \"data.inra.fr\" dataset_keyword <- \"tomato\" url_schema <- 'https://inrae.github.io/ODAM/json-schema/odam-data-package.json' # Define the dataverse server as an env. variable Sys.setenv ( \"DATAVERSE_SERVER\" = dataverse_server ) # Data Search ds <- dataverse :: dataverse_search ( dataset_keyword , \"dataset\" ) # Check if a dataset found with the right type if ( dim ( ds ) [1] == 0 ) stop ( \"No dataset found\" ) if ( length ( \"dataset\" %in% ds $ type ) == 0 ) stop ( \"No dataset found with the right type\" ) # Get list of files ds <- ds[ ds $ type == \"dataset\" , ] # search for 'datapackage.json' ids <- c () for ( i in ds $ global_id ) { if ( length ( dataverse :: dataset_files ( i )) == 0 ) next dflist <- as.data.frame ( t ( simplify2array ( dataverse :: dataset_files ( i )))) if ( sum ( dflist $ label %in% \"datapackage.json\" )) ids <- c ( ids , i ) } # check if successful search if ( length ( ids ) == 0 ) stop ( \"No datapackage found\" ) # if many, take the first id <- ids[1] cat ( \" Dataset found : \" , id , \"\\n\" ) dflist <- as.data.frame ( t ( simplify2array ( dataverse :: dataset_files ( id )))) # Get the datapackage.json content file idx <- which ( \"datapackage.json\" %in% dflist $ label ) file_id <- dflist $ dataFile[[idx]] $ id dp_json <- rawToChar ( dataverse :: get_file ( file_id )) if ( is.null ( dp_json )) stop ( \"the datapackage returned as null\" ) # Get the ODAM data package schema response <- httr :: GET ( url_schema , config ( sslversion = 6 , ssl_verifypeer = 1 )) if ( response $ status_code != 200 ) stop ( \"Error while getting the ODAM data package schema\" ) schema <- httr :: content ( response , as = 'text' ) # Validate the JSON against the ODAM data package schema if ( ! jsonvalidate :: json_validate ( dp_json , schema ) ) stop ( \"the returned datapackage is not a valid ODAM datapackage\" ) # Parse the JSON object to a data.frame dp <- jsonlite :: fromJSON ( dp_json ) # View metadata sapply ( ls ( dflist ), function ( x ){ dflist[ [as.character ( x ) ]][[idx]] }) # Do something with dp ... dp $ resources[ c ( 'name' , 'title' , 'cv_term' ) ] Output $`categories` [1] \"data\" \"datapackage\" \"odam\" \"TSV\" $dataFile $dataFile$`id` [1] 100141 $dataFile$persistentId [1] \"doi:10.15454/95JUTK/SLKZUA\" $dataFile$pidURL [1] \"https://doi.org/10.15454/95JUTK/SLKZUA\" $dataFile$filename [1] \"datapackage.json\" $dataFile$contentType [1] \"application/json\" $dataFile$filesize [1] 46674 $dataFile$description [1] \"ODAM datapackage based on JSON Schema\" $dataFile$storageIdentifier [1] \"s3://prod-datainra:17352703ae5-a43a037ccfaa\" $dataFile$rootDataFileId [1] -1 $dataFile$md5 [1] \"16fc7594899b640028b3d9c634e85d1e\" $dataFile$checksum $dataFile$checksum$`type` [1] \"MD5\" $dataFile$checksum$value [1] \"16fc7594899b640028b3d9c634e85d1e\" $datasetVersionId [1] 261953 $description [1] \"ODAM datapackage based on JSON Schema\" $label [1] \"datapackage.json\" $restricted [1] FALSE $version [1] 1 name title cv_term.label cv_term.path 1 plants Plant features whole plant http://purl.obolibrary.org/obo/PO_0000003 2 samples Sample features organ harvesting http://purl.obolibrary.org/obo/OBI_1110046 3 aliquots Aliquots features organ harvesting http://purl.obolibrary.org/obo/OBI_1110046 4 cellwall_metabo Cell wall Compound quantifications chemical entity http://purl.obolibrary.org/obo/CHEBI_24431 5 cellwall_metaboFW Cell Wall Compound quantifications (FW) chemical entity http://purl.obolibrary.org/obo/CHEBI_24431 6 activome Activome Features chemical entity http://purl.obolibrary.org/obo/CHEBI_24431 7 pools Pools of remaining pools organ harvesting http://purl.obolibrary.org/obo/OBI_1110046 8 qMS_metabo MS Compounds quantification chemical entity http://purl.obolibrary.org/obo/CHEBI_24431 9 qNMR_metabo NMR Compounds quantification chemical entity http://purl.obolibrary.org/obo/CHEBI_24431 10 plato_hexosesP Hexoses Phosphate chemical entity http://purl.obolibrary.org/obo/CHEBI_24431 11 lipids_AG Lipids AG chemical entity http://purl.obolibrary.org/obo/CHEBI_24431 12 AminoAcid Amino Acids chemical entity http://purl.obolibrary.org/obo/CHEBI_24431","title":"Datapackage from Dataverse"},{"location":"json-schema/dataverse/#odam-datapackage-from-a-dataverse-repository","text":"Example of a session with R showing how it is possible to retrieve an ODAM datapackage using a keyword from Data INRAE 1 , a Dataverse repository. 1 Institut National de Recherche pour l'Agriculture, l'Alimentation et l'Environnement. (2018). Data INRAE. https://doi.org/10.14758/9T8G-WJ20 Introduction to Dataverse with R https://cran.r-project.org/web/packages/dataverse/vignettes/A-introduction.html Code packages <- c ( 'httr' , 'jsonlite' , 'jsonvalidate' , 'dataverse' ) if ( length ( setdiff ( packages , rownames ( installed.packages ()))) > 0 ) { install.packages ( setdiff ( packages , rownames ( installed.packages ())), repos = 'http://cran.rstudio.com' ) } library ( dataverse ) library ( httr ) library ( jsonvalidate ) library ( jsonlite ) # User Parameters dataverse_server <- \"data.inra.fr\" dataset_keyword <- \"tomato\" url_schema <- 'https://inrae.github.io/ODAM/json-schema/odam-data-package.json' # Define the dataverse server as an env. variable Sys.setenv ( \"DATAVERSE_SERVER\" = dataverse_server ) # Data Search ds <- dataverse :: dataverse_search ( dataset_keyword , \"dataset\" ) # Check if a dataset found with the right type if ( dim ( ds ) [1] == 0 ) stop ( \"No dataset found\" ) if ( length ( \"dataset\" %in% ds $ type ) == 0 ) stop ( \"No dataset found with the right type\" ) # Get list of files ds <- ds[ ds $ type == \"dataset\" , ] # search for 'datapackage.json' ids <- c () for ( i in ds $ global_id ) { if ( length ( dataverse :: dataset_files ( i )) == 0 ) next dflist <- as.data.frame ( t ( simplify2array ( dataverse :: dataset_files ( i )))) if ( sum ( dflist $ label %in% \"datapackage.json\" )) ids <- c ( ids , i ) } # check if successful search if ( length ( ids ) == 0 ) stop ( \"No datapackage found\" ) # if many, take the first id <- ids[1] cat ( \" Dataset found : \" , id , \"\\n\" ) dflist <- as.data.frame ( t ( simplify2array ( dataverse :: dataset_files ( id )))) # Get the datapackage.json content file idx <- which ( \"datapackage.json\" %in% dflist $ label ) file_id <- dflist $ dataFile[[idx]] $ id dp_json <- rawToChar ( dataverse :: get_file ( file_id )) if ( is.null ( dp_json )) stop ( \"the datapackage returned as null\" ) # Get the ODAM data package schema response <- httr :: GET ( url_schema , config ( sslversion = 6 , ssl_verifypeer = 1 )) if ( response $ status_code != 200 ) stop ( \"Error while getting the ODAM data package schema\" ) schema <- httr :: content ( response , as = 'text' ) # Validate the JSON against the ODAM data package schema if ( ! jsonvalidate :: json_validate ( dp_json , schema ) ) stop ( \"the returned datapackage is not a valid ODAM datapackage\" ) # Parse the JSON object to a data.frame dp <- jsonlite :: fromJSON ( dp_json ) # View metadata sapply ( ls ( dflist ), function ( x ){ dflist[ [as.character ( x ) ]][[idx]] }) # Do something with dp ... dp $ resources[ c ( 'name' , 'title' , 'cv_term' ) ] Output $`categories` [1] \"data\" \"datapackage\" \"odam\" \"TSV\" $dataFile $dataFile$`id` [1] 100141 $dataFile$persistentId [1] \"doi:10.15454/95JUTK/SLKZUA\" $dataFile$pidURL [1] \"https://doi.org/10.15454/95JUTK/SLKZUA\" $dataFile$filename [1] \"datapackage.json\" $dataFile$contentType [1] \"application/json\" $dataFile$filesize [1] 46674 $dataFile$description [1] \"ODAM datapackage based on JSON Schema\" $dataFile$storageIdentifier [1] \"s3://prod-datainra:17352703ae5-a43a037ccfaa\" $dataFile$rootDataFileId [1] -1 $dataFile$md5 [1] \"16fc7594899b640028b3d9c634e85d1e\" $dataFile$checksum $dataFile$checksum$`type` [1] \"MD5\" $dataFile$checksum$value [1] \"16fc7594899b640028b3d9c634e85d1e\" $datasetVersionId [1] 261953 $description [1] \"ODAM datapackage based on JSON Schema\" $label [1] \"datapackage.json\" $restricted [1] FALSE $version [1] 1 name title cv_term.label cv_term.path 1 plants Plant features whole plant http://purl.obolibrary.org/obo/PO_0000003 2 samples Sample features organ harvesting http://purl.obolibrary.org/obo/OBI_1110046 3 aliquots Aliquots features organ harvesting http://purl.obolibrary.org/obo/OBI_1110046 4 cellwall_metabo Cell wall Compound quantifications chemical entity http://purl.obolibrary.org/obo/CHEBI_24431 5 cellwall_metaboFW Cell Wall Compound quantifications (FW) chemical entity http://purl.obolibrary.org/obo/CHEBI_24431 6 activome Activome Features chemical entity http://purl.obolibrary.org/obo/CHEBI_24431 7 pools Pools of remaining pools organ harvesting http://purl.obolibrary.org/obo/OBI_1110046 8 qMS_metabo MS Compounds quantification chemical entity http://purl.obolibrary.org/obo/CHEBI_24431 9 qNMR_metabo NMR Compounds quantification chemical entity http://purl.obolibrary.org/obo/CHEBI_24431 10 plato_hexosesP Hexoses Phosphate chemical entity http://purl.obolibrary.org/obo/CHEBI_24431 11 lipids_AG Lipids AG chemical entity http://purl.obolibrary.org/obo/CHEBI_24431 12 AminoAcid Amino Acids chemical entity http://purl.obolibrary.org/obo/CHEBI_24431","title":"ODAM: Datapackage from a Dataverse repository"},{"location":"json-schema/google-search/","text":"ODAM: Find a Dataverse dataset via Google Search \u00b6 The purpose of this R script is to test if a dataset can be \"findable\" by a search engine such as Google. For this test, we have chosen our example dataset 'frim1'. The searched keywords are thus ' frim1 ' associated with ' dataset '. See the code and its results. Step 1: Start by searching for the keywords in Google Search Step 2 - If found links in step 1, select those that correspond to a Dataverse dataset Step 3 - If found some in step 2, then retrieve the metadata See below the code and its results. Code packages <- c ( 'RCurl' , 'XML' , 'dataverse' ) if ( length ( setdiff ( packages , rownames ( installed.packages ()))) > 0 ) { install.packages ( setdiff ( packages , rownames ( installed.packages ())), repos = 'http://cran.rstudio.com' ) } library ( RCurl ) library ( XML ) library ( dataverse ) # Search terms searchTerms <- \"frim1 dataset\" language <- \"en\" # Collapse search terms. entry <- paste ( searchTerms , collapse = \"+\" ) # Do the search siteHTML <- RCurl :: getForm ( \"http://www.google.com/search\" , hl = \"en\" , lr = \"\" , q = entry , btnG = \"Search\" ) # Extract nodes with the links html <- XML :: htmlTreeParse ( siteHTML , useInternalNodes = TRUE , error = function ( ... ){}) nodes <- XML :: getNodeSet ( html , \"//div[@class='kCrYT']//a\" ) # Get all the links links <- sapply ( nodes , function ( x ) x <- XML :: xmlAttrs ( x ) [[ \"href\" ]] ) links <- links [grep ( \"google.com/search\" , links , invert = TRUE ) ] links <- gsub ( '/url.q=' , '' , gsub ( '&sa.+$' , '' , links )) # harmonizes the writing of URLs links <- gsub ( \"%3D\" , \"=\" , gsub ( \"%3F\" , \"?\" , links ) ) # Compile into a data.frame df <- data.frame ( label = sapply ( XML :: getNodeSet ( html , \"//div[@class='BNeawe vvjwJb AP7Wnd']\" ), function ( x ) { XML :: xmlValue ( x ) } ), links = links , stringsAsFactors = FALSE ) print ( df , right = F ) Output label 1 FRIM - Fruit Integrative Modelling - Experimental ... - Data Inra 2 [ PDF ] pe - pa \u2013 tomato frim1 - CBIB 3 Home \u00b7 inrae / ODAM Wiki \u00b7 GitHub 4 [ PDF ] Data Capture ? Daniel Jacob INRA UMR 1332 BFP 5 [ PDF ] Modeling the growth of tomato fruits based on enzyme ... - HAL - CNRS 6 FRIM - Fruit Integrative Modelling | Zenodo 7 Rodam package - CRAN 8 Odam : Open Data , Access and Mining - SlideShare 9 Make your data great now - SlideShare links 1 https : //data.inra.fr/dataset.xhtml%3FpersistentId%3Ddoi:10.15454/95JUTK 2 http : //services.cbib.u-bordeaux.fr/MERYB/DATA/protocols_upload/1_5_Analytical_177.pdf 3 https : //github.com/INRA/ODAM/wiki 4 https : //hal.archives-ouvertes.fr/hal-02070883/file/Howto_FAIR_DataLifecycle_Aprill2019.pdf 5 https : //hal-cnrs.archives-ouvertes.fr/hal-02611223/document 6 https : //zenodo.org/record/154041 7 https : //cran.r-project.org/web/packages/Rodam/vignettes/Rodam.html 8 https : //www.slideshare.net/danieljacob771282/odam-open-data-access-and-mining 9 https : //www.slideshare.net/danieljacob771282/make-your-data-great-now Code # is there a link to a dataverse site? ret <- grep ( \"dataset.xhtml.persistentId=doi\" , links ) if ( length ( ret ) ) { # Bingo ! doi <- paste0 ( 'doi:' , gsub ( \"^http.+doi:\" , \"\" , links[ret[1]] ) ) dataverse <- gsub ( \"/dataset.+\" , \"\" , links[ret[1]] ) # Retrieving Dataset and File Metadata Sys.setenv ( \"DATAVERSE_SERVER\" = dataverse ) dataset <- dataverse :: get_dataset ( doi ) # Display the dataset information dataset } Output Dataset ( 261843 ) : Version : 4.2 , RELEASED Release Date : 2020 - 06 - 08 T14 : 57 : 17 Z License : NONE 17 Files : label version id contentType 1 datapackage . json 1 99623 application / json 2 frim1 . zip 3 96878 application / zip Code # Get more metadata if ( length ( ret ) ) { metadata <- dataverse :: dataset_metadata ( doi ) metadata $ fields[ ! metadata $ fields $ multiple | metadata $ fields $ typeClass == \"controlledVocabulary\" , c ( \"typeName\" , \"value\" ) ] } Output typeName value 1 title FRIM - Fruit Integrative Modelling 2 alternativeURL https : //pmb-bordeaux.fr/dataexplorer/?ds=frim1 6 kindOfData Dataset 7 dataOrigin experimental data 8 subject Computer science , Information management , Omics , Plant Health and Pathology 9 lifeCycleStep Study design , Data collection 12 language English 14 productionDate 2010 15 productionPlace INVENIO , Ste Livrade , France 17 depositor Jacob , Daniel 18 dateOfDeposit 2018 - 10 - 08","title":"Google search"},{"location":"json-schema/google-search/#odam-find-a-dataverse-dataset-via-google-search","text":"The purpose of this R script is to test if a dataset can be \"findable\" by a search engine such as Google. For this test, we have chosen our example dataset 'frim1'. The searched keywords are thus ' frim1 ' associated with ' dataset '. See the code and its results. Step 1: Start by searching for the keywords in Google Search Step 2 - If found links in step 1, select those that correspond to a Dataverse dataset Step 3 - If found some in step 2, then retrieve the metadata See below the code and its results. Code packages <- c ( 'RCurl' , 'XML' , 'dataverse' ) if ( length ( setdiff ( packages , rownames ( installed.packages ()))) > 0 ) { install.packages ( setdiff ( packages , rownames ( installed.packages ())), repos = 'http://cran.rstudio.com' ) } library ( RCurl ) library ( XML ) library ( dataverse ) # Search terms searchTerms <- \"frim1 dataset\" language <- \"en\" # Collapse search terms. entry <- paste ( searchTerms , collapse = \"+\" ) # Do the search siteHTML <- RCurl :: getForm ( \"http://www.google.com/search\" , hl = \"en\" , lr = \"\" , q = entry , btnG = \"Search\" ) # Extract nodes with the links html <- XML :: htmlTreeParse ( siteHTML , useInternalNodes = TRUE , error = function ( ... ){}) nodes <- XML :: getNodeSet ( html , \"//div[@class='kCrYT']//a\" ) # Get all the links links <- sapply ( nodes , function ( x ) x <- XML :: xmlAttrs ( x ) [[ \"href\" ]] ) links <- links [grep ( \"google.com/search\" , links , invert = TRUE ) ] links <- gsub ( '/url.q=' , '' , gsub ( '&sa.+$' , '' , links )) # harmonizes the writing of URLs links <- gsub ( \"%3D\" , \"=\" , gsub ( \"%3F\" , \"?\" , links ) ) # Compile into a data.frame df <- data.frame ( label = sapply ( XML :: getNodeSet ( html , \"//div[@class='BNeawe vvjwJb AP7Wnd']\" ), function ( x ) { XML :: xmlValue ( x ) } ), links = links , stringsAsFactors = FALSE ) print ( df , right = F ) Output label 1 FRIM - Fruit Integrative Modelling - Experimental ... - Data Inra 2 [ PDF ] pe - pa \u2013 tomato frim1 - CBIB 3 Home \u00b7 inrae / ODAM Wiki \u00b7 GitHub 4 [ PDF ] Data Capture ? Daniel Jacob INRA UMR 1332 BFP 5 [ PDF ] Modeling the growth of tomato fruits based on enzyme ... - HAL - CNRS 6 FRIM - Fruit Integrative Modelling | Zenodo 7 Rodam package - CRAN 8 Odam : Open Data , Access and Mining - SlideShare 9 Make your data great now - SlideShare links 1 https : //data.inra.fr/dataset.xhtml%3FpersistentId%3Ddoi:10.15454/95JUTK 2 http : //services.cbib.u-bordeaux.fr/MERYB/DATA/protocols_upload/1_5_Analytical_177.pdf 3 https : //github.com/INRA/ODAM/wiki 4 https : //hal.archives-ouvertes.fr/hal-02070883/file/Howto_FAIR_DataLifecycle_Aprill2019.pdf 5 https : //hal-cnrs.archives-ouvertes.fr/hal-02611223/document 6 https : //zenodo.org/record/154041 7 https : //cran.r-project.org/web/packages/Rodam/vignettes/Rodam.html 8 https : //www.slideshare.net/danieljacob771282/odam-open-data-access-and-mining 9 https : //www.slideshare.net/danieljacob771282/make-your-data-great-now Code # is there a link to a dataverse site? ret <- grep ( \"dataset.xhtml.persistentId=doi\" , links ) if ( length ( ret ) ) { # Bingo ! doi <- paste0 ( 'doi:' , gsub ( \"^http.+doi:\" , \"\" , links[ret[1]] ) ) dataverse <- gsub ( \"/dataset.+\" , \"\" , links[ret[1]] ) # Retrieving Dataset and File Metadata Sys.setenv ( \"DATAVERSE_SERVER\" = dataverse ) dataset <- dataverse :: get_dataset ( doi ) # Display the dataset information dataset } Output Dataset ( 261843 ) : Version : 4.2 , RELEASED Release Date : 2020 - 06 - 08 T14 : 57 : 17 Z License : NONE 17 Files : label version id contentType 1 datapackage . json 1 99623 application / json 2 frim1 . zip 3 96878 application / zip Code # Get more metadata if ( length ( ret ) ) { metadata <- dataverse :: dataset_metadata ( doi ) metadata $ fields[ ! metadata $ fields $ multiple | metadata $ fields $ typeClass == \"controlledVocabulary\" , c ( \"typeName\" , \"value\" ) ] } Output typeName value 1 title FRIM - Fruit Integrative Modelling 2 alternativeURL https : //pmb-bordeaux.fr/dataexplorer/?ds=frim1 6 kindOfData Dataset 7 dataOrigin experimental data 8 subject Computer science , Information management , Omics , Plant Health and Pathology 9 lifeCycleStep Study design , Data collection 12 language English 14 productionDate 2010 15 productionPlace INVENIO , Ste Livrade , France 17 depositor Jacob , Daniel 18 dateOfDeposit 2018 - 10 - 08","title":"ODAM: Find a Dataverse dataset via Google Search"},{"location":"json-schema/odam-data-package/","text":"{ \"$schema\" : \"http://json-schema.org/draft-07/schema#\" , \"$id\" : \"https://inrae.github.io/ODAM/json-schema/odam-data-package.json\" , \"title\" : \"odam-data-package\" , \"description\" : \"Description of an ODAM data package\" , \"type\" : \"object\" , \"definitions\" : { \"profile\" : { \"title\" : \"odam-data-package\" , \"description\" : \"odam-data-package profile is an extension of the tabular-data-package profile\" , \"type\" : \"string\" }, \"path\" : { \"title\" : \"Path\" , \"description\" : \"A fully qualified URL, or a POSIX file path..\" , \"type\" : \"string\" , \"pattern\" : \"^(?=^[^./~])(^((?!\\\\.{2}).)*$).*$\" , \"context\" : \"Implementations need to negotiate the type of path provided, and dereference the data accordingly.\" }, \"name\" : { \"title\" : \"Name\" , \"description\" : \"An identifier string. Lower case characters with `_` are allowed.\" , \"type\" : \"string\" , \"pattern\" : \"^([a-zA-Z0-9_])+$\" , \"context\" : \"This is ideally a url-usable and human-readable name. Name `SHOULD` be invariant, meaning it `SHOULD NOT` change when its parent descriptor is updated.\" }, \"title\" : { \"title\" : \"Title\" , \"description\" : \"A human-readable title.\" , \"type\" : \"string\" }, \"email\" : { \"title\" : \"Email\" , \"description\" : \"An email address.\" , \"type\" : \"string\" , \"format\" : \"email\" }, \"keywords\" : { \"title\" : \"Keywords\" , \"description\" : \"A list of keywords that describe this package.\" , \"type\" : \"array\" , \"minItems\" : 1 , \"items\" : { \"type\" : \"string\" } }, \"license\" : { \"title\" : \"License\" , \"description\" : \"A license for this descriptor.\" , \"type\" : \"object\" , \"properties\" : { \"name\" : { \"title\" : \"Open Definition license identifier\" , \"description\" : \"MUST be an Open Definition license identifier, see http://licenses.opendefinition.org/\" , \"type\" : \"string\" , \"pattern\" : \"^([-a-zA-Z0-9._])+$\" }, \"path\" : { \"$ref\" : \"#/definitions/path\" }, \"title\" : { \"$ref\" : \"#/definitions/title\" } }, \"context\" : \"Use of this property does not imply that the person was the original creator of, or a contributor to, the data in the descriptor, but refers to the composition of the descriptor itself.\" }, \"licenses\" : { \"title\" : \"Licenses\" , \"description\" : \"The license(s) under which this package is published.\" , \"type\" : \"array\" , \"minItems\" : 1 , \"items\" : { \"$ref\" : \"#/definitions/license\" }, \"context\" : \"This property is not legally binding and does not guarantee that the package is licensed under the terms defined herein.\" }, \"tabularDataResources\" : { \"title\" : \"Tabular Data Resources\" , \"description\" : \"An `array` of Tabular Data Resource objects, each compliant with the [Tabular Data Resource](/tabular-data-resource/) specification.\" , \"type\" : \"array\" , \"minItems\" : 1 , \"items\" : { \"$ref\" : \"https://inrae.github.io/ODAM/json-schema/odam-data-resource.json\" } }, \"source\" : { \"title\" : \"Source\" , \"description\" : \"A source file.\" , \"type\" : \"object\" , \"required\" : [ \"title\" ], \"properties\" : { \"title\" : { \"$ref\" : \"#/definitions/title\" }, \"path\" : { \"$ref\" : \"#/definitions/path\" }, \"email\" : { \"$ref\" : \"#/definitions/email\" } } }, \"sources\" : { \"title\" : \"Sources\" , \"description\" : \"The raw sources for this resource.\" , \"type\" : \"array\" , \"minItems\" : 0 , \"items\" : { \"$ref\" : \"#/definitions/source\" } } }, \"properties\" : { \"profile\" : { \"$ref\" : \"#/definitions/profile\" }, \"name\" : { \"$ref\" : \"#/definitions/name\" }, \"datapackage_version\" : { \"type\" : \"string\" , \"pattern\" : \"^([0-9.])+$\" }, \"keywords\" : { \"$ref\" : \"#/definitions/keywords\" }, \"licenses\" : { \"$ref\" : \"#/definitions/licenses\" }, \"resources\" : { \"$ref\" : \"#/definitions/tabularDataResources\" }, \"sources\" : { \"$ref\" : \"#/definitions/sources\" } }, \"required\" : [ \"profile\" , \"name\" , \"licenses\" , \"resources\" ] }","title":"ODAM-data-package"},{"location":"json-schema/odam-data-resource/","text":"{ \"$schema\" : \"http://json-schema.org/draft-07/schema#\" , \"$id\" : \"https://inrae.github.io/ODAM/json-schema/odam-data-resource.json\" , \"title\" : \"odam-data-resource\" , \"description\" : \"Description of an ODAM data resource\" , \"type\" : \"object\" , \"definitions\" : { \"profile\" : { \"title\" : \"odam-data-resource\" , \"description\" : \"odam-data-resource profile is an extension of the tabular data resource profile\" , \"type\" : \"string\" }, \"name\" : { \"title\" : \"Name\" , \"description\" : \"An identifier string. Lower case characters with `_` are allowed.\" , \"type\" : \"string\" , \"pattern\" : \"^([a-zA-Z0-9_])+$\" , \"context\" : \"This is ideally a url-usable and human-readable name. Name `SHOULD` be invariant, meaning it `SHOULD NOT` change when its parent descriptor is updated.\" }, \"label\" : { \"title\" : \"Label\" , \"description\" : \"A very short description where spaces are allowed.\" , \"type\" : \"string\" , \"context\" : \"Ideally it corresponds to the label for CV Term\" }, \"title\" : { \"title\" : \"Title\" , \"description\" : \"A human-readable title.\" , \"type\" : \"string\" }, \"description\" : { \"title\" : \"Description\" , \"description\" : \"A text description. Markdown is encouraged.\" , \"type\" : \"string\" }, \"path\" : { \"title\" : \"Path\" , \"description\" : \"A fully qualified URL, or a POSIX file path..\" , \"type\" : \"string\" , \"pattern\" : \"^(?=^[^./~])(^((?!\\\\.{2}).)*$).*$\" , \"context\" : \"Implementations need to negotiate the type of path provided, and dereference the data accordingly.\" }, \"cv_term\" : { \"title\" : \"CV Term\" , \"description\" : \"Controlled Vocabulary based on ontology\" , \"type\" : \"object\" , \"properties\" : { \"label\" : { \"$ref\" : \"#/definitions/label\" }, \"path\" : { \"$ref\" : \"#/definitions/path\" } } }, \"csvDialect\" : { \"title\" : \"CSV Dialect\" , \"description\" : \"The CSV dialect descriptor.\" , \"required\" : [ \"delimiter\" , \"doubleQuote\" ], \"properties\" : { \"csvddfVersion\" : { \"$ref\" : \"#/definitions/csvddfVersion\" }, \"delimiter\" : { \"$ref\" : \"#/definitions/delimiter\" }, \"doubleQuote\" : { \"$ref\" : \"#/definitions/doubleQuote\" }, \"lineTerminator\" : { \"$ref\" : \"#/definitions/lineTerminator\" }, \"quoteChar\" : { \"$ref\" : \"#/definitions/quoteChar\" }, \"escapeChar\" : { \"$ref\" : \"#/definitions/escapeChar\" }, \"skipInitialSpace\" : { \"$ref\" : \"#/definitions/skipInitialSpace\" }, \"header\" : { \"$ref\" : \"#/definitions/header\" }, \"commentChar\" : { \"$ref\" : \"#/definitions/commentChar\" }, \"caseSensitiveHeader\" : { \"$ref\" : \"#/definitions/caseSensitiveHeader\" } } }, \"csvddfVersion\" : { \"title\" : \"CSV Dialect schema version\" , \"description\" : \"A number to indicate the schema version of CSV Dialect. Version 1.0 was named CSV Dialect Description Format and used different field names.\" , \"type\" : \"number\" , \"default\" : 1.2 }, \"delimiter\" : { \"title\" : \"Delimiter\" , \"description\" : \"A character sequence to use as the field separator.\" , \"type\" : \"string\" , \"default\" : \",\" }, \"doubleQuote\" : { \"title\" : \"Double Quote\" , \"description\" : \"Specifies the handling of quotes inside fields.\" , \"context\" : \"If Double Quote is set to true, two consecutive quotes must be interpreted as one.\" , \"type\" : \"boolean\" , \"default\" : true }, \"lineTerminator\" : { \"title\" : \"Line Terminator\" , \"description\" : \"Specifies the character sequence that must be used to terminate rows.\" , \"type\" : \"string\" , \"default\" : \"\\n\" }, \"quoteChar\" : { \"title\" : \"Quote Character\" , \"description\" : \"Specifies a one-character string to use as the quoting character.\" , \"type\" : \"string\" , \"default\" : \"\\\"\" }, \"escapeChar\" : { \"title\" : \"Escape Character\" , \"description\" : \"Specifies a one-character string to use as the escape character.\" , \"type\" : \"string\" }, \"skipInitialSpace\" : { \"title\" : \"Skip Initial Space\" , \"description\" : \"Specifies the interpretation of whitespace immediately following a delimiter. If false, whitespace immediately after a delimiter should be treated as part of the subsequent field.\" , \"type\" : \"boolean\" , \"default\" : true }, \"header\" : { \"title\" : \"Header\" , \"description\" : \"Specifies if the file includes a header row, always as the first row in the file.\" , \"type\" : \"boolean\" , \"default\" : true }, \"commentChar\" : { \"title\" : \"Comment Character\" , \"description\" : \"Specifies a character sequence causing the rest of the line after it to be ignored.\" , \"type\" : \"string\" }, \"caseSensitiveHeader\" : { \"title\" : \"Case Sensitive Header\" , \"description\" : \"Specifies if the case of headers is meaningful.\" , \"context\" : \"Use of case in source CSV files is not always an intentional decision. For example, should \\\"CAT\\\" and \\\"Cat\\\" be considered to have the same meaning.\" , \"type\" : \"boolean\" , \"default\" : false }, \"format\" : { \"title\" : \"Format\" , \"description\" : \"The file format of this resource.\" , \"context\" : \"`csv`, `xls`, `json` are examples of common formats.\" , \"type\" : \"string\" }, \"mediatype\" : { \"title\" : \"Media Type\" , \"description\" : \"The media type of this resource. Can be any valid media type listed with [IANA](https://www.iana.org/assignments/media-types/media-types.xhtml).\" , \"type\" : \"string\" , \"pattern\" : \"^(.+)/(.+)$\" }, \"encoding\" : { \"title\" : \"Encoding\" , \"description\" : \"The file encoding of this resource.\" , \"type\" : \"string\" , \"default\" : \"utf-8\" }, \"tableSchema\" : { \"title\" : \"Table Schema\" , \"description\" : \"A Table Schema for this resource, compliant with the [Table Schema](/tableschema/) specification.\" , \"type\" : \"object\" , \"required\" : [ \"fields\" , \"categories\" ], \"properties\" : { \"fields\" : { \"type\" : \"array\" , \"minItems\" : 1 , \"items\" : { \"$ref\" : \"#/definitions/tableSchemaField\" }, \"description\" : \"An `array` of Table Schema Field objects.\" }, \"primaryKey\" : { \"$ref\" : \"#/definitions/tableSchemaPrimaryKey\" }, \"foreignKeys\" : { \"type\" : \"array\" , \"minItems\" : 1 , \"items\" : { \"$ref\" : \"#/definitions/tableSchemaForeignKey\" } }, \"categories\" : { \"type\" : \"array\" , \"minItems\" : 1 , \"items\" : { \"$ref\" : \"#/definitions/tableSchemaCategory\" }, \"description\" : \"An `array` of Table Schema Category objects.\" } } }, \"tableSchemaField\" : { \"title\" : \"Table Schema Field\" , \"type\" : \"object\" , \"required\" : [ \"name\" , \"type\" , \"title\" ], \"properties\" : { \"name\" : { \"$ref\" : \"#/definitions/name\" }, \"type\" : { \"type\" : \"string\" , \"enum\" : [ \"number\" , \"string\" ] }, \"title\" : { \"$ref\" : \"#/definitions/title\" }, \"unit\" : { \"type\" : \"string\" }, \"cv_term\" : { \"$ref\" : \"#/definitions/cv_term\" }, \"constraints\" : { \"title\" : \"Constraints\" , \"description\" : \"The following constraints are supported for `string` fields.\" , \"type\" : \"object\" , \"properties\" : { \"required\" : { \"type\" : \"boolean\" , \"description\" : \"Indicates whether a property must have a value for each instance.\" , \"context\" : \"An empty string is considered to be a missing value.\" }, \"unique\" : { \"type\" : \"boolean\" , \"description\" : \"When `true`, each value for the property `MUST` be unique.\" } } } } }, \"tableSchemaPrimaryKey\" : { \"oneOf\" : [ { \"type\" : \"array\" , \"minItems\" : 1 , \"uniqueItems\" : false , \"items\" : { \"type\" : \"string\" } }, { \"type\" : \"string\" } ], \"description\" : \"A primary key is a field name or an array of field names, whose values `MUST` uniquely identify each row in the table.\" }, \"tableSchemaForeignKey\" : { \"title\" : \"Table Schema Foreign Key\" , \"description\" : \"Table Schema Foreign Key\" , \"type\" : \"object\" , \"required\" : [ \"fields\" , \"reference\" ], \"properties\" : { \"fields\" : { \"type\" : \"string\" , \"description\" : \"Fields that make up the primary key.\" }, \"reference\" : { \"type\" : \"object\" , \"required\" : [ \"resource\" , \"fields\" ], \"properties\" : { \"resource\" : { \"type\" : \"string\" , \"default\" : \"\" }, \"fields\" : { \"type\" : \"string\" } } } } }, \"tableSchemaCategory\" : { \"title\" : \"Table Schema Category\" , \"type\" : \"object\" , \"properties\" : { \"name\" : { \"type\" : \"string\" , \"enum\" : [ \"identifier\" , \"factor\" , \"quantitative\" , \"qualitative\" ] }, \"fields\" : { \"type\" : \"array\" } } }, \"tableSchemaMissingValues\" : { \"type\" : \"array\" , \"items\" : { \"type\" : \"string\" }, \"default\" : [ \"\" , \"NA\" ] } }, \"properties\" : { \"path\" : { \"$ref\" : \"#/definitions/path\" }, \"profile\" : { \"$ref\" : \"#/definitions/profile\" }, \"name\" : { \"$ref\" : \"#/definitions/name\" }, \"title\" : { \"$ref\" : \"#/definitions/title\" }, \"identifier\" : { \"description\" : \"name of the identifier field (column name). i.e same as primaryKey\" , \"type\" : \"string\" }, \"obtainedFrom\" : { \"description\" : \"name of the file \" , \"type\" : \"string\" }, \"joinkey\" : { \"description\" : \"name of the identifier field (column name) that serves to link with its father file i.e same as foreignKey\" , \"type\" : \"string\" }, \"cv_term\" : { \"$ref\" : \"#/definitions/cv_term\" }, \"schema\" : { \"$ref\" : \"#/definitions/tableSchema\" }, \"dialect\" : { \"$ref\" : \"#/definitions/csvDialect\" }, \"encoding\" : { \"$ref\" : \"#/definitions/encoding\" }, \"format\" : { \"$ref\" : \"#/definitions/format\" }, \"mediatype\" : { \"$ref\" : \"#/definitions/mediatype\" }, \"missingValues\" : { \"$ref\" : \"#/definitions/tableSchemaMissingValues\" } }, \"required\" : [ \"path\" , \"name\" , \"title\" , \"identifier\" , \"schema\" , \"dialect\" , \"format\" ] }","title":"ODAM-data-ressource"},{"location":"json-schema/session-examples/","text":"Session example with R using a datapackage \u00b6 In this example, the datapackage is directly generated from an ODAM repository . But it is also possible to retrieve it from a Dataverse repository from a keyword. See an Example json_validate: Validate a json file https://rdrr.io/cran/jsonvalidate/man/json_validate.html Code packages <- c ( 'httr' , 'jsonlite' , 'jsonvalidate' ) if ( length ( setdiff ( packages , rownames ( installed.packages ()))) > 0 ) { install.packages ( setdiff ( packages , rownames ( installed.packages ())), repos = 'http://cran.rstudio.com' ) } library ( httr ) library ( jsonvalidate ) library ( jsonlite ) options ( width = 256 ) # URL of the ODAM data repository odam_url <- 'https://pmb-bordeaux.fr/getdata' # ID of the dataset dataset <- 'frim1' # Get the ODAM data package schema URL <- 'https://inrae.github.io/ODAM/json-schema/odam-data-package.json' response <- httr :: GET ( URL , config ( sslversion = 6 , ssl_verifypeer = 1 )) schema <- httr :: content ( response , as = 'text' ) # Get structural metadata information in datapackage format (json) for a dataset # directly from its ODAM repository. As the option links is set to 1, we will have # the absolute reference for data files (see 'path' below) URL <- sprintf ( '%s/query/%s/datapackage?links=1' , odam_url , dataset ) response <- httr :: GET ( URL , config ( sslversion = 6 , ssl_verifypeer = 1 )) dp_json <- httr :: content ( response , as = 'text' ) # Validate the JSON against the ODAM data package schema jsonvalidate :: json_validate ( dp_json , schema ) Output [1] TRUE Code # Parse the JSON object to a data.frame dp <- jsonlite :: fromJSON ( dp_json ) # View licenses print ( dp $ licenses , right = F ) Output name path title 1 ODC - BY - 1.0 https : //opendatacommons.org/licenses/by/ Open Data Commons Attribution License 1.0 Code resources <- dp $ resources # List some metadata about the dataset resources[ c ( \"name\" , \"title\" , \"identifier\" , \"obtainedFrom\" , \"joinkey\" , \"path\" ) ] Output name title identifier obtainedFrom joinkey path 1 plants Plant features PlantID < NA > < NA > https : //pmb-bordeaux.fr/getdata/tsv/frim1/plants 2 samples Sample features SampleID plants PlantID https : //pmb-bordeaux.fr/getdata/tsv/frim1/samples 3 aliquots Aliquots features AliquotID samples SampleID https : //pmb-bordeaux.fr/getdata/tsv/frim1/aliquots 4 cellwall_metabo Cell wall Compound quantifications AliquotID aliquots AliquotID https : //pmb-bordeaux.fr/getdata/tsv/frim1/cellwall_metabo 5 cellwall_metaboFW Cell Wall Compound quantifications ( FW ) AliquotID aliquots AliquotID https : //pmb-bordeaux.fr/getdata/tsv/frim1/cellwall_metaboFW 6 activome Activome Features AliquotID aliquots AliquotID https : //pmb-bordeaux.fr/getdata/tsv/frim1/activome 7 pools Pools of remaining pools PoolID samples SampleID https : //pmb-bordeaux.fr/getdata/tsv/frim1/pools 8 qMS_metabo MS Compounds quantification PoolID pools PoolID https : //pmb-bordeaux.fr/getdata/tsv/frim1/qMS_metabo 9 qNMR_metabo NMR Compounds quantification PoolID pools PoolID https : //pmb-bordeaux.fr/getdata/tsv/frim1/qNMR_metabo 10 plato_hexosesP Hexoses Phosphate AliquotID aliquots AliquotID https : //pmb-bordeaux.fr/getdata/tsv/frim1/plato_hexosesP 11 lipids_AG Lipids AG AliquotID aliquots AliquotID https : //pmb-bordeaux.fr/getdata/tsv/frim1/lipids_AG 12 AminoAcid Amino Acids AliquotID aliquots AliquotID https : //pmb-bordeaux.fr/getdata/tsv/frim1/AminoAcid Code # Read the 'samples' data file - index=2 index <- 2 M <- read.table ( url ( resources[ \"path\" ] $ path[index] ), header = resources $ dialect $ header[index] , sep = resources $ dialect $ delimiter[index] ) # Display an extract M[1 : 10 , ] Output SampleID PlantID Truss DevStage FruitAge HarvestDate HarvestHour FruitPosition FruitDiameter FruitHeight FruitFW FruitDW DW 1 1 A26 T5 FF .01 08 DPA 40379 0.5 2 NA NA 0.72 0.090216 NA 2 1 C2 T5 FF .01 08 DPA 40379 0.5 3 NA NA 0.56 0.070168 NA 3 1 D15 T5 FF .01 08 DPA 40379 0.5 4 NA NA 0.78 0.097734 NA 4 1 E19 T5 FF .01 08 DPA 40379 0.5 4 NA NA 0.66 0.082698 NA 5 1 E34 T5 FF .01 08 DPA 40379 0.5 3 NA NA 0.7 0.087710 NA 6 1 E38 T5 FF .01 08 DPA 40379 0.5 3 NA NA 0.7 0.087710 NA 7 1 H29 T5 FF .01 08 DPA 40379 0.5 5 NA NA 1.24 0.155372 NA 8 1 H34 T5 FF .01 08 DPA 40379 0.5 4 NA NA 0.86 0.107758 NA 9 1 H52 T5 FF .01 08 DPA 40379 0.5 5 NA NA 0.77 0.096481 NA 10 1 H61 T5 FF .01 08 DPA 40379 0.5 5 NA NA 0.56 0.070168 NA Code # Get the categories for the 'samples' data subset categories <- resources $ schema $ categories[[index]] categories # List the 'quantitative' variables for the 'samples' data subset categories $ fields[ categories $ name == \"quantitative\" ][[1]] Output name fields 1 identifier SampleID 2 factor DevStage , FruitAge 3 quantitative FruitDiameter , FruitHeight , FruitFW , FruitDW , DW 4 qualitative Truss [ 1 ] \"FruitDiameter\" \"FruitHeight\" \"FruitFW\" \"FruitDW\" \"DW\" Code # Get the list of data subsets that were obtained from the 'samples' index <- 2 subsets <- resources $ schema $ foreignKeys[[index]] $ reference[ , 1 ] if ( ! is.null ( subsets ) ) resources[ resources $ name %in% subsets , c ( \"name\" , \"title\" , \"identifier\" , \"obtainedFrom\" ) ] # Get the list of data subsets that were obtained from the 'aliquots' index <- 3 subsets <- resources $ schema $ foreignKeys[[index]] $ reference[ , 1 ] if ( ! is.null ( subsets ) ) resources[ resources $ name %in% subsets , c ( \"name\" , \"title\" , \"identifier\" , \"obtainedFrom\" ) ] Output # List of data subsets that were obtained from the 'samples' name title identifier obtainedFrom 3 aliquots Aliquots features AliquotID samples 7 pools Pools of remaining pools PoolID samples # List of data subsets that were obtained from the 'aliquots' name title identifier obtainedFrom 4 cellwall_metabo Cell wall Compound quantifications AliquotID aliquots 5 cellwall_metaboFW Cell Wall Compound quantifications ( FW ) AliquotID aliquots 6 activome Activome Features AliquotID aliquots 10 plato_hexosesP Hexoses Phosphate AliquotID aliquots 11 lipids_AG Lipids AG AliquotID aliquots 12 AminoAcid Amino Acids AliquotID aliquots Code # getMergedDataset : # Returns the data subset resulting from the merging of each data subset from which # the previous data subset was obtained (i.e. going back up the chain of linking # data subsets) # Method : Performs a 'Rigth Join' between the different tables (data subsets) getMergedDataset <- function ( resources , subset , verbose = FALSE ) { M <- NULL s <- subset while ( sum ( resources $ name %in% s ) ) { if ( verbose ) cat ( \"data subset \" , s , \"\\n\" ) # index for subset s i1 <- which ( resources $ name %in% s ) # get metadata from subset s m1 <- resources[i1 , ] # Read data for subset s M1 <- read.table ( url ( resources[ \"path\" ] $ path[i1] ), header = resources $ dialect $ header[i1] , sep = resources $ dialect $ delimiter[i1] ) if ( ! is.null ( M )) # Merge the subset s - Right Join M <- merge ( M1 , M , by = m1 $ identifier , all.y = TRUE ) else M <- M1 # the data subset from which it was obtained. s <- m1 $ obtainedFrom # End of the chain ? if ( is.na ( s )) break } unique ( M ) } # Merge the 'plants', 'samples', 'aliquots' and 'activome' data subsets M <- getMergedDataset ( resources , 'activome' , TRUE ) dim ( M ) # Display column names of the merged data subset colnames ( M ) Output data subset activome data subset aliquots data subset samples data subset plants [ 1 ] 1272 55 [ 1 ] \"PlantID\" \"Rank\" \"PlantNum\" \"Treatment\" \"SampleID\" [ 6 ] \"Truss\" \"DevStage\" \"FruitAge\" \"HarvestDate\" \"HarvestHour\" [ 11 ] \"FruitPosition\" \"FruitDiameter\" \"FruitHeight\" \"FruitFW\" \"FruitDW\" [ 16 ] \"DW\" \"AliquotID\" \"PGM\" \"cFBPase\" \"PyrK\" [ 21 ] \"CitS\" \"PFP\" \"Aconitase\" \"PFK\" \"FruK\" [ 26 ] \"pFBPase\" \"GluK\" \"NAD_ISODH\" \"Enolase\" \"NADP_ISODH\" [ 31 ] \"PEPC\" \"Aldolase\" \"Succ_CoA_ligase\" \"NAD_MalDH\" \"AlaAT\" [ 36 ] \"Fumarase\" \"AspAT\" \"NADP_GluDH\" \"NAD_GAPDH\" \"NADP_GAPDH\" [ 41 ] \"NAD_GluDH\" \"TPI\" \"PGK\" \"Neutral_Inv\" \"Acid_Inv\" [ 46 ] \"G6PDH\" \"UGPase\" \"SuSy\" \"NAD_ME\" \"ShiDH\" [ 51 ] \"NADP_ME\" \"PGI\" \"StarchS\" \"AGPase\" \"SPS\" Session example with Python using a datapackage \u00b6 datapackage-py https://pypi.org/project/datapackage/ Code from datapackage import Package import requests import json import jsonschema import pandas as pd # URL of the ODAM data repository odam_url = 'https://pmb-bordeaux.fr/getdata' # ID of the dataset dataset = 'frim1' # Read the datapackage.json as a dict url = odam_url + '/query/' + dataset + '/datapackage?links=1' dp = Package ( url ) # Read the ODAM data package schema as a dict url_schema = 'https://inrae.github.io/ODAM/json-schema/odam-data-package.json' response = requests . get ( url_schema ) schema = json . loads ( response . text ) # if the package is a valid ODAM data package, then shows its resource names if jsonschema . Draft7Validator ( schema ) . is_valid ( dp . descriptor ) == True : print ( dp . resource_names ) else : print ( \"Error: datapackage is not a valid ODAM datapackage\" ) Output ['plants', 'samples', 'aliquots', 'cellwall_metabo', 'cellwall_metaboFW', 'activome', 'pools', 'qMS_metabo', 'qNMR_metabo', 'plato_hexosesP', 'lipids_AG', 'AminoAcid'] Code # Get the 'plants' subsets into a data.frame id = dp . resource_names . index ( 'plants' ) df = pd . DataFrame ( dp . resources [ id ] . read ( keyed = True )) # Print the data.frame df Output PlantID Rank PlantNum Treatment 0 A1 A 1 Control 1 A2 A 2 Control 2 A3 A 3 Control 3 A4 A 4 Control 4 A5 A 5 Control .. ... ... ... ... 547 G65 G 548 WaterStress 548 G66 G 549 WaterStress 549 G67 G 550 WaterStress 550 G68 G 551 WaterStress 551 G69 G 552 WaterStress [552 rows x 4 columns] Code # Get the headers of the first resource (id=0) dp . resources [ 0 ] . _Resource__table_options Output {'scheme': None, 'format': 'csv', 'encoding': 'utf-8', 'pick_fields': None, 'skip_fields': None, 'pick_rows': [], 'skip_rows': [], 'delimiter': '\\t', 'doublequote': False, 'lineterminator': '\\n', 'skipinitialspace': True} Code # Get the resource descriptor of the 'samples' id = dp . resource_names . index ( 'samples' ) res = dp . resources [ id ] . _Resource__current_descriptor # List some attributes for x in [ 'path' , 'profile' , 'name' , 'title' , 'identifier' , 'obtainedFrom' , 'joinkey' ]: print ( \" %s : %s \" % ( x , res [ x ])) Output path: https://pmb-bordeaux.fr/getdata/tsv/frim1/samples profile: https://inrae.github.io/ODAM/json-schema/odam-data-resource.json name: samples title: Sample features identifier: SampleID obtainedFrom: plants joinkey: PlantID Code # Get the categories of the 'samples' pd . DataFrame ( res [ 'schema' ][ 'categories' ]) Output name fields 0 identifier [SampleID] 1 factor [DevStage, FruitAge] 2 quantitative [FruitDiameter, FruitHeight, FruitFW, FruitDW,... 3 qualitative [Truss] Code # Get the data subsets that were obtained from the 'samples' print ( pd . DataFrame ( res [ 'schema' ][ 'foreignKeys' ])) the_list = [] for x in res [ 'schema' ][ 'foreignKeys' ]: the_list += [ x [ 'reference' ][ 'resource' ] ] the_list Output fields reference 0 SampleID {'resource': 'aliquots', 'fields': 'SampleID'} 1 SampleID {'resource': 'pools', 'fields': 'SampleID'} ['aliquots', 'pools'] Code # Merge the 'samples' subset with the data subset from which it was obtained id = dp . resource_names . index ( 'samples' ) res = dp . resources [ id ] . _Resource__current_descriptor id_from = dp . resource_names . index ( res [ 'obtainedFrom' ]) df_from = pd . DataFrame ( dp . resources [ id_from ] . read ( keyed = True )) df = pd . DataFrame ( dp . resources [ id ] . read ( keyed = True )) # Right join merged_inner = pd . merge ( left = df_from , right = df , how = 'right' , left_on = res [ 'joinkey' ], right_on = res [ 'joinkey' ]) merged_inner Output PlantID Rank PlantNum Treatment ... FruitHeight FruitFW FruitDW DW 0 A1 A 1 Control ... 10.42 0.81 0.098091 None 1 A1 A 1 Control ... 31.77 21.43 1.470098 None 2 A1 A 1 Control ... 46.85 64.05 4.18887 None 3 A1 A 1 Control ... 43.35 66.64 3.338664 None 4 A2 A 2 Control ... 44.93 66.98 3.355698 None ... ... ... ... ... ... ... ... ... 1282 G67 G 550 WaterStress ... 45.72 70.23 None None 1283 G68 G 551 WaterStress ... 36.56 38.6 None None 1284 G69 G 552 WaterStress ... 39.1 45.47 2.63726 None 1285 G69 G 552 WaterStress ... 46.59 65.73 3.227343 None 1286 G69 G 552 WaterStress ... 40.4 58.51 4.908989 None [1287 rows x 16 columns]","title":"R and Python session examples"},{"location":"json-schema/session-examples/#session-example-with-r-using-a-datapackage","text":"In this example, the datapackage is directly generated from an ODAM repository . But it is also possible to retrieve it from a Dataverse repository from a keyword. See an Example json_validate: Validate a json file https://rdrr.io/cran/jsonvalidate/man/json_validate.html Code packages <- c ( 'httr' , 'jsonlite' , 'jsonvalidate' ) if ( length ( setdiff ( packages , rownames ( installed.packages ()))) > 0 ) { install.packages ( setdiff ( packages , rownames ( installed.packages ())), repos = 'http://cran.rstudio.com' ) } library ( httr ) library ( jsonvalidate ) library ( jsonlite ) options ( width = 256 ) # URL of the ODAM data repository odam_url <- 'https://pmb-bordeaux.fr/getdata' # ID of the dataset dataset <- 'frim1' # Get the ODAM data package schema URL <- 'https://inrae.github.io/ODAM/json-schema/odam-data-package.json' response <- httr :: GET ( URL , config ( sslversion = 6 , ssl_verifypeer = 1 )) schema <- httr :: content ( response , as = 'text' ) # Get structural metadata information in datapackage format (json) for a dataset # directly from its ODAM repository. As the option links is set to 1, we will have # the absolute reference for data files (see 'path' below) URL <- sprintf ( '%s/query/%s/datapackage?links=1' , odam_url , dataset ) response <- httr :: GET ( URL , config ( sslversion = 6 , ssl_verifypeer = 1 )) dp_json <- httr :: content ( response , as = 'text' ) # Validate the JSON against the ODAM data package schema jsonvalidate :: json_validate ( dp_json , schema ) Output [1] TRUE Code # Parse the JSON object to a data.frame dp <- jsonlite :: fromJSON ( dp_json ) # View licenses print ( dp $ licenses , right = F ) Output name path title 1 ODC - BY - 1.0 https : //opendatacommons.org/licenses/by/ Open Data Commons Attribution License 1.0 Code resources <- dp $ resources # List some metadata about the dataset resources[ c ( \"name\" , \"title\" , \"identifier\" , \"obtainedFrom\" , \"joinkey\" , \"path\" ) ] Output name title identifier obtainedFrom joinkey path 1 plants Plant features PlantID < NA > < NA > https : //pmb-bordeaux.fr/getdata/tsv/frim1/plants 2 samples Sample features SampleID plants PlantID https : //pmb-bordeaux.fr/getdata/tsv/frim1/samples 3 aliquots Aliquots features AliquotID samples SampleID https : //pmb-bordeaux.fr/getdata/tsv/frim1/aliquots 4 cellwall_metabo Cell wall Compound quantifications AliquotID aliquots AliquotID https : //pmb-bordeaux.fr/getdata/tsv/frim1/cellwall_metabo 5 cellwall_metaboFW Cell Wall Compound quantifications ( FW ) AliquotID aliquots AliquotID https : //pmb-bordeaux.fr/getdata/tsv/frim1/cellwall_metaboFW 6 activome Activome Features AliquotID aliquots AliquotID https : //pmb-bordeaux.fr/getdata/tsv/frim1/activome 7 pools Pools of remaining pools PoolID samples SampleID https : //pmb-bordeaux.fr/getdata/tsv/frim1/pools 8 qMS_metabo MS Compounds quantification PoolID pools PoolID https : //pmb-bordeaux.fr/getdata/tsv/frim1/qMS_metabo 9 qNMR_metabo NMR Compounds quantification PoolID pools PoolID https : //pmb-bordeaux.fr/getdata/tsv/frim1/qNMR_metabo 10 plato_hexosesP Hexoses Phosphate AliquotID aliquots AliquotID https : //pmb-bordeaux.fr/getdata/tsv/frim1/plato_hexosesP 11 lipids_AG Lipids AG AliquotID aliquots AliquotID https : //pmb-bordeaux.fr/getdata/tsv/frim1/lipids_AG 12 AminoAcid Amino Acids AliquotID aliquots AliquotID https : //pmb-bordeaux.fr/getdata/tsv/frim1/AminoAcid Code # Read the 'samples' data file - index=2 index <- 2 M <- read.table ( url ( resources[ \"path\" ] $ path[index] ), header = resources $ dialect $ header[index] , sep = resources $ dialect $ delimiter[index] ) # Display an extract M[1 : 10 , ] Output SampleID PlantID Truss DevStage FruitAge HarvestDate HarvestHour FruitPosition FruitDiameter FruitHeight FruitFW FruitDW DW 1 1 A26 T5 FF .01 08 DPA 40379 0.5 2 NA NA 0.72 0.090216 NA 2 1 C2 T5 FF .01 08 DPA 40379 0.5 3 NA NA 0.56 0.070168 NA 3 1 D15 T5 FF .01 08 DPA 40379 0.5 4 NA NA 0.78 0.097734 NA 4 1 E19 T5 FF .01 08 DPA 40379 0.5 4 NA NA 0.66 0.082698 NA 5 1 E34 T5 FF .01 08 DPA 40379 0.5 3 NA NA 0.7 0.087710 NA 6 1 E38 T5 FF .01 08 DPA 40379 0.5 3 NA NA 0.7 0.087710 NA 7 1 H29 T5 FF .01 08 DPA 40379 0.5 5 NA NA 1.24 0.155372 NA 8 1 H34 T5 FF .01 08 DPA 40379 0.5 4 NA NA 0.86 0.107758 NA 9 1 H52 T5 FF .01 08 DPA 40379 0.5 5 NA NA 0.77 0.096481 NA 10 1 H61 T5 FF .01 08 DPA 40379 0.5 5 NA NA 0.56 0.070168 NA Code # Get the categories for the 'samples' data subset categories <- resources $ schema $ categories[[index]] categories # List the 'quantitative' variables for the 'samples' data subset categories $ fields[ categories $ name == \"quantitative\" ][[1]] Output name fields 1 identifier SampleID 2 factor DevStage , FruitAge 3 quantitative FruitDiameter , FruitHeight , FruitFW , FruitDW , DW 4 qualitative Truss [ 1 ] \"FruitDiameter\" \"FruitHeight\" \"FruitFW\" \"FruitDW\" \"DW\" Code # Get the list of data subsets that were obtained from the 'samples' index <- 2 subsets <- resources $ schema $ foreignKeys[[index]] $ reference[ , 1 ] if ( ! is.null ( subsets ) ) resources[ resources $ name %in% subsets , c ( \"name\" , \"title\" , \"identifier\" , \"obtainedFrom\" ) ] # Get the list of data subsets that were obtained from the 'aliquots' index <- 3 subsets <- resources $ schema $ foreignKeys[[index]] $ reference[ , 1 ] if ( ! is.null ( subsets ) ) resources[ resources $ name %in% subsets , c ( \"name\" , \"title\" , \"identifier\" , \"obtainedFrom\" ) ] Output # List of data subsets that were obtained from the 'samples' name title identifier obtainedFrom 3 aliquots Aliquots features AliquotID samples 7 pools Pools of remaining pools PoolID samples # List of data subsets that were obtained from the 'aliquots' name title identifier obtainedFrom 4 cellwall_metabo Cell wall Compound quantifications AliquotID aliquots 5 cellwall_metaboFW Cell Wall Compound quantifications ( FW ) AliquotID aliquots 6 activome Activome Features AliquotID aliquots 10 plato_hexosesP Hexoses Phosphate AliquotID aliquots 11 lipids_AG Lipids AG AliquotID aliquots 12 AminoAcid Amino Acids AliquotID aliquots Code # getMergedDataset : # Returns the data subset resulting from the merging of each data subset from which # the previous data subset was obtained (i.e. going back up the chain of linking # data subsets) # Method : Performs a 'Rigth Join' between the different tables (data subsets) getMergedDataset <- function ( resources , subset , verbose = FALSE ) { M <- NULL s <- subset while ( sum ( resources $ name %in% s ) ) { if ( verbose ) cat ( \"data subset \" , s , \"\\n\" ) # index for subset s i1 <- which ( resources $ name %in% s ) # get metadata from subset s m1 <- resources[i1 , ] # Read data for subset s M1 <- read.table ( url ( resources[ \"path\" ] $ path[i1] ), header = resources $ dialect $ header[i1] , sep = resources $ dialect $ delimiter[i1] ) if ( ! is.null ( M )) # Merge the subset s - Right Join M <- merge ( M1 , M , by = m1 $ identifier , all.y = TRUE ) else M <- M1 # the data subset from which it was obtained. s <- m1 $ obtainedFrom # End of the chain ? if ( is.na ( s )) break } unique ( M ) } # Merge the 'plants', 'samples', 'aliquots' and 'activome' data subsets M <- getMergedDataset ( resources , 'activome' , TRUE ) dim ( M ) # Display column names of the merged data subset colnames ( M ) Output data subset activome data subset aliquots data subset samples data subset plants [ 1 ] 1272 55 [ 1 ] \"PlantID\" \"Rank\" \"PlantNum\" \"Treatment\" \"SampleID\" [ 6 ] \"Truss\" \"DevStage\" \"FruitAge\" \"HarvestDate\" \"HarvestHour\" [ 11 ] \"FruitPosition\" \"FruitDiameter\" \"FruitHeight\" \"FruitFW\" \"FruitDW\" [ 16 ] \"DW\" \"AliquotID\" \"PGM\" \"cFBPase\" \"PyrK\" [ 21 ] \"CitS\" \"PFP\" \"Aconitase\" \"PFK\" \"FruK\" [ 26 ] \"pFBPase\" \"GluK\" \"NAD_ISODH\" \"Enolase\" \"NADP_ISODH\" [ 31 ] \"PEPC\" \"Aldolase\" \"Succ_CoA_ligase\" \"NAD_MalDH\" \"AlaAT\" [ 36 ] \"Fumarase\" \"AspAT\" \"NADP_GluDH\" \"NAD_GAPDH\" \"NADP_GAPDH\" [ 41 ] \"NAD_GluDH\" \"TPI\" \"PGK\" \"Neutral_Inv\" \"Acid_Inv\" [ 46 ] \"G6PDH\" \"UGPase\" \"SuSy\" \"NAD_ME\" \"ShiDH\" [ 51 ] \"NADP_ME\" \"PGI\" \"StarchS\" \"AGPase\" \"SPS\"","title":"Session example with R using a datapackage"},{"location":"json-schema/session-examples/#session-example-with-python-using-a-datapackage","text":"datapackage-py https://pypi.org/project/datapackage/ Code from datapackage import Package import requests import json import jsonschema import pandas as pd # URL of the ODAM data repository odam_url = 'https://pmb-bordeaux.fr/getdata' # ID of the dataset dataset = 'frim1' # Read the datapackage.json as a dict url = odam_url + '/query/' + dataset + '/datapackage?links=1' dp = Package ( url ) # Read the ODAM data package schema as a dict url_schema = 'https://inrae.github.io/ODAM/json-schema/odam-data-package.json' response = requests . get ( url_schema ) schema = json . loads ( response . text ) # if the package is a valid ODAM data package, then shows its resource names if jsonschema . Draft7Validator ( schema ) . is_valid ( dp . descriptor ) == True : print ( dp . resource_names ) else : print ( \"Error: datapackage is not a valid ODAM datapackage\" ) Output ['plants', 'samples', 'aliquots', 'cellwall_metabo', 'cellwall_metaboFW', 'activome', 'pools', 'qMS_metabo', 'qNMR_metabo', 'plato_hexosesP', 'lipids_AG', 'AminoAcid'] Code # Get the 'plants' subsets into a data.frame id = dp . resource_names . index ( 'plants' ) df = pd . DataFrame ( dp . resources [ id ] . read ( keyed = True )) # Print the data.frame df Output PlantID Rank PlantNum Treatment 0 A1 A 1 Control 1 A2 A 2 Control 2 A3 A 3 Control 3 A4 A 4 Control 4 A5 A 5 Control .. ... ... ... ... 547 G65 G 548 WaterStress 548 G66 G 549 WaterStress 549 G67 G 550 WaterStress 550 G68 G 551 WaterStress 551 G69 G 552 WaterStress [552 rows x 4 columns] Code # Get the headers of the first resource (id=0) dp . resources [ 0 ] . _Resource__table_options Output {'scheme': None, 'format': 'csv', 'encoding': 'utf-8', 'pick_fields': None, 'skip_fields': None, 'pick_rows': [], 'skip_rows': [], 'delimiter': '\\t', 'doublequote': False, 'lineterminator': '\\n', 'skipinitialspace': True} Code # Get the resource descriptor of the 'samples' id = dp . resource_names . index ( 'samples' ) res = dp . resources [ id ] . _Resource__current_descriptor # List some attributes for x in [ 'path' , 'profile' , 'name' , 'title' , 'identifier' , 'obtainedFrom' , 'joinkey' ]: print ( \" %s : %s \" % ( x , res [ x ])) Output path: https://pmb-bordeaux.fr/getdata/tsv/frim1/samples profile: https://inrae.github.io/ODAM/json-schema/odam-data-resource.json name: samples title: Sample features identifier: SampleID obtainedFrom: plants joinkey: PlantID Code # Get the categories of the 'samples' pd . DataFrame ( res [ 'schema' ][ 'categories' ]) Output name fields 0 identifier [SampleID] 1 factor [DevStage, FruitAge] 2 quantitative [FruitDiameter, FruitHeight, FruitFW, FruitDW,... 3 qualitative [Truss] Code # Get the data subsets that were obtained from the 'samples' print ( pd . DataFrame ( res [ 'schema' ][ 'foreignKeys' ])) the_list = [] for x in res [ 'schema' ][ 'foreignKeys' ]: the_list += [ x [ 'reference' ][ 'resource' ] ] the_list Output fields reference 0 SampleID {'resource': 'aliquots', 'fields': 'SampleID'} 1 SampleID {'resource': 'pools', 'fields': 'SampleID'} ['aliquots', 'pools'] Code # Merge the 'samples' subset with the data subset from which it was obtained id = dp . resource_names . index ( 'samples' ) res = dp . resources [ id ] . _Resource__current_descriptor id_from = dp . resource_names . index ( res [ 'obtainedFrom' ]) df_from = pd . DataFrame ( dp . resources [ id_from ] . read ( keyed = True )) df = pd . DataFrame ( dp . resources [ id ] . read ( keyed = True )) # Right join merged_inner = pd . merge ( left = df_from , right = df , how = 'right' , left_on = res [ 'joinkey' ], right_on = res [ 'joinkey' ]) merged_inner Output PlantID Rank PlantNum Treatment ... FruitHeight FruitFW FruitDW DW 0 A1 A 1 Control ... 10.42 0.81 0.098091 None 1 A1 A 1 Control ... 31.77 21.43 1.470098 None 2 A1 A 1 Control ... 46.85 64.05 4.18887 None 3 A1 A 1 Control ... 43.35 66.64 3.338664 None 4 A2 A 2 Control ... 44.93 66.98 3.355698 None ... ... ... ... ... ... ... ... ... 1282 G67 G 550 WaterStress ... 45.72 70.23 None None 1283 G68 G 551 WaterStress ... 36.56 38.6 None None 1284 G69 G 552 WaterStress ... 39.1 45.47 2.63726 None 1285 G69 G 552 WaterStress ... 46.59 65.73 3.227343 None 1286 G69 G 552 WaterStress ... 40.4 58.51 4.908989 None [1287 rows x 16 columns]","title":"Session example with Python using a datapackage"}]}